{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2 - Part 2\n",
    "POS Tagging with feed forward Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/development/arif/.conda/envs/stable/bin/python3\n"
     ]
    }
   ],
   "source": [
    "!which python3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/development/arif/.conda/envs/stable/bin/pip\n"
     ]
    }
   ],
   "source": [
    "! which pip"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "import os\n",
    "from torch import nn\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seeding randomness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57340"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(brown.sents())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56057"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(brown.words()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57340\n",
      "[('The', 'DET'), ('Fulton', 'NOUN'), ('County', 'NOUN'), ('Grand', 'ADJ'), ('Jury', 'NOUN'), ('said', 'VERB'), ('Friday', 'NOUN'), ('an', 'DET'), ('investigation', 'NOUN'), ('of', 'ADP'), (\"Atlanta's\", 'NOUN'), ('recent', 'ADJ'), ('primary', 'NOUN'), ('election', 'NOUN'), ('produced', 'VERB'), ('``', '.'), ('no', 'DET'), ('evidence', 'NOUN'), (\"''\", '.'), ('that', 'ADP'), ('any', 'DET'), ('irregularities', 'NOUN'), ('took', 'VERB'), ('place', 'NOUN'), ('.', '.')]\n",
      "[('The', 'DET'), ('jury', 'NOUN'), ('further', 'ADV'), ('said', 'VERB'), ('in', 'ADP'), ('term-end', 'NOUN'), ('presentments', 'NOUN'), ('that', 'ADP'), ('the', 'DET'), ('City', 'NOUN'), ('Executive', 'ADJ'), ('Committee', 'NOUN'), (',', '.'), ('which', 'DET'), ('had', 'VERB'), ('over-all', 'ADJ'), ('charge', 'NOUN'), ('of', 'ADP'), ('the', 'DET'), ('election', 'NOUN'), (',', '.'), ('``', '.'), ('deserves', 'VERB'), ('the', 'DET'), ('praise', 'NOUN'), ('and', 'CONJ'), ('thanks', 'NOUN'), ('of', 'ADP'), ('the', 'DET'), ('City', 'NOUN'), ('of', 'ADP'), ('Atlanta', 'NOUN'), (\"''\", '.'), ('for', 'ADP'), ('the', 'DET'), ('manner', 'NOUN'), ('in', 'ADP'), ('which', 'DET'), ('the', 'DET'), ('election', 'NOUN'), ('was', 'VERB'), ('conducted', 'VERB'), ('.', '.')]\n",
      "[('The', 'DET'), ('September-October', 'NOUN'), ('term', 'NOUN'), ('jury', 'NOUN'), ('had', 'VERB'), ('been', 'VERB'), ('charged', 'VERB'), ('by', 'ADP'), ('Fulton', 'NOUN'), ('Superior', 'ADJ'), ('Court', 'NOUN'), ('Judge', 'NOUN'), ('Durwood', 'NOUN'), ('Pye', 'NOUN'), ('to', 'PRT'), ('investigate', 'VERB'), ('reports', 'NOUN'), ('of', 'ADP'), ('possible', 'ADJ'), ('``', '.'), ('irregularities', 'NOUN'), (\"''\", '.'), ('in', 'ADP'), ('the', 'DET'), ('hard-fought', 'ADJ'), ('primary', 'NOUN'), ('which', 'DET'), ('was', 'VERB'), ('won', 'VERB'), ('by', 'ADP'), ('Mayor-nominate', 'NOUN'), ('Ivan', 'NOUN'), ('Allen', 'NOUN'), ('Jr.', 'NOUN'), ('.', '.')]\n",
      "[('``', '.'), ('Only', 'ADV'), ('a', 'DET'), ('relative', 'ADJ'), ('handful', 'NOUN'), ('of', 'ADP'), ('such', 'ADJ'), ('reports', 'NOUN'), ('was', 'VERB'), ('received', 'VERB'), (\"''\", '.'), (',', '.'), ('the', 'DET'), ('jury', 'NOUN'), ('said', 'VERB'), (',', '.'), ('``', '.'), ('considering', 'ADP'), ('the', 'DET'), ('widespread', 'ADJ'), ('interest', 'NOUN'), ('in', 'ADP'), ('the', 'DET'), ('election', 'NOUN'), (',', '.'), ('the', 'DET'), ('number', 'NOUN'), ('of', 'ADP'), ('voters', 'NOUN'), ('and', 'CONJ'), ('the', 'DET'), ('size', 'NOUN'), ('of', 'ADP'), ('this', 'DET'), ('city', 'NOUN'), (\"''\", '.'), ('.', '.')]\n",
      "[('The', 'DET'), ('jury', 'NOUN'), ('said', 'VERB'), ('it', 'PRON'), ('did', 'VERB'), ('find', 'VERB'), ('that', 'ADP'), ('many', 'ADJ'), ('of', 'ADP'), (\"Georgia's\", 'NOUN'), ('registration', 'NOUN'), ('and', 'CONJ'), ('election', 'NOUN'), ('laws', 'NOUN'), ('``', '.'), ('are', 'VERB'), ('outmoded', 'ADJ'), ('or', 'CONJ'), ('inadequate', 'ADJ'), ('and', 'CONJ'), ('often', 'ADV'), ('ambiguous', 'ADJ'), (\"''\", '.'), ('.', '.')]\n",
      "[('It', 'PRON'), ('recommended', 'VERB'), ('that', 'ADP'), ('Fulton', 'NOUN'), ('legislators', 'NOUN'), ('act', 'VERB'), ('``', '.'), ('to', 'PRT'), ('have', 'VERB'), ('these', 'DET'), ('laws', 'NOUN'), ('studied', 'VERB'), ('and', 'CONJ'), ('revised', 'VERB'), ('to', 'ADP'), ('the', 'DET'), ('end', 'NOUN'), ('of', 'ADP'), ('modernizing', 'VERB'), ('and', 'CONJ'), ('improving', 'VERB'), ('them', 'PRON'), (\"''\", '.'), ('.', '.')]\n",
      "[('The', 'DET'), ('grand', 'ADJ'), ('jury', 'NOUN'), ('commented', 'VERB'), ('on', 'ADP'), ('a', 'DET'), ('number', 'NOUN'), ('of', 'ADP'), ('other', 'ADJ'), ('topics', 'NOUN'), (',', '.'), ('among', 'ADP'), ('them', 'PRON'), ('the', 'DET'), ('Atlanta', 'NOUN'), ('and', 'CONJ'), ('Fulton', 'NOUN'), ('County', 'NOUN'), ('purchasing', 'VERB'), ('departments', 'NOUN'), ('which', 'DET'), ('it', 'PRON'), ('said', 'VERB'), ('``', '.'), ('are', 'VERB'), ('well', 'ADV'), ('operated', 'VERB'), ('and', 'CONJ'), ('follow', 'VERB'), ('generally', 'ADV'), ('accepted', 'VERB'), ('practices', 'NOUN'), ('which', 'DET'), ('inure', 'VERB'), ('to', 'ADP'), ('the', 'DET'), ('best', 'ADJ'), ('interest', 'NOUN'), ('of', 'ADP'), ('both', 'DET'), ('governments', 'NOUN'), (\"''\", '.'), ('.', '.')]\n",
      "[('Merger', 'NOUN'), ('proposed', 'VERB')]\n",
      "[('However', 'ADV'), (',', '.'), ('the', 'DET'), ('jury', 'NOUN'), ('said', 'VERB'), ('it', 'PRON'), ('believes', 'VERB'), ('``', '.'), ('these', 'DET'), ('two', 'NUM'), ('offices', 'NOUN'), ('should', 'VERB'), ('be', 'VERB'), ('combined', 'VERB'), ('to', 'PRT'), ('achieve', 'VERB'), ('greater', 'ADJ'), ('efficiency', 'NOUN'), ('and', 'CONJ'), ('reduce', 'VERB'), ('the', 'DET'), ('cost', 'NOUN'), ('of', 'ADP'), ('administration', 'NOUN'), (\"''\", '.'), ('.', '.')]\n",
      "[('The', 'DET'), ('City', 'NOUN'), ('Purchasing', 'VERB'), ('Department', 'NOUN'), (',', '.'), ('the', 'DET'), ('jury', 'NOUN'), ('said', 'VERB'), (',', '.'), ('``', '.'), ('is', 'VERB'), ('lacking', 'VERB'), ('in', 'ADP'), ('experienced', 'VERB'), ('clerical', 'ADJ'), ('personnel', 'NOUN'), ('as', 'ADP'), ('a', 'DET'), ('result', 'NOUN'), ('of', 'ADP'), ('city', 'NOUN'), ('personnel', 'NOUN'), ('policies', 'NOUN'), (\"''\", '.'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "dataset = brown.tagged_sents(tagset='universal')\n",
    "print(len(dataset))\n",
    "for sent in dataset[:10]:\n",
    "    print(sent)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, string, unicodedata\n",
    "import nltk\n",
    "import contractions\n",
    "import inflect\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "\n",
    "\n",
    "def remove_non_ascii(words):\n",
    "    \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def to_lowercase(words):\n",
    "    \"\"\"Convert all characters to lowercase from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = word.lower()\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def remove_punctuation(words):\n",
    "    \"\"\"Remove punctuation from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = re.sub(r'[^\\w\\s]', '', word)\n",
    "        if new_word != '':\n",
    "            new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def replace_numbers(words):\n",
    "    \"\"\"Replace all interger occurrences in list of tokenized words with textual representation\"\"\"\n",
    "    p = inflect.engine()\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word.isdigit():\n",
    "            new_word = p.number_to_words(word)\n",
    "            new_words.append(new_word)\n",
    "        else:\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "def remove_stopwords(words):\n",
    "    \"\"\"Remove stop words from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word not in stopwords.words('english'):\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "def stem_words(words):\n",
    "    \"\"\"Stem words in list of tokenized words\"\"\"\n",
    "    stemmer = LancasterStemmer()\n",
    "    stems = []\n",
    "    for word in words:\n",
    "        stem = stemmer.stem(word)\n",
    "        stems.append(stem)\n",
    "    return stems\n",
    "\n",
    "def lemmatize_verbs(words):\n",
    "    \"\"\"Lemmatize verbs in list of tokenized words\"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmas = []\n",
    "    for word in words:\n",
    "        lemma = lemmatizer.lemmatize(word, pos='v')\n",
    "        lemmas.append(lemma)\n",
    "    return lemmas\n",
    "\n",
    "def normalize(words):\n",
    "    words = remove_non_ascii(words)\n",
    "    words = to_lowercase(words)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS tags are:\n",
      "['NUM', 'DET', 'ADJ', 'ADV', 'PRT', '.', 'CONJ', 'VERB', 'X', 'ADP', 'NOUN', 'PRON']\n",
      "Number of POS tags are: 12\n"
     ]
    }
   ],
   "source": [
    "_, tags = zip(*[pair for sent in dataset for pair in sent])\n",
    "universal_tags = list(set(tags))\n",
    "del tags\n",
    "print(f\"POS tags are:\\n{universal_tags}\\nNumber of POS tags are: {len(universal_tags)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tag import untag"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seperating sentences and their tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(57340, 57340)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = [untag(sent) for sent in dataset]\n",
    "pos_tags = [list(list(zip(*sent))[1]) for sent in dataset]\n",
    "\n",
    "len(sentences), len(pos_tags)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def show_random_elements(dataset, num_examples=10):\n",
    "    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n",
    "    picks = []\n",
    "    for _ in range(num_examples):\n",
    "        pick = random.randint(0, len(dataset)-1)\n",
    "        while pick in picks:\n",
    "            pick = random.randint(0, len(dataset)-1)\n",
    "        picks.append(pick)\n",
    "    \n",
    "    df = dataset.iloc[picks]\n",
    "    display(HTML(df.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentences</th>\n",
       "      <th>pos_tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>41905</th>\n",
       "      <td>[When, Linda, Kay, had, put, up, her, breakfast, dishes, and, mopped, her, linoleum, rugs, ,, she, would, go, to, the, Big, House, .]</td>\n",
       "      <td>[ADV, NOUN, NOUN, VERB, VERB, PRT, DET, NOUN, NOUN, CONJ, VERB, DET, NOUN, NOUN, ., PRON, VERB, VERB, ADP, DET, ADJ, NOUN, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7296</th>\n",
       "      <td>[The, critics', campaign, finally, inspired, the, first, major, U.S., exhibit, of, Schiele's, works, .]</td>\n",
       "      <td>[DET, NOUN, NOUN, ADV, VERB, DET, ADJ, ADJ, NOUN, NOUN, ADP, NOUN, NOUN, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1639</th>\n",
       "      <td>[You, can, get, into, an, argument, about, fallout, shelters, at, the, drop, of, a, beer, stein, in, clubs, and, pubs, these, nights, .]</td>\n",
       "      <td>[PRON, VERB, VERB, ADP, DET, NOUN, ADP, NOUN, NOUN, ADP, DET, NOUN, ADP, DET, NOUN, NOUN, ADP, NOUN, CONJ, NOUN, DET, NOUN, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48598</th>\n",
       "      <td>[``, To, me, you'll, always, be, the, girl, o', my, dreams, ,, an', the, sweetest, flower, that, grows, '', .]</td>\n",
       "      <td>[., ADP, PRON, PRT, ADV, VERB, DET, NOUN, ADP, DET, NOUN, ., CONJ, DET, ADJ, NOUN, DET, VERB, ., .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18024</th>\n",
       "      <td>[Another, poultice, was, made, from, the, inner, bark, of, the, elm, tree, ,, steeped, in, water, until, it, formed, a, sticky, ,, gummy, solution, .]</td>\n",
       "      <td>[DET, NOUN, VERB, VERB, ADP, DET, ADJ, NOUN, ADP, DET, NOUN, NOUN, ., VERB, ADP, NOUN, ADP, PRON, VERB, DET, ADJ, ., ADJ, NOUN, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16049</th>\n",
       "      <td>[When, a, husband, is, sexually, selfish, and, heedless, of, his, wife's, desires, ,, she, is, cheated, of, the, fulfillment, and, pleasure, nature, intended, for, her, .]</td>\n",
       "      <td>[ADV, DET, NOUN, VERB, ADV, ADJ, CONJ, ADJ, ADP, DET, NOUN, NOUN, ., PRON, VERB, VERB, ADP, DET, NOUN, CONJ, NOUN, NOUN, VERB, ADP, PRON, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14628</th>\n",
       "      <td>[These, include, :, leaves, of, absences, ,, illnesses, ,, and, layoffs, .]</td>\n",
       "      <td>[DET, VERB, ., NOUN, ADP, NOUN, ., NOUN, ., CONJ, NOUN, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9144</th>\n",
       "      <td>[More, and, more, boats, move, overland, on, wheels, (, 1.8, million, trailers, are, now, in, use, ), and, Midwesterners, taking, long, weekends, can, travel, south, with, their, craft, .]</td>\n",
       "      <td>[ADJ, CONJ, ADJ, NOUN, VERB, ADV, ADP, NOUN, ., NUM, NUM, NOUN, VERB, ADV, ADP, NOUN, ., CONJ, NOUN, VERB, ADJ, NOUN, VERB, VERB, NOUN, ADP, DET, NOUN, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48265</th>\n",
       "      <td>[Why, had, I, registered, ?, ?]</td>\n",
       "      <td>[ADV, VERB, PRON, VERB, ., .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6717</th>\n",
       "      <td>[At, Khrushchev's, door, ,, therefore, ,, can, be, placed, the, primary, blame, but, also, at, fault, are, those, who, permitted, themselves, to, be, intimidated, .]</td>\n",
       "      <td>[ADP, NOUN, NOUN, ., ADV, ., VERB, VERB, VERB, DET, ADJ, NOUN, CONJ, ADV, ADP, NOUN, VERB, DET, PRON, VERB, PRON, PRT, VERB, VERB, .]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.DataFrame.from_dict({\n",
    "    \"sentences\" : sentences,\n",
    "    \"pos_tags\" : pos_tags\n",
    "})\n",
    "show_random_elements(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import urllib.request\n",
    "# import zipfile\n",
    "\n",
    "# url = \"https://dl.fbaipublicfiles.com/fasttext/vectors-english/crawl-300d-2M.vec.zip\"\n",
    "# filename = \"crawl-300d-2M.vec.zip\"\n",
    "\n",
    "# # Download the zip file\n",
    "# urllib.request.urlretrieve(url, filename)\n",
    "\n",
    "# # Extract the contents of the zip file\n",
    "# with zipfile.ZipFile(filename, 'r') as zip_ref:\n",
    "#     zip_ref.extractall('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"crawl-300d-2M.vec\", \"r\") as vectors_file:\n",
    "    file_content = vectors_file.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1st line in the above file, file_content tells the number of words for which the file has embeddings followed by the dimensions of the embeddings.\n",
      "1999995 300\n"
     ]
    }
   ],
   "source": [
    "print(f\"1st line in the above file, file_content tells the number of words for which the file has embeddings followed by the dimensions of the embeddings.\")\n",
    "print(file_content[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The rest of the lines have a word at every line followed by 300 numbers for the 300d word embeddings of the word, a sample is shown below.\n",
      "Wreck-It -0.2941 -0.0440 0.6280 0.3474 0.5014 -1.0144 0.5379 -0.2273 -0.0119 -0.2189 0.0294 1.2556 -0.1439 0.2246 0.0659 -0.0031 0.1349 0.3719 -0.5852 -0.3380 0.4387 0.4592 0.3349 -0.4750 -0.3464 0.6705 -0.5252 0.3864 -0.1200 0.1228 -0.1977 0.9876 0.3050 0.2154 0.8681 -0.2432 0.0239 -0.2227 -0.3440 0.5673 0.7380 0.1509 0.0166 0.6669 -0.4747 0.2333 0.0375 0.7208 -0.4009 -0.3855 0.2377 0.2651 1.1744 -0.5010 -0.0412 0.3804 0.2547 0.2091 -0.4663 0.0147 -0.5130 -0.1848 0.1726 0.3357 0.8713 0.7080 0.8966 -0.8118 0.0568 -0.5825 -0.6531 0.2765 -0.1806 0.2960 0.4276 0.3042 -0.2592 0.2431 -0.3561 -0.0708 0.5813 0.2686 0.4225 -0.1953 0.4349 -0.3406 0.0300 -0.5246 -0.1651 -0.4520 0.1233 0.1704 -0.0815 0.2655 -0.2337 -0.4135 -0.0224 0.5407 0.0084 -0.2461 -0.2733 0.2132 -0.0585 -0.1620 0.3127 -0.3421 -0.7229 0.6596 0.4628 -0.2809 -0.4614 0.0984 0.4248 -0.7097 -0.6926 -0.2865 -0.2062 0.1679 0.5525 1.1290 0.4129 -0.0405 -0.3346 0.5831 0.0978 0.5933 0.3822 0.0782 0.5366 0.0893 0.1838 -0.3765 -0.1353 -0.2665 0.2884 0.0338 0.3135 0.3790 0.0613 -1.1838 0.7059 -0.0032 -0.7622 -0.0853 0.1068 0.1581 0.0458 -0.3253 0.5998 -0.0966 -0.3853 -0.3784 -0.3247 -0.2576 -0.1264 -0.1625 -0.4903 0.0068 0.0494 -0.2624 0.4401 -0.3206 -0.2813 -0.8219 -0.0642 -0.3398 -0.4013 0.0047 0.5654 -0.0131 0.2542 0.7568 -0.0900 -0.1934 -0.2259 0.0175 -0.1163 -0.5619 0.3204 -0.2209 -0.4131 -0.3155 0.3351 -0.3892 0.2301 0.3221 0.6421 0.0404 0.2047 -0.1865 -0.0301 -0.1017 0.8583 -0.2083 0.0215 -0.2059 0.3847 -0.4732 -0.0813 0.3973 0.0370 0.7586 -0.2466 -0.1258 -0.2439 0.1726 -0.6203 0.2324 0.3644 -0.1661 0.4178 0.7222 0.0595 0.1415 -0.2501 -0.0563 -0.4324 -0.3738 -0.6433 0.5802 0.0708 0.1372 0.2733 0.5993 0.0704 -0.1506 -0.1654 -0.1691 0.2999 0.9661 0.1036 0.0590 -0.2831 0.2441 0.3117 -0.5550 -0.5624 0.3262 -0.1758 0.4945 0.0192 0.2376 0.0736 0.2961 0.2882 -0.0147 -0.0382 -0.5705 0.0426 -0.4484 0.3738 0.3013 0.3638 -0.5417 -0.2380 -0.2479 0.5896 -0.1416 0.1045 -0.3489 0.3540 0.4003 0.2967 -0.8028 -0.4608 0.4359 0.4934 0.4434 0.3757 0.0643 0.4889 -0.2141 -0.2113 0.0666 -0.2011 0.4473 -0.0421 0.0789 0.2993 0.1816 0.7716 -0.2896 -0.5658 -0.3240 0.6860 -0.2534 0.8589 0.4585 -0.0724 -0.0139 0.6477 -0.0292 0.0980 -0.2820 0.2774 0.7660 -0.1910 -0.1462 -0.1695 -0.3359\n"
     ]
    }
   ],
   "source": [
    "print(f\"The rest of the lines have a word at every line followed by 300 numbers for the 300d word embeddings of the word, a sample is shown below.\")\n",
    "print(file_content[424242])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making a dictionary mapping from the word to the embedding, called `word2embedding`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2embedding = dict()\n",
    "for line in file_content[1:]:\n",
    "    word2embedding[line.split()[0]] = torch.tensor(list(map(float, line.split()[1:])))\n",
    "    \n",
    "vocab = list(word2embedding.keys())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As `file_content` is a large variable that is no longer needed, we delete it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "del file_content"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking for duplication is vocabulary `vocab`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1999995, 1999995)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab), len(set(vocab))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that same word can have different embeddings for its capitalized and non-capitalized versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 3.4100e-02,  2.3550e-01, -6.3600e-02, -2.6600e-02,  3.9000e-02,\n",
       "         1.8200e-02,  1.5850e-01, -3.9070e-01, -4.3700e-02, -4.8400e-02,\n",
       "        -1.0740e-01,  8.3800e-02, -2.5350e-01, -3.0200e-02, -1.5200e-01,\n",
       "        -2.3300e-02,  2.1290e-01, -1.2400e-02, -5.9100e-02,  4.3200e-02,\n",
       "        -2.9000e-03, -6.3700e-02,  8.1700e-02, -5.1700e-02,  5.1900e-02,\n",
       "         4.9900e-02, -1.5120e-01, -1.5300e-02, -5.8800e-02, -3.3890e-01,\n",
       "         3.1600e-02,  2.5000e-03,  1.7000e-02,  2.0200e-01,  2.9000e-02,\n",
       "        -2.1000e-03, -2.6000e-03,  5.3000e-02,  1.3900e-02,  1.2660e-01,\n",
       "         5.7500e-02, -2.5300e-02, -7.8000e-02, -1.8300e-02, -1.4100e-01,\n",
       "        -8.2000e-03,  4.2100e-02, -5.5000e-03, -1.9000e-03, -7.8200e-02,\n",
       "         2.3600e-02,  3.4040e-01, -1.3570e-01, -9.4500e-02, -2.3200e-02,\n",
       "         4.2600e-02,  5.9800e-02,  2.1380e-01,  1.0600e-02, -8.6500e-02,\n",
       "         2.4990e-01,  2.7580e-01,  1.0400e-01,  1.2040e-01, -1.4020e-01,\n",
       "        -1.0300e-02, -2.1500e-01,  2.8000e-03,  1.2580e-01,  1.1140e-01,\n",
       "        -3.1460e-01, -7.2900e-02, -7.0200e-02,  5.8500e-02, -1.1880e-01,\n",
       "         2.7060e-01, -1.6350e-01,  2.0660e-01,  3.0180e-01, -1.4190e-01,\n",
       "         1.3858e+00, -6.3400e-02,  3.0500e-02, -5.9600e-02,  1.5500e-01,\n",
       "         3.3100e-02,  1.0880e-01,  1.2620e-01, -8.1300e-02, -1.0400e-02,\n",
       "         2.9500e-02,  8.1410e-01,  4.3400e-02,  3.1000e-03,  1.6690e-01,\n",
       "         1.5860e-01,  6.7020e-01,  2.9540e-01,  6.1400e-02, -2.2500e-02,\n",
       "         1.8100e-02, -5.6400e-02, -2.8800e-02,  1.5880e-01, -1.5910e-01,\n",
       "        -3.9000e-03,  2.2260e-01,  1.1670e-01, -5.5400e-02,  2.3440e-01,\n",
       "        -5.6300e-02, -7.2190e-01,  1.5200e-02, -7.3900e-02, -1.4840e-01,\n",
       "         1.0640e-01, -2.2190e-01,  4.5800e-02,  1.8970e-01,  5.7300e-02,\n",
       "        -2.4300e-02, -8.2600e-02,  1.4700e-02,  2.2200e-02,  7.2300e-02,\n",
       "        -6.2000e-02,  3.2300e-02,  4.3300e-02,  6.9220e-01, -3.7500e-02,\n",
       "         1.3460e-01, -2.0000e-04,  1.1120e-01, -1.0060e-01, -6.3000e-03,\n",
       "        -1.4100e-02,  2.3790e-01,  4.8100e-02, -8.8200e-02, -6.3890e-01,\n",
       "         1.4750e-01,  1.3000e-03,  4.3800e-02,  8.8500e-02,  1.0900e-02,\n",
       "        -1.5610e-01,  1.1270e-01,  4.6000e-03, -9.5100e-02, -2.8900e-02,\n",
       "        -9.1700e-02,  4.5500e-02, -9.6000e-03, -3.0500e-02, -1.0090e-01,\n",
       "        -7.0600e-02,  8.5100e-02,  1.1090e-01, -3.8700e-02, -9.4900e-02,\n",
       "         3.6790e-01,  3.0900e-02,  6.3100e-02,  6.0200e-02, -1.8900e-02,\n",
       "        -1.2000e-03, -7.1000e-03, -5.7900e-02, -1.3130e-01,  4.1000e-03,\n",
       "        -6.2300e-02,  1.0660e-01, -1.3000e-03, -4.4900e-02,  3.1560e-01,\n",
       "         8.9000e-02, -8.4200e-02,  2.1000e-02,  2.4200e-02, -1.1300e-02,\n",
       "         5.8400e-02,  6.4000e-02,  1.1000e-03, -5.2100e-02,  5.4300e-02,\n",
       "         4.6300e-02,  1.9970e-01,  2.7700e-02,  6.5900e-02,  4.9300e-02,\n",
       "        -4.3400e-02,  2.0000e-02,  7.8000e-03, -9.6000e-02,  1.8000e-02,\n",
       "         9.9000e-03,  1.3640e-01,  1.0950e-01,  1.0040e-01,  1.1670e-01,\n",
       "        -1.3660e-01,  3.0400e-02, -2.1100e-02,  1.5480e-01, -8.4600e-02,\n",
       "         1.7300e-02,  2.2200e-02,  2.3300e-02,  1.8660e-01,  1.8450e-01,\n",
       "         1.6700e-02,  4.4670e-01, -1.7300e-02,  8.6900e-02, -4.6300e-02,\n",
       "        -1.0140e-01,  7.7300e-02, -5.0900e-02, -3.0100e-02, -6.4500e-02,\n",
       "         1.1040e-01,  6.7260e-01,  7.3000e-03,  4.8400e-02, -5.2800e-02,\n",
       "        -1.4290e-01,  5.0300e-02, -2.1300e-02, -8.6000e-02, -1.8100e-02,\n",
       "         8.7000e-02, -7.2000e-02,  4.5100e-02,  9.0100e-02, -3.5500e-02,\n",
       "         1.1100e-02,  1.0770e-01,  1.0000e-02, -1.1120e-01,  7.5200e-02,\n",
       "        -5.5200e-02, -6.7100e-02,  2.9600e-02, -1.2000e-03, -1.6260e-01,\n",
       "         6.4700e-02, -6.8500e-01,  1.9400e-02,  4.7700e-02,  4.6500e-02,\n",
       "         5.4600e-02,  1.0030e-01,  7.5000e-03, -2.4300e-02, -1.3780e-01,\n",
       "        -6.8900e-02,  8.5100e-02, -5.3400e-02, -5.3300e-02,  3.8200e-02,\n",
       "         1.3900e-02, -2.5100e-02,  1.3660e-01,  8.4800e-02,  7.8100e-02,\n",
       "         5.4100e-02, -5.2800e-02,  1.2000e-02, -4.4900e-02,  7.9900e-02,\n",
       "         1.3500e-02, -1.3730e-01, -6.8000e-03,  6.2100e-02, -2.8000e-03,\n",
       "        -4.1700e-02,  1.4930e-01, -8.3900e-02,  2.6000e-03,  1.6100e-02,\n",
       "         2.3720e-01,  3.3440e-01, -1.3660e-01,  5.6000e-02,  2.1400e-02,\n",
       "         3.6400e-02, -4.9000e-03,  1.8000e-02, -1.1410e-01, -1.1790e-01,\n",
       "        -6.0400e-02,  1.0200e-02, -2.8900e-02, -7.2480e-01,  8.2300e-02,\n",
       "        -4.1600e-02, -6.6400e-02,  3.9460e-01, -2.7500e-02,  1.5310e-01])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2embedding['The']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 2.3100e-02,  1.7000e-02,  1.5700e-02, -7.7300e-02,  1.0880e-01,\n",
       "         3.1000e-03, -1.4870e-01, -2.6720e-01, -3.5700e-02, -4.8700e-02,\n",
       "         8.0700e-02,  1.5320e-01, -7.3900e-02, -2.9100e-02, -4.4500e-02,\n",
       "        -1.4000e-03,  1.0140e-01,  1.8600e-02, -2.5300e-02,  2.0000e-02,\n",
       "        -2.6000e-03, -1.7900e-02,  5.0000e-04,  5.4000e-03, -1.3400e-02,\n",
       "         2.3300e-02, -7.5500e-02, -1.5600e-02,  4.1500e-02, -4.9850e-01,\n",
       "         4.1000e-02, -6.1600e-02,  4.7000e-03,  3.2500e-02, -1.6200e-02,\n",
       "        -1.7200e-02,  9.8800e-02,  7.6600e-02, -7.9600e-02, -3.4500e-02,\n",
       "         1.2400e-02, -1.0070e-01, -2.9200e-02, -7.6200e-02, -1.2610e-01,\n",
       "        -5.3100e-02,  4.2400e-02,  1.4400e-02, -6.8300e-02,  2.8590e-01,\n",
       "         3.9900e-02,  2.0100e-02,  3.2400e-01, -6.5600e-02, -4.9700e-02,\n",
       "         9.0000e-03,  9.0200e-02, -1.3800e-02, -4.1200e-02, -2.9700e-02,\n",
       "         3.1390e-01, -1.4280e-01,  1.6600e-02, -2.1900e-02, -5.7500e-02,\n",
       "         1.3590e-01, -1.6550e-01,  1.9000e-03,  3.2300e-02, -1.3000e-03,\n",
       "        -3.0330e-01, -9.1000e-03,  1.4620e-01,  1.8600e-01, -5.2400e-02,\n",
       "         1.8860e-01, -7.3720e-01, -2.4800e-02, -2.0500e-02,  2.2000e-03,\n",
       "         5.9880e-01, -3.5900e-02, -2.6900e-02, -4.8300e-02,  1.0900e-02,\n",
       "        -4.4000e-03,  5.9200e-02,  1.7400e-02,  1.0000e-03, -1.2000e-03,\n",
       "        -2.5100e-02,  4.6200e-01, -4.4300e-02, -3.5000e-02,  1.1500e-02,\n",
       "         1.4960e-01,  3.1250e-01, -9.1000e-03,  2.5170e-01,  6.5400e-02,\n",
       "         2.3700e-02, -4.3200e-02,  9.5200e-02,  6.5000e-02, -2.9320e-01,\n",
       "         6.3000e-02,  2.3600e-02,  3.4000e-02, -1.2000e-03,  8.8900e-02,\n",
       "        -6.0000e-04, -1.7360e-01,  3.7400e-02,  3.1300e-02, -6.1840e-01,\n",
       "         2.8200e-02, -3.8360e-01,  5.8900e-02,  2.4430e-01,  6.0200e-02,\n",
       "         5.7000e-03, -3.8000e-03,  1.3520e-01,  5.3000e-03,  1.9300e-02,\n",
       "        -2.1300e-02,  2.4800e-02,  2.1400e-02,  2.3340e-01, -4.3800e-02,\n",
       "         5.2700e-02,  2.6200e-02,  6.5500e-02, -8.5900e-02,  2.6420e-01,\n",
       "        -3.9300e-02, -1.6300e-02,  6.8100e-02, -1.7500e-02, -1.1580e-01,\n",
       "         9.5000e-02,  4.7500e-02,  6.9000e-03,  5.1640e-01, -2.6000e-03,\n",
       "        -2.5500e-02, -8.0100e-02, -2.6200e-02,  1.1130e-01,  7.9800e-02,\n",
       "        -1.5000e-03,  2.5200e-02, -3.7900e-02, -2.6000e-02, -2.8200e-02,\n",
       "        -4.2000e-02,  4.8200e-02, -1.7500e-02,  2.8200e-02,  4.0000e-02,\n",
       "         3.9980e-01, -1.0540e-01,  7.5500e-02,  1.0270e-01, -1.9900e-02,\n",
       "         3.8100e-02, -3.3300e-02, -3.4200e-02,  2.6700e-02,  8.6500e-02,\n",
       "         2.4000e-03, -9.1000e-03,  1.6300e-02, -2.8700e-02,  3.6400e-02,\n",
       "        -2.0200e-02, -3.6700e-02, -3.5600e-02, -6.1400e-02, -5.5100e-02,\n",
       "         2.6490e-01, -3.7100e-02,  2.0700e-02,  3.6400e-02,  5.1200e-02,\n",
       "        -8.4300e-02, -1.3800e-02,  7.1000e-02,  8.4300e-02,  2.9100e-02,\n",
       "        -1.0000e-02,  3.9800e-02, -6.4600e-02, -5.9500e-02, -2.5800e-02,\n",
       "        -2.8200e-02,  3.1100e-02,  1.4700e-02, -4.4900e-02,  2.7600e-02,\n",
       "        -1.1680e-01,  2.1900e-02, -2.3100e-02, -1.6200e-02, -2.8600e-02,\n",
       "         1.2800e-02, -2.5900e-02,  1.5300e-02,  1.0420e-01, -1.2070e-01,\n",
       "        -1.3500e-02,  5.4050e-01, -3.6200e-02,  4.7600e-02, -1.8000e-02,\n",
       "        -7.3500e-02,  3.4000e-03, -2.6000e-03, -5.7000e-03,  3.8000e-02,\n",
       "        -4.0100e-02, -1.0160e-01,  3.4400e-02,  4.0200e-02,  5.1300e-02,\n",
       "        -8.1500e-02,  3.9000e-02,  7.6000e-03,  1.7500e-02,  3.0000e-03,\n",
       "        -7.0700e-02,  1.5000e-02, -1.1740e-01,  2.6600e-02, -7.9500e-02,\n",
       "         1.9880e-01,  9.7800e-02, -5.8700e-02, -5.3300e-02,  2.7300e-02,\n",
       "         4.4200e-02, -4.6300e-02, -7.0800e-02,  1.7600e-02, -9.9400e-02,\n",
       "         8.4600e-02,  3.6200e-01, -2.0700e-02,  2.5600e-02, -1.4500e-02,\n",
       "         3.0900e-02,  8.2000e-03,  4.2000e-03, -3.1400e-02,  1.1960e-01,\n",
       "        -3.4600e-02,  3.8600e-02, -3.6800e-02, -3.3300e-02, -3.2000e-03,\n",
       "        -4.8000e-03, -6.0000e-04,  5.0900e-02, -2.3200e-02,  1.1830e-01,\n",
       "        -1.3140e-01,  1.4900e-02,  7.6200e-02, -1.6100e-02,  1.6000e-02,\n",
       "         3.9000e-02, -1.9220e-01,  3.1000e-03, -6.6600e-02,  5.9300e-02,\n",
       "        -6.2100e-02,  4.2100e-02,  3.2800e-02, -9.0100e-02, -1.5900e-02,\n",
       "         1.0150e-01,  6.1640e-01, -6.5000e-02,  1.2410e-01,  5.9000e-03,\n",
       "         6.5300e-02, -3.8600e-02,  1.6600e-02,  4.0300e-02,  1.6900e-02,\n",
       "        -8.0000e-04,  5.2000e-03, -3.6300e-02, -2.5080e-01,  1.2520e-01,\n",
       "        -1.0080e-01, -3.0800e-02,  7.4400e-02, -1.1180e-01,  9.6300e-02])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2embedding['the']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Looking at data type of embedding to be sure\n",
    "word2embedding['the'].dtype"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking if we have word embeddings for individual letters and symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have following individual characters:\n",
      "['!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', ':', ';', '<', '=', '>', '?', '@', '[', '\\\\', ']', '^', '_', '`', '{', '|', '}', '~', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z']\n",
      "from which only characters ['_', '`'] donot have embeddings in word2embedding.\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "list_of_all = [x for x in string.punctuation] + [x for x in string.digits] + [x for x in string.ascii_letters]\n",
    "not_present = list()\n",
    "for item in list_of_all:\n",
    "    if item not in vocab:\n",
    "        not_present.append(item)\n",
    "\n",
    "print(f\"We have following individual characters:\\n{list_of_all}\\nfrom which only characters {not_present} donot have embeddings in word2embedding.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:5\n"
     ]
    }
   ],
   "source": [
    "# Make device agnostic code\n",
    "device = \"cuda:5\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 1\n",
    "X = list()\n",
    "y = list()\n",
    "\n",
    "null_token = \"[NULL_TOKEN]\"\n",
    "for sentence, pos_tags in zip(sentences, pos_tags):\n",
    "    for i in range(len(sentence)):\n",
    "        y.append(pos_tags[i])\n",
    "        padded_sentence = [null_token] * window_size + sentence + [null_token] * window_size\n",
    "        X.append(padded_sentence[i : i + 2*window_size + 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([['[NULL_TOKEN]', 'The', 'Fulton'],\n",
       "  ['The', 'Fulton', 'County'],\n",
       "  ['Fulton', 'County', 'Grand'],\n",
       "  ['County', 'Grand', 'Jury'],\n",
       "  ['Grand', 'Jury', 'said']],\n",
       " ['DET', 'NOUN', 'NOUN', 'ADJ', 'NOUN'])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[:5], y[:5]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting `[NULL_TOKEN]` embeddings as all zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2embedding[null_token] = torch.zeros_like(word2embedding['The'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " torch.float32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2embedding[null_token], word2embedding[null_token].dtype"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code for handling embeddings of words not in the word embeddings file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hey I Yann, how you and how it going ? That interesting: I love to hear more about it.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def decontracted(phrase):\n",
    "    # general\n",
    "    phrase = re.sub(r\"n\\'t\", \"\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \"\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \"\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \"\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \"\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \"\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \"\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \"\", phrase)\n",
    "    phrase = re.sub(r\"\\'$\", \"\", phrase)\n",
    "    return phrase\n",
    "\n",
    "\n",
    "test = \"Hey I'm Yann, how're you and how's it going ? That's interesting: I'd love to hear more about it.\"\n",
    "print(decontracted(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2embedding[\"``\"] = word2embedding[\"'\"]\n",
    "word2embedding[\"''\"] = word2embedding[\"'\"]\n",
    "word2embedding[not_present[0]] = word2embedding[\"-\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# Define a regular expression pattern\n",
    "# Pattern to check if a number only has digits, dollar signs, and commas.\n",
    "\n",
    "non_alphabet_pattern = r'^[^a-zA-Z]+$'\n",
    "non_alphabet_word_list = []\n",
    "for word in list(word2embedding.keys()):\n",
    "    if re.match(non_alphabet_pattern, word):\n",
    "        # print(word)\n",
    "        non_alphabet_word_list.append(word)\n",
    "\n",
    "word2embedding[\"[NON_ALPHABET]\"] = torch.mean(\n",
    "    torch.stack([word2embedding[num] for num in non_alphabet_word_list]), \n",
    "    dim=0\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[',',\n",
       " '.',\n",
       " ')',\n",
       " ':',\n",
       " '\"',\n",
       " '(',\n",
       " '!',\n",
       " '?',\n",
       " '/',\n",
       " '-',\n",
       " '”',\n",
       " '“',\n",
       " '...',\n",
       " ';',\n",
       " \"'\",\n",
       " '–',\n",
       " '1',\n",
       " '2',\n",
       " '’',\n",
       " '&',\n",
       " '3',\n",
       " '…',\n",
       " '—',\n",
       " ']',\n",
       " '|',\n",
       " '4',\n",
       " '[',\n",
       " '5',\n",
       " '10',\n",
       " '*',\n",
       " '‘',\n",
       " '6',\n",
       " '#',\n",
       " '..',\n",
       " '8',\n",
       " '$',\n",
       " '7',\n",
       " '12',\n",
       " '2012',\n",
       " '--',\n",
       " '2013',\n",
       " '2017',\n",
       " '+',\n",
       " '11',\n",
       " '2016',\n",
       " '@',\n",
       " '>',\n",
       " '20',\n",
       " '....',\n",
       " '2011',\n",
       " '2014',\n",
       " '15',\n",
       " '9',\n",
       " '2015',\n",
       " '=',\n",
       " '2010',\n",
       " '16',\n",
       " '13',\n",
       " '14',\n",
       " '•',\n",
       " '30',\n",
       " '18',\n",
       " '2009',\n",
       " '17',\n",
       " '2008',\n",
       " '%',\n",
       " '»',\n",
       " '0',\n",
       " '25',\n",
       " '21',\n",
       " '22',\n",
       " '19',\n",
       " '24',\n",
       " '2007',\n",
       " '23',\n",
       " '~',\n",
       " '50',\n",
       " '2006',\n",
       " '26',\n",
       " '28',\n",
       " '100',\n",
       " '27',\n",
       " '2005',\n",
       " '<',\n",
       " '29',\n",
       " '40',\n",
       " '·',\n",
       " '2004',\n",
       " '.....',\n",
       " '31',\n",
       " '}',\n",
       " '�',\n",
       " '\\\\',\n",
       " '{',\n",
       " '2003',\n",
       " '2000',\n",
       " '£',\n",
       " '«',\n",
       " '→',\n",
       " '^',\n",
       " '60',\n",
       " '32',\n",
       " '2002',\n",
       " '®',\n",
       " '35',\n",
       " '2001',\n",
       " '01',\n",
       " '07',\n",
       " '02',\n",
       " '🙂',\n",
       " '08',\n",
       " '09',\n",
       " '06',\n",
       " '......',\n",
       " '03',\n",
       " '05',\n",
       " '04',\n",
       " '45',\n",
       " '80',\n",
       " '36',\n",
       " '200',\n",
       " '←',\n",
       " '34',\n",
       " '33',\n",
       " '70',\n",
       " '€',\n",
       " '™',\n",
       " '1999',\n",
       " '42',\n",
       " '90',\n",
       " '500',\n",
       " '38',\n",
       " '48',\n",
       " '1998',\n",
       " '37',\n",
       " '44',\n",
       " '©',\n",
       " '55',\n",
       " '300',\n",
       " '39',\n",
       " '1997',\n",
       " '41',\n",
       " '1996',\n",
       " '52',\n",
       " '46',\n",
       " '\\ufeff',\n",
       " '1995',\n",
       " '00',\n",
       " '43',\n",
       " '1994',\n",
       " '47',\n",
       " '1990',\n",
       " '\\xad',\n",
       " '49',\n",
       " '150',\n",
       " '54',\n",
       " '75',\n",
       " '51',\n",
       " '56',\n",
       " '400',\n",
       " '65',\n",
       " '1992',\n",
       " '\\u200b',\n",
       " '1000',\n",
       " '120',\n",
       " '1993',\n",
       " '101',\n",
       " '64',\n",
       " '−',\n",
       " '53',\n",
       " '1991',\n",
       " '58',\n",
       " '\\x93',\n",
       " '\\x94',\n",
       " '57',\n",
       " '1989',\n",
       " '›',\n",
       " '59',\n",
       " '1980',\n",
       " '2.0',\n",
       " '250',\n",
       " '\\x97',\n",
       " '.......',\n",
       " '72',\n",
       " '1984',\n",
       " '1988',\n",
       " '×',\n",
       " '1986',\n",
       " '110',\n",
       " '62',\n",
       " '1970',\n",
       " '1987',\n",
       " '1985',\n",
       " '66',\n",
       " '85',\n",
       " '99',\n",
       " '95',\n",
       " '§',\n",
       " '68',\n",
       " '---',\n",
       " '61',\n",
       " '600',\n",
       " '63',\n",
       " '800',\n",
       " '1983',\n",
       " '360',\n",
       " '1982',\n",
       " '2018',\n",
       " '1979',\n",
       " '69',\n",
       " '67',\n",
       " '1981',\n",
       " '88',\n",
       " '96',\n",
       " '1969',\n",
       " '1978',\n",
       " '1975',\n",
       " '1.5',\n",
       " '►',\n",
       " '74',\n",
       " '78',\n",
       " '76',\n",
       " '1976',\n",
       " '71',\n",
       " '♥',\n",
       " '2.5',\n",
       " '98',\n",
       " '82',\n",
       " '1968',\n",
       " '84',\n",
       " '1977',\n",
       " '1973',\n",
       " '😉',\n",
       " '1960',\n",
       " '77',\n",
       " '130',\n",
       " '73',\n",
       " '1972',\n",
       " '°',\n",
       " '180',\n",
       " '1967',\n",
       " '1974',\n",
       " '140',\n",
       " '92',\n",
       " '1,000',\n",
       " '86',\n",
       " '102',\n",
       " '350',\n",
       " '81',\n",
       " '91',\n",
       " '1971',\n",
       " '1964',\n",
       " '94',\n",
       " '3.5',\n",
       " '160',\n",
       " '1965',\n",
       " '1950',\n",
       " '125',\n",
       " '89',\n",
       " '1.0',\n",
       " '10,000',\n",
       " '83',\n",
       " '93',\n",
       " '79',\n",
       " '104',\n",
       " '87',\n",
       " '........',\n",
       " '97',\n",
       " '1963',\n",
       " '105',\n",
       " '\\x95',\n",
       " '1966',\n",
       " '3.0',\n",
       " '111',\n",
       " '1-2',\n",
       " '2-3',\n",
       " '700',\n",
       " '106',\n",
       " '108',\n",
       " '112',\n",
       " '\\x96',\n",
       " '1945',\n",
       " '1961',\n",
       " '1.',\n",
       " '1940',\n",
       " '103',\n",
       " '4.5',\n",
       " '220',\n",
       " '1900',\n",
       " '210',\n",
       " '3-4',\n",
       " '2.',\n",
       " '2020',\n",
       " '1962',\n",
       " '●',\n",
       " '1956',\n",
       " '115',\n",
       " '4.0',\n",
       " '，',\n",
       " '128',\n",
       " '240',\n",
       " '。',\n",
       " '1920',\n",
       " '114',\n",
       " '900',\n",
       " '170',\n",
       " '1930',\n",
       " '123',\n",
       " '\\x92',\n",
       " '„',\n",
       " '1.1',\n",
       " '1500',\n",
       " '1948',\n",
       " 'Â',\n",
       " '450',\n",
       " '5.0',\n",
       " '.........',\n",
       " '107',\n",
       " '1941',\n",
       " '1.2',\n",
       " '1955',\n",
       " '1959',\n",
       " '116',\n",
       " '1939',\n",
       " '1944',\n",
       " '±',\n",
       " '122',\n",
       " '135',\n",
       " '★',\n",
       " '3000',\n",
       " '100,000',\n",
       " '1942',\n",
       " 'à',\n",
       " '■',\n",
       " '1800',\n",
       " '1933',\n",
       " '118',\n",
       " '-1',\n",
       " '1947',\n",
       " '109',\n",
       " '1911',\n",
       " '´',\n",
       " '1958',\n",
       " '000',\n",
       " '0.5',\n",
       " '1957',\n",
       " '124',\n",
       " '1934',\n",
       " '230',\n",
       " '1917',\n",
       " '1954',\n",
       " '5,000',\n",
       " '113',\n",
       " '201',\n",
       " '121',\n",
       " '1951',\n",
       " '😀',\n",
       " '2,000',\n",
       " '2.1',\n",
       " '5000',\n",
       " '1953',\n",
       " '1914',\n",
       " '½',\n",
       " '126',\n",
       " '1929',\n",
       " '132',\n",
       " '1910',\n",
       " '320',\n",
       " '365',\n",
       " '1949',\n",
       " '1943',\n",
       " '1937',\n",
       " '175',\n",
       " '′',\n",
       " '1938',\n",
       " '↑',\n",
       " '1200',\n",
       " '911',\n",
       " '1936',\n",
       " '144',\n",
       " '1952',\n",
       " '1946',\n",
       " '2.2',\n",
       " '117',\n",
       " '1-3',\n",
       " '225',\n",
       " '190',\n",
       " '‐',\n",
       " '202',\n",
       " '119',\n",
       " '1918',\n",
       " '″',\n",
       " '750',\n",
       " '..........',\n",
       " '1935',\n",
       " '20,000',\n",
       " '3.',\n",
       " '、',\n",
       " '145',\n",
       " '\\u200e',\n",
       " '1912',\n",
       " '¶',\n",
       " '1.3',\n",
       " '1.4',\n",
       " '1915',\n",
       " '1913',\n",
       " 'и',\n",
       " '127',\n",
       " '134',\n",
       " '1919',\n",
       " '50,000',\n",
       " '1928',\n",
       " '2-1',\n",
       " '3,000',\n",
       " '260',\n",
       " '4-5',\n",
       " '205',\n",
       " '212',\n",
       " '131',\n",
       " '136',\n",
       " '650',\n",
       " '1916',\n",
       " '1927',\n",
       " '270',\n",
       " '330',\n",
       " '1932',\n",
       " '3.2',\n",
       " '1925',\n",
       " '155',\n",
       " 'â',\n",
       " '138',\n",
       " '3.1',\n",
       " '165',\n",
       " '5.5',\n",
       " '1922',\n",
       " '204',\n",
       " 'в',\n",
       " '152',\n",
       " '142',\n",
       " '310',\n",
       " '550',\n",
       " '7.5',\n",
       " '133',\n",
       " '4.2',\n",
       " '6.5',\n",
       " '1890',\n",
       " '1905',\n",
       " '1850',\n",
       " '129',\n",
       " '1926',\n",
       " '1931',\n",
       " '¬',\n",
       " '1-',\n",
       " '1908',\n",
       " '1.6',\n",
       " '1921',\n",
       " '：',\n",
       " '1600',\n",
       " '280',\n",
       " '2019',\n",
       " '5.1',\n",
       " '302',\n",
       " '1923',\n",
       " '141',\n",
       " '206',\n",
       " '215',\n",
       " '1901',\n",
       " '1880',\n",
       " '―',\n",
       " '420',\n",
       " '1906',\n",
       " '208',\n",
       " '154',\n",
       " '2.3',\n",
       " '137',\n",
       " '30,000',\n",
       " '4.1',\n",
       " '151',\n",
       " '1909',\n",
       " '146',\n",
       " '3-5',\n",
       " '148',\n",
       " '2.4',\n",
       " '2-',\n",
       " '156',\n",
       " '255',\n",
       " '185',\n",
       " '162',\n",
       " '1865',\n",
       " '256',\n",
       " '1860',\n",
       " '1907',\n",
       " '1.8',\n",
       " '1924',\n",
       " '1861',\n",
       " '4000',\n",
       " '1862',\n",
       " '6.0',\n",
       " '147',\n",
       " '143',\n",
       " '₹',\n",
       " '1870',\n",
       " '2-0',\n",
       " '・',\n",
       " '²',\n",
       " '5-6',\n",
       " '2-4',\n",
       " '1904',\n",
       " '301',\n",
       " '139',\n",
       " '168',\n",
       " '0.1',\n",
       " '1903',\n",
       " '216',\n",
       " '4.3',\n",
       " '222',\n",
       " '203',\n",
       " '...........',\n",
       " '4-6',\n",
       " '8.5',\n",
       " '¢',\n",
       " '1863',\n",
       " '158',\n",
       " '410',\n",
       " '1898',\n",
       " '1864',\n",
       " '192',\n",
       " '1902',\n",
       " '1-0',\n",
       " '304',\n",
       " '3-1',\n",
       " '2500',\n",
       " '211',\n",
       " '149',\n",
       " '214',\n",
       " '¦',\n",
       " '6-8',\n",
       " '164',\n",
       " '153',\n",
       " '\\uf0a7',\n",
       " '172',\n",
       " '325',\n",
       " '195',\n",
       " '3.3',\n",
       " ',,',\n",
       " '4.',\n",
       " '340',\n",
       " '161',\n",
       " '1895',\n",
       " '1812',\n",
       " '3-0',\n",
       " '404',\n",
       " '166',\n",
       " '（',\n",
       " '1-4',\n",
       " '401',\n",
       " '‹',\n",
       " '275',\n",
       " '510',\n",
       " '1,500',\n",
       " '224',\n",
       " '1-1',\n",
       " '1896',\n",
       " '157',\n",
       " '）',\n",
       " '4,000',\n",
       " '176',\n",
       " '235',\n",
       " '15,000',\n",
       " '182',\n",
       " '1899',\n",
       " '1100',\n",
       " '2.8',\n",
       " '¡',\n",
       " '1300',\n",
       " '218',\n",
       " '207',\n",
       " '305',\n",
       " '174',\n",
       " '720',\n",
       " '‚',\n",
       " 'на',\n",
       " '308',\n",
       " '163',\n",
       " '\\uf0b7',\n",
       " '159',\n",
       " '171',\n",
       " '1893',\n",
       " '¼',\n",
       " '1700',\n",
       " '-2',\n",
       " '1889',\n",
       " '440',\n",
       " '178',\n",
       " '1888',\n",
       " '8.1',\n",
       " '312',\n",
       " '520',\n",
       " '1897',\n",
       " '¥',\n",
       " '290',\n",
       " '1.7',\n",
       " '380',\n",
       " '167',\n",
       " '40,000',\n",
       " '184',\n",
       " '199',\n",
       " '6-3',\n",
       " '480',\n",
       " '181',\n",
       " '3.4',\n",
       " '512',\n",
       " '1891',\n",
       " '303',\n",
       " '850',\n",
       " '25,000',\n",
       " '‑',\n",
       " '232',\n",
       " '1-5',\n",
       " '188',\n",
       " '252',\n",
       " '1885',\n",
       " '3-2',\n",
       " '1892',\n",
       " '3-',\n",
       " '7.0',\n",
       " '209',\n",
       " '245',\n",
       " '221',\n",
       " '306',\n",
       " '169',\n",
       " '186',\n",
       " '†',\n",
       " '226',\n",
       " '6-1',\n",
       " '1400',\n",
       " '1886',\n",
       " '............',\n",
       " '430',\n",
       " '6000',\n",
       " '1894',\n",
       " '177',\n",
       " '213',\n",
       " '173',\n",
       " '8.0',\n",
       " '1840',\n",
       " '223',\n",
       " '❤',\n",
       " '315',\n",
       " '200,000',\n",
       " '1881',\n",
       " '183',\n",
       " '228',\n",
       " '6-7',\n",
       " '311',\n",
       " '\\x91',\n",
       " '1848',\n",
       " '5.2',\n",
       " '191',\n",
       " '501',\n",
       " '1830',\n",
       " '375',\n",
       " '----',\n",
       " '1871',\n",
       " '↓',\n",
       " '1887',\n",
       " '4.4',\n",
       " '234',\n",
       " '6-2',\n",
       " '2.6',\n",
       " '7.1',\n",
       " '0.2',\n",
       " '196',\n",
       " '187',\n",
       " '194',\n",
       " '242',\n",
       " '4-3',\n",
       " '179',\n",
       " '3.6',\n",
       " '231',\n",
       " '370',\n",
       " '198',\n",
       " '6-4',\n",
       " '¿',\n",
       " '265',\n",
       " '1882',\n",
       " '4-1',\n",
       " '1876',\n",
       " '8-10',\n",
       " '1883',\n",
       " '9-11',\n",
       " '1866',\n",
       " '189',\n",
       " '6,000',\n",
       " '1884',\n",
       " '1875',\n",
       " '1867',\n",
       " '217',\n",
       " '5-7',\n",
       " '402',\n",
       " '460',\n",
       " '♦',\n",
       " '1879',\n",
       " '9.5',\n",
       " '316',\n",
       " '500,000',\n",
       " '2.7',\n",
       " '6.1',\n",
       " '219',\n",
       " '1872',\n",
       " '1776',\n",
       " '405',\n",
       " '285',\n",
       " '193',\n",
       " '5.',\n",
       " '2022',\n",
       " '254',\n",
       " '236',\n",
       " '1868',\n",
       " '197',\n",
       " '-5',\n",
       " '540',\n",
       " '2,500',\n",
       " '0.3',\n",
       " '411',\n",
       " '1878',\n",
       " '530',\n",
       " '1851',\n",
       " '322',\n",
       " '1873',\n",
       " '5-10',\n",
       " '335',\n",
       " '620',\n",
       " '425',\n",
       " '4.6',\n",
       " '1869',\n",
       " '318',\n",
       " '238',\n",
       " '640',\n",
       " '4.7',\n",
       " '1859',\n",
       " '610',\n",
       " '227',\n",
       " '244',\n",
       " '229',\n",
       " '.-',\n",
       " '1820',\n",
       " '1877',\n",
       " '333',\n",
       " '3.8',\n",
       " '10-12',\n",
       " '「',\n",
       " '390',\n",
       " '314',\n",
       " '1857',\n",
       " '4-0',\n",
       " '5.3',\n",
       " '241',\n",
       " '233',\n",
       " '251',\n",
       " '1874',\n",
       " '4-',\n",
       " '2021',\n",
       " '.1',\n",
       " '！',\n",
       " '12,000',\n",
       " '1854',\n",
       " '248',\n",
       " '1855',\n",
       " '502',\n",
       " '3.7',\n",
       " '1,200',\n",
       " '.............',\n",
       " '6.2',\n",
       " '504',\n",
       " '1.9',\n",
       " '920',\n",
       " '321',\n",
       " '307',\n",
       " '0.0',\n",
       " '264',\n",
       " '\\x9d',\n",
       " '246',\n",
       " '243',\n",
       " '247',\n",
       " '1858',\n",
       " '2030',\n",
       " '666',\n",
       " '4.8',\n",
       " '」',\n",
       " '412',\n",
       " '【',\n",
       " '7.2',\n",
       " '1836',\n",
       " '8000',\n",
       " '1856',\n",
       " '262',\n",
       " '8,000',\n",
       " '7-8',\n",
       " '1-10',\n",
       " '295',\n",
       " '60,000',\n",
       " '328',\n",
       " '300,000',\n",
       " '0.8',\n",
       " '10.5',\n",
       " '0.4',\n",
       " '272',\n",
       " '-3',\n",
       " '237',\n",
       " '403',\n",
       " '239',\n",
       " '999',\n",
       " '】',\n",
       " '6-0',\n",
       " '249',\n",
       " '288',\n",
       " '1849',\n",
       " '்',\n",
       " '406',\n",
       " '5.6',\n",
       " '3-6',\n",
       " '560',\n",
       " '▼',\n",
       " '🙁',\n",
       " '258',\n",
       " '☆',\n",
       " '0.6',\n",
       " '5.4',\n",
       " '😛',\n",
       " '630',\n",
       " '2-5',\n",
       " '1841',\n",
       " '324',\n",
       " '1852',\n",
       " '0.05',\n",
       " '1845',\n",
       " '950',\n",
       " '10-15',\n",
       " '1837',\n",
       " '2100',\n",
       " 'с',\n",
       " '1847',\n",
       " '313',\n",
       " '266',\n",
       " '3500',\n",
       " '470',\n",
       " '580',\n",
       " '1853',\n",
       " '1835',\n",
       " '345',\n",
       " '253',\n",
       " '6.3',\n",
       " '710',\n",
       " '276',\n",
       " '1844',\n",
       " '505',\n",
       " '10.0',\n",
       " '660',\n",
       " '408',\n",
       " '1.00',\n",
       " '261',\n",
       " '1846',\n",
       " '416',\n",
       " '268',\n",
       " '▪',\n",
       " '1838',\n",
       " '299',\n",
       " '7000',\n",
       " '1842',\n",
       " '309',\n",
       " '282',\n",
       " '\\x03',\n",
       " '10000',\n",
       " '\\x99',\n",
       " '323',\n",
       " '7,000',\n",
       " '\\x7f',\n",
       " '.5',\n",
       " '\\uf0d8',\n",
       " '1832',\n",
       " '₱',\n",
       " '250,000',\n",
       " '¨',\n",
       " '332',\n",
       " '415',\n",
       " '355',\n",
       " '555',\n",
       " '286',\n",
       " '007',\n",
       " '5.7',\n",
       " '274',\n",
       " '2016-17',\n",
       " 'è',\n",
       " '336',\n",
       " '810',\n",
       " '257',\n",
       " '001',\n",
       " '9.0',\n",
       " '271',\n",
       " '525',\n",
       " '0.7',\n",
       " '7.3',\n",
       " '2012-13',\n",
       " '680',\n",
       " '267',\n",
       " '2-2',\n",
       " '326',\n",
       " '570',\n",
       " '284',\n",
       " '1.25',\n",
       " '777',\n",
       " '³',\n",
       " '1001',\n",
       " '0.9',\n",
       " '4-2',\n",
       " 'é',\n",
       " '2013-14',\n",
       " '263',\n",
       " '414',\n",
       " '1831',\n",
       " '..............',\n",
       " '278',\n",
       " '6.',\n",
       " '10.1',\n",
       " '273',\n",
       " '-4',\n",
       " 'α',\n",
       " '281',\n",
       " '1833',\n",
       " '269',\n",
       " '506',\n",
       " '5-',\n",
       " '150,000',\n",
       " '351',\n",
       " '12.5',\n",
       " '1815',\n",
       " '277',\n",
       " '1839',\n",
       " '1789',\n",
       " '327',\n",
       " '0.01',\n",
       " '259',\n",
       " '292',\n",
       " '✔',\n",
       " '287',\n",
       " '352',\n",
       " '331',\n",
       " '3.9',\n",
       " '5-0',\n",
       " '1834',\n",
       " '283',\n",
       " '342',\n",
       " '1-6',\n",
       " '2011-12',\n",
       " '422',\n",
       " '1843',\n",
       " '0.25',\n",
       " '5-1',\n",
       " '2025',\n",
       " '6.4',\n",
       " '9000',\n",
       " '1790',\n",
       " '601',\n",
       " '8.2',\n",
       " '366',\n",
       " '338',\n",
       " '15-20',\n",
       " '289',\n",
       " '2200',\n",
       " '1810',\n",
       " '317',\n",
       " '2.9',\n",
       " '508',\n",
       " '602',\n",
       " '-----',\n",
       " '357',\n",
       " '298',\n",
       " '2400',\n",
       " '2050',\n",
       " '319',\n",
       " '334',\n",
       " '1825',\n",
       " '1818',\n",
       " '291',\n",
       " '343',\n",
       " 'для',\n",
       " '384',\n",
       " '604',\n",
       " '279',\n",
       " '😦',\n",
       " '1821',\n",
       " '5-8',\n",
       " '356',\n",
       " '820',\n",
       " '296',\n",
       " '730',\n",
       " '1024',\n",
       " '490',\n",
       " '294',\n",
       " ...]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_alphabet_word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-2.3801e-02,  1.8670e-01,  1.9960e-01,  7.0248e-03, -8.0534e-03,\n",
       "         -5.3447e-02,  1.4042e-01, -1.7458e-01,  7.8274e-02,  1.5561e-01,\n",
       "         -4.5317e-02,  7.2526e-01, -1.1935e-01, -5.1581e-02,  2.1298e-03,\n",
       "         -2.8344e-02, -9.0454e-02, -6.5800e-02, -1.1985e-01, -4.8639e-02,\n",
       "         -3.0983e-02,  7.6132e-03,  9.0279e-02,  3.6764e-02,  1.9740e-02,\n",
       "          6.0368e-02,  3.1835e-02,  5.4218e-02,  8.5114e-02, -1.4278e-01,\n",
       "          4.6164e-02,  1.1858e-01, -1.2316e-01,  1.9545e-01,  2.7163e-02,\n",
       "          9.2470e-02, -5.0190e-02, -3.2961e-03,  1.9826e-01, -1.3013e-01,\n",
       "         -2.2385e-03,  2.2055e-01, -2.1592e-02, -1.2384e-01, -9.1814e-02,\n",
       "          9.0625e-02, -9.9048e-02,  1.1771e-01, -9.5122e-02,  2.1546e-01,\n",
       "          1.0088e-01,  3.1869e-01,  8.8194e-01, -1.4286e-02,  6.7230e-03,\n",
       "         -6.0228e-02, -3.5785e-02,  1.1145e-01,  1.1655e-01, -7.1183e-02,\n",
       "         -3.9561e-02,  3.9254e-01,  1.1663e-01,  1.3539e-02,  1.9445e-02,\n",
       "         -1.0353e-01,  5.9360e-02, -4.1234e-02, -2.3428e-02,  1.0930e-01,\n",
       "          8.4847e-02,  3.5891e-02,  1.0019e-02, -3.7004e-01, -6.1089e-02,\n",
       "          3.1997e-01, -1.6114e-01,  1.1043e-01, -4.9789e-01,  8.2268e-02,\n",
       "          1.7488e-01,  1.0496e-01, -8.5231e-02, -2.4321e-03,  3.3878e-01,\n",
       "         -1.2224e-01, -2.7220e-02,  1.1893e-02,  1.8600e-02,  6.3680e-02,\n",
       "         -1.0287e-02,  3.2088e-01,  6.0460e-02,  5.2799e-02,  4.2003e-02,\n",
       "          1.0061e-01,  1.2872e-01,  1.2141e-01,  1.1158e-01, -1.7041e-02,\n",
       "          5.3914e-02,  4.0434e-02, -1.3376e-01,  2.8819e-02,  3.8462e-02,\n",
       "          1.0792e-01, -2.5876e-02, -1.1688e-01,  1.2143e-01,  3.1100e-02,\n",
       "         -2.9611e-01,  3.6133e-01,  1.2190e-01, -1.1884e-01, -1.0723e+00,\n",
       "         -3.4929e-02, -7.6930e-02,  9.0017e-02, -4.5770e-01,  9.0880e-02,\n",
       "         -2.3590e-01, -6.1055e-02,  3.2549e-02, -2.9284e-02, -5.7091e-02,\n",
       "          4.3535e-02, -1.6749e-03, -9.2155e-02,  5.1372e-01, -6.4648e-04,\n",
       "         -2.2834e-02,  8.6854e-02,  1.3130e-01,  1.2228e-01, -3.1912e-01,\n",
       "          6.9936e-02,  4.0078e-01,  9.3487e-02, -1.0091e-03, -7.5265e-01,\n",
       "         -8.3286e-02,  9.8745e-02, -4.8490e-02, -3.2763e-01, -5.9911e-02,\n",
       "          1.9733e-01,  1.7922e-01,  7.9987e-02, -5.8042e-02,  6.5489e-03,\n",
       "         -7.8065e-02,  6.1339e-02, -2.4206e-02, -3.1782e-02, -1.4030e-02,\n",
       "         -1.2211e-01,  8.9880e-02, -7.2821e-02, -1.1158e-01, -2.0094e-02,\n",
       "          5.9030e-01, -2.7187e-01,  3.8898e-02,  6.2620e-02, -9.5611e-02,\n",
       "         -3.7169e-02, -9.3104e-02, -1.3039e-01, -1.0840e-01, -4.3327e-02,\n",
       "         -4.4890e-02, -4.1741e-02, -7.6869e-02, -1.2968e-02, -2.1890e-01,\n",
       "          1.2399e-01,  1.0578e-01, -1.4529e-01,  9.3155e-02, -1.9347e-02,\n",
       "         -3.1273e-01, -2.3815e-02, -5.6529e-02, -2.7949e-02,  6.8388e-02,\n",
       "          1.4061e-01,  8.8238e-02, -1.5484e-01,  1.6972e-01, -1.1273e-01,\n",
       "         -7.4577e-03,  1.9100e-02,  1.7173e-01, -5.3803e-02, -8.9547e-02,\n",
       "          9.0985e-02, -4.0183e-02, -1.2302e-02,  5.1276e-02, -3.2075e-02,\n",
       "          3.3810e-03, -8.5996e-02,  1.2948e-02, -4.7192e-02, -4.2155e-03,\n",
       "          1.5450e-01, -2.9719e-02,  8.7894e-02, -1.5028e-01,  1.1585e-02,\n",
       "         -7.7371e-02,  7.8539e-01, -9.0044e-02,  8.8893e-02, -8.7922e-02,\n",
       "          6.2379e-02, -7.0870e-02,  1.3583e-01,  3.2528e-02, -1.0864e-01,\n",
       "          8.7316e-02, -2.7371e-01, -7.6228e-03,  4.9978e-02, -2.9836e-02,\n",
       "         -4.4768e-02, -1.1534e-01,  4.0964e-02, -5.5070e-02,  4.4035e-02,\n",
       "         -1.2394e-01,  5.5978e-02, -2.2867e-02, -2.9995e-02, -8.2816e-02,\n",
       "         -9.4715e-01, -6.1216e-02,  1.8971e-02,  1.6313e-01,  6.4169e-02,\n",
       "         -1.3446e-02,  9.1063e-02,  9.2241e-02, -7.1648e-02,  2.2271e-01,\n",
       "          1.4418e-01, -4.1598e-01, -6.8717e-03, -5.7573e-02,  1.7423e-03,\n",
       "         -9.6931e-03, -1.3695e-01, -5.8281e-03, -1.3666e-01,  7.5332e-02,\n",
       "          4.2473e-02, -3.9873e-02,  3.6948e-02,  1.2935e-01, -1.6181e-02,\n",
       "          2.6206e-02,  4.2483e-02, -5.0744e-02,  1.3576e-01,  1.8293e-01,\n",
       "          7.1630e-02, -2.3983e-02, -2.9791e-01,  2.2179e-01,  4.4252e-02,\n",
       "          1.4093e-01,  1.0734e-01,  4.8553e-02,  2.5365e-01, -5.8706e-02,\n",
       "         -2.4030e-02,  1.0953e-03,  1.0886e-01, -3.8533e-02, -1.1904e-02,\n",
       "          1.3801e-01, -3.2291e-01, -1.5570e-02, -1.2893e-02, -1.7400e-01,\n",
       "          9.7249e-02, -3.7845e-02,  5.7839e-02, -9.1402e-02, -6.2146e-02,\n",
       "          3.2638e-03,  6.7335e-03,  9.0159e-02,  2.2526e-01,  2.7464e-01,\n",
       "         -3.9501e-01, -1.3426e-01,  1.9058e-01, -2.4600e-02, -3.2449e-02]),\n",
       " torch.Size([300]))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2embedding[\"[NON_ALPHABET]\"], word2embedding[\"[NON_ALPHABET]\"].shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A sample of how we'll use subwords seperated by puctuations to find embeddings of large compund words, not present in `vocab`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['e', 'x', '-', 'M', 'r', 's', '.']\n",
      "[2, 6]\n",
      "['ex', '-', 'Mrs', '.']\n",
      "tensor([ 7.9575e-02, -3.7300e-02, -2.1530e-01, -1.4200e-02, -1.6500e-03,\n",
      "        -1.2237e-01,  1.7050e-01, -4.2400e-02, -5.5150e-02, -7.7375e-02,\n",
      "        -6.5125e-02,  2.7000e-01, -2.2225e-02, -2.6000e-03, -4.0350e-02,\n",
      "        -5.8175e-02, -2.6825e-02,  1.0875e-02, -8.1950e-02,  9.5975e-02,\n",
      "         3.1750e-03,  2.5700e-02, -9.6450e-02, -1.6780e-01, -2.9275e-02,\n",
      "        -3.0750e-03,  5.5250e-03,  7.0350e-02,  9.0525e-02,  1.4958e-01,\n",
      "         3.6800e-02, -3.3500e-02, -1.0813e-01,  1.6862e-01, -1.0500e-01,\n",
      "        -3.1500e-02, -5.7125e-02,  0.0000e+00, -3.2725e-02, -6.4325e-02,\n",
      "        -9.5225e-02, -3.7150e-02,  1.3418e-01,  1.0030e-01,  1.9827e-01,\n",
      "         3.7025e-02, -1.5350e-02, -7.5175e-02, -6.0300e-02, -2.5732e-01,\n",
      "        -5.3400e-02,  7.7875e-02,  2.9350e-02, -2.5450e-02,  1.7875e-02,\n",
      "        -9.3400e-02, -6.8050e-02,  2.2273e-01,  6.0375e-02,  2.5000e-04,\n",
      "        -7.0575e-02,  1.9107e-01,  4.5425e-02, -7.5250e-03,  5.3400e-02,\n",
      "        -1.7648e-01,  8.2500e-03,  2.2900e-02,  4.6600e-02,  6.3200e-02,\n",
      "        -2.6282e-01, -3.6150e-02, -6.7100e-02, -1.2050e-02,  2.5000e-04,\n",
      "         6.3000e-03, -5.2885e-01,  5.1525e-02, -2.3858e-01,  1.1675e-02,\n",
      "         3.1423e-01, -4.5200e-02,  7.7250e-03,  4.2875e-02, -4.7250e-03,\n",
      "        -2.7500e-04,  6.4475e-02, -2.0750e-03,  7.4375e-02, -6.9850e-02,\n",
      "         5.3700e-02,  1.2870e-01,  1.0925e-02,  2.1650e-02,  1.4250e-02,\n",
      "        -8.0600e-02,  9.5825e-02,  1.6178e-01, -1.9850e-02, -1.4080e-01,\n",
      "        -3.8575e-02, -1.4712e-01, -2.1700e-02,  1.1860e-01,  1.2740e-01,\n",
      "         4.2575e-02, -1.8550e-02,  4.2425e-02, -1.6560e-01,  3.2425e-02,\n",
      "        -1.1775e-02, -1.3778e-01, -6.6150e-02, -3.3150e-02, -4.1045e-01,\n",
      "        -1.9725e-02, -3.8500e-03, -2.2658e-01, -1.6032e-01, -2.5725e-02,\n",
      "        -7.0550e-02, -5.6775e-02,  7.8700e-02,  1.3483e-01,  1.9235e-01,\n",
      "         9.6100e-02,  7.5275e-02, -9.8175e-02,  1.8982e-01, -3.6237e-01,\n",
      "         4.5550e-02, -5.6700e-02,  4.3100e-02, -5.7900e-02,  9.5500e-03,\n",
      "         9.4425e-02,  3.9275e-02, -2.3725e-02,  4.2275e-02, -1.1443e-01,\n",
      "        -2.3125e-02,  1.2270e-01,  4.4450e-02, -2.3825e-01, -5.2225e-02,\n",
      "         1.8082e-01,  1.7180e-01,  9.0050e-02, -4.6100e-02, -1.7100e-02,\n",
      "         1.0577e-01,  2.1305e-01,  5.2275e-02,  9.0400e-02,  1.6550e-02,\n",
      "         2.8975e-02, -8.9725e-02, -1.2658e-01, -4.7250e-02, -1.2425e-02,\n",
      "         5.9022e-01, -9.7125e-02,  4.6800e-02, -1.4175e-01,  8.2500e-03,\n",
      "         6.2950e-02,  4.5900e-02,  6.7575e-02, -4.3900e-02, -9.1000e-03,\n",
      "        -1.1007e-01,  6.5175e-02,  1.3230e-01,  5.9150e-02, -1.9557e-01,\n",
      "         6.9725e-02, -2.9425e-02,  6.8850e-02, -1.0930e-01,  8.2875e-02,\n",
      "        -1.6345e-01, -4.1800e-02,  7.1250e-02,  1.6550e-02,  5.7400e-02,\n",
      "         1.0855e-01,  3.3275e-02, -8.2825e-02, -1.9425e-02, -5.6950e-02,\n",
      "        -1.9050e-02,  1.4000e-02, -2.5575e-02,  8.7250e-03, -3.4325e-02,\n",
      "         5.0625e-02, -2.3375e-02, -3.2625e-02, -9.3625e-02, -6.5700e-02,\n",
      "        -8.0075e-02,  3.5600e-02,  5.0400e-02, -3.5800e-02,  2.7330e-01,\n",
      "         1.1225e-01,  8.3625e-02, -3.7550e-02,  3.5125e-02, -5.6750e-02,\n",
      "         9.4175e-02,  7.0893e-01, -3.5550e-02, -3.8525e-02, -4.5075e-02,\n",
      "         7.4750e-03, -2.4470e-01,  5.0200e-02, -4.9950e-02, -9.0000e-04,\n",
      "        -9.0700e-02,  6.3225e-02, -4.4000e-02,  1.4530e-01, -2.5800e-02,\n",
      "         1.5902e-01, -6.2925e-02,  5.8200e-02, -2.2575e-02, -4.7850e-02,\n",
      "        -7.5375e-02, -7.7475e-02, -1.6692e-01, -1.3250e-02, -1.1265e-01,\n",
      "        -1.5782e-01, -9.8250e-02, -1.9083e-01, -1.1450e-02, -3.5000e-03,\n",
      "         7.0875e-02,  9.5400e-02,  6.1950e-02,  4.7250e-02,  5.5375e-02,\n",
      "         1.3875e-02, -2.9702e-01, -1.4150e-02, -1.5850e-02, -1.6817e-01,\n",
      "        -1.3880e-01,  1.3100e-01, -6.9450e-02,  7.4050e-02, -1.3720e-01,\n",
      "         3.7030e-01,  1.6402e-01, -2.9000e-02, -1.3300e-02,  1.2863e-01,\n",
      "        -7.3125e-02, -1.4650e-02,  4.4875e-02,  1.6925e-02,  1.0060e-01,\n",
      "         6.6300e-02, -4.7025e-02,  8.3600e-02,  5.0001e-05, -2.0250e-03,\n",
      "        -1.3920e-01, -6.9500e-02,  2.0250e-02,  3.3250e-02, -1.1340e-01,\n",
      "         1.7427e-01, -7.3975e-02,  5.0125e-02, -7.3000e-02,  1.9090e-01,\n",
      "         4.3975e-02, -1.7235e-01,  3.3825e-02,  3.6175e-02, -6.7525e-02,\n",
      "         8.2050e-02, -1.2075e-01, -1.2150e-01, -7.7625e-02, -7.6800e-02,\n",
      "         5.5950e-02,  6.9500e-03,  1.8900e-02, -2.9600e-02, -1.0750e-03,\n",
      "        -2.9025e-02,  1.8125e-02,  9.2125e-02,  4.1625e-02, -3.8200e-02])\n"
     ]
    }
   ],
   "source": [
    "letters = list(\"ex-Mrs.\")\n",
    "punctuations = [x for x in string.punctuation]\n",
    "positions = [i for i, x in enumerate(letters) if x in punctuations]\n",
    "\n",
    "print(letters)\n",
    "print(positions)\n",
    "\n",
    "sub_words = []\n",
    "for i, j in zip([0] + positions, positions + [None]):\n",
    "    if i==0:\n",
    "        sub_words.append(\"\".join(letters[i:j]))\n",
    "    else:\n",
    "        sub_words.append(letters[i])\n",
    "        sub_words.append(\"\".join(letters[i+1:j]))\n",
    "    \n",
    "while \"\" in sub_words:\n",
    "    sub_words.remove(\"\")\n",
    "print(sub_words)\n",
    "\n",
    "sub_words_embeddings = []\n",
    "for sub_word in sub_words:\n",
    "    sub_words_embeddings.append(word2embedding[sub_word])\n",
    "\n",
    "print(torch.mean(torch.stack(sub_words_embeddings), dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "punctuations = [x for x in string.punctuation]\n",
    "\n",
    "word2embedding[\"[UNK]\"] = torch.mean(torch.stack([embedding for _, embedding in word2embedding.items()]), dim=0)\n",
    "\n",
    "set_of_vocab = set(word2embedding.keys())\n",
    "\n",
    "def get_embeddings(word, word2embedding, words_not_present):\n",
    "    if word in word2embedding.keys():\n",
    "        return word2embedding[word]\n",
    "    elif re.match(non_alphabet_pattern, word):\n",
    "        return word2embedding[\"[NON_ALPHABET]\"]\n",
    "    elif decontracted(word) in word2embedding.keys():\n",
    "        return word2embedding[decontracted(word)]\n",
    "    elif decontracted(word).lower() in word2embedding.keys():\n",
    "        return word2embedding[decontracted(word).lower()]\n",
    "    else:\n",
    "        letters = list(word)\n",
    "        positions = [i for i, x in enumerate(letters) if x in punctuations]\n",
    "\n",
    "        sub_words = []\n",
    "        for i, j in zip([0] + positions, positions + [None]):\n",
    "            if i==0:\n",
    "                sub_words.append(\"\".join(letters[i:j]))\n",
    "            else:\n",
    "                sub_words.append(letters[i])\n",
    "                sub_words.append(\"\".join(letters[i+1:j]))\n",
    "            \n",
    "        while \"\" in sub_words:\n",
    "            sub_words.remove(\"\")\n",
    "        \n",
    "        if set(sub_words).issubset(set_of_vocab):\n",
    "            return torch.mean(torch.stack([get_embeddings(sub_word, word2embedding, words_not_present) for sub_word in sub_words]), dim=0)\n",
    "        else:\n",
    "            words_not_present.append(word)\n",
    "            return word2embedding[\"[UNK]\"]\n",
    "            # words_embeddings.append(word2embedding[word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1161192/1161192 [00:11<00:00, 102332.93it/s]\n"
     ]
    }
   ],
   "source": [
    "words_not_present = list()\n",
    "\n",
    "X_embeddings = list()\n",
    "for words in tqdm(X):\n",
    "    words_embeddings = list()\n",
    "    for word in words:\n",
    "        words_embeddings.append(get_embeddings(word, word2embedding, words_not_present))\n",
    "    X_embeddings.append(torch.stack(words_embeddings))\n",
    "\n",
    "X_embeddings = torch.stack(X_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"words_not_present.txt\", \"w\") as f:\n",
    "    for word in words_not_present:\n",
    "        f.write(f\"{word}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7165"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words_not_present)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1703"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(words_not_present))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000001, 2000001)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = list(word2embedding.keys())\n",
    "len(vocab), len(set(vocab))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A dictionary to convert pos tags to ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_tag_to_id = dict()\n",
    "for i, tag in enumerate(sorted(universal_tags)):\n",
    "    pos_tag_to_id[tag] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'.': 0,\n",
       " 'ADJ': 1,\n",
       " 'ADP': 2,\n",
       " 'ADV': 3,\n",
       " 'CONJ': 4,\n",
       " 'DET': 5,\n",
       " 'NOUN': 6,\n",
       " 'NUM': 7,\n",
       " 'PRON': 8,\n",
       " 'PRT': 9,\n",
       " 'VERB': 10,\n",
       " 'X': 11}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_tag_to_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1161192, 1161192)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_embeddings), len(y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actual conversion of labels (pos tags) to ids below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(y)):\n",
    "    y[i] = torch.tensor(pos_tag_to_id[y[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.stack(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.int64"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.int64"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[0].dtype"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test-train split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_embeddings, \n",
    "                                                    y,\n",
    "                                                    test_size = 0.1,\n",
    "                                                    random_state =42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1045072, 116120, 1045072, 116120)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train), len(X_test), len(y_train), len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_train), type(X_test), type(y_train), type(y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final leg of data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Tensor, 1045072, torch.Tensor, 3, torch.Tensor, 300)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_train), len(X_train), type(X_train[0]), len(X_train[0]), type(X_train[0][0]), len(X_train[0][0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`X_train` is of dimension `num_of_data_points` x `window_size` x `embedding_size`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the number of epochs\n",
    "epochs = 3\n",
    "\n",
    "# Put the data on the target device\n",
    "X_train, y_train = X_train.to(device), y_train.to(device)\n",
    "X_test, y_test = X_test.to(device), y_test.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct a model that subclasses NN.Module\n",
    "\n",
    "class POS_tagger(nn.Module):\n",
    "    def __init__(self, embedding_size, window_size, hidden_layer_size, num_tags, device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding_size = embedding_size\n",
    "        self.window_size = window_size\n",
    "        self.device = device\n",
    "        self.layers_embeddigs_to_hidden = nn.ModuleList([nn.Linear(in_features=embedding_size, out_features=hidden_layer_size, bias=True) for _ in range(2*window_size + 1)])\n",
    "        self.activation = nn.ReLU()\n",
    "        self.layer_hidden_to_output = nn.Linear(in_features=hidden_layer_size, out_features=num_tags, bias=True)\n",
    "        # self.output_probabilities = nn.Softmax(dim=1)\n",
    "        \n",
    "        \n",
    "        \n",
    "    # Define a `forward()` method that outlines the forward pass (or forward computation) of the model\n",
    "    def forward(self, batched_embeddings):\n",
    "        inputs_to_hidden = torch.stack([\n",
    "            torch.stack([layer(embedding) for embedding, layer in zip(embeddings, self.layers_embeddigs_to_hidden)])\n",
    "            for embeddings in batched_embeddings\n",
    "            ]).to(self.device)\n",
    "        return self.layer_hidden_to_output(self.activation(torch.mean(inputs_to_hidden, dim=1)))\n",
    "        # return self.output_probabilities(self.layer_embeddigs_to_word(self.layer_context_to_embeddings(context_embeddings_average)))            # x -> layer_1 -> layer_2 -> output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_tagger = POS_tagger(embedding_size=300, window_size=1, hidden_layer_size=128, num_tags=len(universal_tags), device=device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda:5'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for parameter in pos_tagger.layers_embeddigs_to_hidden[0].parameters():\n",
    "#     print(parameter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS_tagger(\n",
      "  (layers_embeddigs_to_hidden): ModuleList(\n",
      "    (0): Linear(in_features=300, out_features=128, bias=True)\n",
      "    (1): Linear(in_features=300, out_features=128, bias=True)\n",
      "    (2): Linear(in_features=300, out_features=128, bias=True)\n",
      "  )\n",
      "  (activation): ReLU()\n",
      "  (layer_hidden_to_output): Linear(in_features=128, out_features=12, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(pos_tagger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('layers_embeddigs_to_hidden.0.weight',\n",
       "              tensor([[ 0.0441,  0.0479, -0.0135,  ..., -0.0045, -0.0192, -0.0187],\n",
       "                      [ 0.0019, -0.0122, -0.0199,  ..., -0.0508,  0.0403,  0.0067],\n",
       "                      [-0.0311,  0.0302, -0.0546,  ..., -0.0237, -0.0143, -0.0223],\n",
       "                      ...,\n",
       "                      [ 0.0527, -0.0576,  0.0222,  ..., -0.0255, -0.0458,  0.0570],\n",
       "                      [-0.0353,  0.0326,  0.0238,  ...,  0.0176, -0.0237,  0.0533],\n",
       "                      [-0.0546,  0.0252, -0.0439,  ..., -0.0310,  0.0087,  0.0520]],\n",
       "                     device='cuda:5')),\n",
       "             ('layers_embeddigs_to_hidden.0.bias',\n",
       "              tensor([ 0.0218, -0.0260, -0.0268,  0.0015,  0.0040,  0.0442, -0.0430, -0.0533,\n",
       "                       0.0353, -0.0284, -0.0435,  0.0147, -0.0560,  0.0334, -0.0252,  0.0453,\n",
       "                      -0.0549, -0.0550, -0.0233,  0.0488, -0.0500, -0.0354,  0.0525,  0.0450,\n",
       "                       0.0050, -0.0485,  0.0436,  0.0297,  0.0431,  0.0114, -0.0386, -0.0523,\n",
       "                      -0.0429,  0.0566,  0.0141,  0.0296, -0.0269,  0.0502,  0.0169,  0.0431,\n",
       "                       0.0321, -0.0338,  0.0509, -0.0269,  0.0335,  0.0005, -0.0272,  0.0361,\n",
       "                       0.0275,  0.0062, -0.0429,  0.0280,  0.0330, -0.0203,  0.0156, -0.0255,\n",
       "                       0.0331, -0.0441,  0.0236, -0.0445,  0.0133,  0.0271,  0.0242,  0.0190,\n",
       "                       0.0563, -0.0545, -0.0235, -0.0224,  0.0550,  0.0205, -0.0157,  0.0414,\n",
       "                       0.0060, -0.0537,  0.0081, -0.0243, -0.0392,  0.0068,  0.0293, -0.0534,\n",
       "                       0.0528, -0.0004,  0.0016, -0.0540, -0.0010,  0.0158, -0.0485, -0.0408,\n",
       "                       0.0152, -0.0038,  0.0569,  0.0004,  0.0056, -0.0296, -0.0184,  0.0332,\n",
       "                      -0.0533,  0.0056,  0.0394, -0.0167, -0.0546, -0.0511, -0.0299,  0.0030,\n",
       "                       0.0387,  0.0413,  0.0105, -0.0195,  0.0115,  0.0042, -0.0105,  0.0157,\n",
       "                       0.0058,  0.0411,  0.0249,  0.0418,  0.0144, -0.0152, -0.0543, -0.0387,\n",
       "                      -0.0361,  0.0253, -0.0313,  0.0500, -0.0139, -0.0484, -0.0296, -0.0215],\n",
       "                     device='cuda:5')),\n",
       "             ('layers_embeddigs_to_hidden.1.weight',\n",
       "              tensor([[-0.0284, -0.0291,  0.0352,  ..., -0.0543, -0.0342,  0.0292],\n",
       "                      [ 0.0433, -0.0361, -0.0383,  ...,  0.0286,  0.0063,  0.0268],\n",
       "                      [-0.0553,  0.0436, -0.0180,  ..., -0.0451,  0.0423, -0.0205],\n",
       "                      ...,\n",
       "                      [ 0.0553,  0.0394, -0.0271,  ..., -0.0211, -0.0547, -0.0343],\n",
       "                      [ 0.0449, -0.0468,  0.0046,  ..., -0.0398, -0.0138,  0.0206],\n",
       "                      [-0.0453, -0.0433, -0.0235,  ..., -0.0035, -0.0061, -0.0303]],\n",
       "                     device='cuda:5')),\n",
       "             ('layers_embeddigs_to_hidden.1.bias',\n",
       "              tensor([ 0.0527,  0.0455,  0.0085,  0.0432, -0.0571,  0.0188,  0.0050,  0.0508,\n",
       "                      -0.0357, -0.0157, -0.0457, -0.0323,  0.0130, -0.0117,  0.0238,  0.0182,\n",
       "                       0.0534, -0.0290,  0.0334, -0.0090, -0.0193, -0.0489, -0.0168, -0.0058,\n",
       "                       0.0325, -0.0413, -0.0435,  0.0163, -0.0206, -0.0334, -0.0196,  0.0113,\n",
       "                       0.0134,  0.0370,  0.0442, -0.0193, -0.0276,  0.0078,  0.0533,  0.0484,\n",
       "                       0.0390,  0.0310,  0.0381,  0.0413,  0.0479,  0.0027, -0.0512, -0.0326,\n",
       "                      -0.0001, -0.0285, -0.0359,  0.0035,  0.0174,  0.0184, -0.0536,  0.0161,\n",
       "                      -0.0232, -0.0140,  0.0200,  0.0563, -0.0478, -0.0032, -0.0436,  0.0538,\n",
       "                      -0.0403,  0.0314,  0.0287, -0.0050, -0.0484,  0.0491,  0.0394,  0.0538,\n",
       "                      -0.0273,  0.0287, -0.0502, -0.0521,  0.0447, -0.0245, -0.0065, -0.0340,\n",
       "                      -0.0391, -0.0398, -0.0069,  0.0038,  0.0238,  0.0036, -0.0025, -0.0141,\n",
       "                       0.0529, -0.0218,  0.0274,  0.0422,  0.0527,  0.0115,  0.0231, -0.0059,\n",
       "                       0.0132,  0.0449,  0.0020,  0.0517, -0.0214,  0.0169, -0.0192, -0.0320,\n",
       "                       0.0093, -0.0378, -0.0034, -0.0329, -0.0151,  0.0412, -0.0166,  0.0548,\n",
       "                      -0.0091, -0.0448,  0.0338, -0.0321, -0.0346, -0.0364, -0.0549, -0.0081,\n",
       "                       0.0326,  0.0089,  0.0448, -0.0527, -0.0146, -0.0058,  0.0343, -0.0551],\n",
       "                     device='cuda:5')),\n",
       "             ('layers_embeddigs_to_hidden.2.weight',\n",
       "              tensor([[ 1.2610e-02,  5.5967e-02,  5.5824e-02,  ..., -2.8539e-02,\n",
       "                        3.2861e-02, -4.4127e-02],\n",
       "                      [-1.1434e-02,  3.0724e-02,  4.1624e-02,  ..., -1.4850e-02,\n",
       "                       -2.3970e-02,  2.3851e-02],\n",
       "                      [ 3.5804e-02, -8.9477e-03, -2.2625e-02,  ...,  2.3383e-02,\n",
       "                       -1.0851e-02,  2.2588e-02],\n",
       "                      ...,\n",
       "                      [-2.2069e-02, -4.0066e-02, -4.0909e-02,  ...,  9.1288e-03,\n",
       "                       -3.8281e-02,  1.2751e-02],\n",
       "                      [ 2.2340e-02,  2.8260e-02, -2.6397e-02,  ...,  9.6211e-05,\n",
       "                       -7.4482e-03,  1.9437e-02],\n",
       "                      [ 3.4908e-02,  3.2221e-02,  4.6117e-02,  ...,  2.6888e-02,\n",
       "                       -3.7303e-02, -2.4602e-02]], device='cuda:5')),\n",
       "             ('layers_embeddigs_to_hidden.2.bias',\n",
       "              tensor([-0.0039, -0.0518,  0.0028,  0.0352, -0.0284, -0.0389, -0.0183, -0.0148,\n",
       "                      -0.0404, -0.0141, -0.0547,  0.0508, -0.0528, -0.0339, -0.0102,  0.0260,\n",
       "                      -0.0496, -0.0015,  0.0234,  0.0141, -0.0265, -0.0301, -0.0074,  0.0077,\n",
       "                      -0.0201,  0.0175, -0.0386, -0.0300, -0.0565,  0.0497,  0.0408, -0.0172,\n",
       "                       0.0082,  0.0038, -0.0244, -0.0121, -0.0162, -0.0452,  0.0123,  0.0434,\n",
       "                       0.0145,  0.0293,  0.0209,  0.0151,  0.0275, -0.0010, -0.0168, -0.0443,\n",
       "                       0.0165,  0.0188, -0.0053,  0.0357, -0.0390, -0.0082,  0.0548,  0.0032,\n",
       "                      -0.0415,  0.0088, -0.0427,  0.0154, -0.0075, -0.0184, -0.0062, -0.0435,\n",
       "                      -0.0423,  0.0355, -0.0432,  0.0198,  0.0112,  0.0092,  0.0425, -0.0278,\n",
       "                       0.0142,  0.0030, -0.0450,  0.0245,  0.0440, -0.0334, -0.0231, -0.0179,\n",
       "                       0.0424,  0.0521,  0.0054,  0.0218,  0.0510, -0.0114,  0.0432,  0.0007,\n",
       "                      -0.0022, -0.0367,  0.0192, -0.0036,  0.0043, -0.0071, -0.0306,  0.0227,\n",
       "                       0.0375,  0.0207,  0.0223,  0.0450,  0.0416,  0.0274, -0.0328,  0.0057,\n",
       "                       0.0486, -0.0430,  0.0357,  0.0315, -0.0314, -0.0116,  0.0509,  0.0188,\n",
       "                      -0.0095, -0.0129,  0.0408,  0.0258, -0.0290,  0.0266, -0.0278,  0.0062,\n",
       "                      -0.0002, -0.0566,  0.0554,  0.0506,  0.0014,  0.0183,  0.0004, -0.0174],\n",
       "                     device='cuda:5')),\n",
       "             ('layer_hidden_to_output.weight',\n",
       "              tensor([[-0.0513,  0.0005,  0.0451,  ..., -0.0775, -0.0433, -0.0242],\n",
       "                      [-0.0854,  0.0559, -0.0404,  ...,  0.0105,  0.0300,  0.0826],\n",
       "                      [-0.0570, -0.0829,  0.0716,  ..., -0.0777, -0.0553, -0.0236],\n",
       "                      ...,\n",
       "                      [-0.0516, -0.0567,  0.0140,  ..., -0.0420, -0.0324, -0.0384],\n",
       "                      [-0.0423,  0.0633,  0.0643,  ...,  0.0532, -0.0314,  0.0139],\n",
       "                      [-0.0123, -0.0700,  0.0361,  ...,  0.0371,  0.0001, -0.0186]],\n",
       "                     device='cuda:5')),\n",
       "             ('layer_hidden_to_output.bias',\n",
       "              tensor([-0.0726,  0.0578,  0.0627, -0.0320,  0.0069, -0.0649, -0.0158,  0.0819,\n",
       "                      -0.0662, -0.0412,  0.0459, -0.0182], device='cuda:5'))])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_tagger.state_dict()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training loop setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a loss function\n",
    "loss_fn = nn.CrossEntropyLoss() # CrossEntropyLoss -> sigmoid activation is built-in\n",
    "\n",
    "# Create an optimizer\n",
    "optimizer = torch.optim.Adam(params=pos_tagger.parameters(),\n",
    "                            lr = 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.memory_allocated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.max_memory_allocated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accuracy\n",
    "def accuracy_fn(y_true, y_pred):\n",
    "    correct = torch.eq(y_true, y_pred).sum().item()      # torch.eq(y_true, y_pred) checks how many of y_true are actually equal to y_pred\n",
    "    accuracy = correct / len(y_pred) * 100\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.tensor(torch.nn.Softmax(y_logits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4083/4083 [05:55<00:00, 11.48it/s, batch_loss=0.0624]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Loss: 0.06243 | Acc: 97.27% | Test loss: 0.08928 | Test acc: 97.17%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4083/4083 [06:08<00:00, 11.08it/s, batch_loss=0.0812]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | Loss: 0.08118 | Acc: 97.27% | Test loss: 0.07083 | Test acc: 97.71%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4083/4083 [05:40<00:00, 11.98it/s, batch_loss=0.0574]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 | Loss: 0.05736 | Acc: 97.27% | Test loss: 0.06310 | Test acc: 97.98%\n"
     ]
    }
   ],
   "source": [
    "batch_size = 256\n",
    "# Building the training and evaluation loop\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = 0.0\n",
    "    ## Training\n",
    "    pos_tagger.train()\n",
    "    with tqdm(np.random.permutation(range(0, len(X_train), batch_size))) as progress_bar:\n",
    "        for i in progress_bar:\n",
    "            # 1. Forward pass\n",
    "            y_logits = pos_tagger(X_train[i:i+batch_size]).squeeze()      # squeeze removes an extra one dimension from a tensor\n",
    "            # y_pred = torch.argmax(torch.nn.Softmax(y_logits), dim=1)   # Can't make it work with sotmax for some reason\n",
    "            y_pred = torch.argmax(y_logits, dim=1)\n",
    "        \n",
    "            # 2. Calculate loss / accuracy\n",
    "            loss = loss_fn(y_logits,        # nn.CrossEntropyLoss expects raw logits as input\n",
    "                        y_train[i:i+batch_size])\n",
    "        \n",
    "            epoch_loss += loss\n",
    "            \n",
    "            acc = accuracy_fn(y_true=y_train[i:i+batch_size],\n",
    "                            y_pred=y_pred)\n",
    "        \n",
    "            # 3. Optimizer zero grad\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "            # 4. Loss backward (back propagation)\n",
    "            loss.backward()\n",
    "        \n",
    "            # 5. Optimizer step (gradient descent)\n",
    "            optimizer.step()\n",
    "            \n",
    "            progress_bar.set_postfix(batch_loss=loss.item())\n",
    "        \n",
    "        \n",
    "        ### Testing\n",
    "        pos_tagger.eval()\n",
    "        with torch.inference_mode():\n",
    "            # 1. Forward pass\n",
    "            test_logits = pos_tagger(X_test).squeeze()      # squeeze removes an extra one dimension from a tensor\n",
    "            test_pred = torch.argmax(test_logits, dim=1)\n",
    "            \n",
    "            # 2. Calculate loss / accuracy\n",
    "            test_loss = loss_fn(test_logits,        # nn.CrossEntropyLoss expects raw logits as input\n",
    "                                y_test)\n",
    "            \n",
    "            test_acc = accuracy_fn(y_true=y_test,\n",
    "                            y_pred=test_pred)\n",
    "            \n",
    "        # if epoch % 10 == 0:\n",
    "        print(f\"Epoch: {epoch} | Loss: {loss:.5f} | Acc: {acc:.2f}% | Test loss: {test_loss:.5f} | Test acc: {test_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code for Demo of POS Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '.',\n",
       " 1: 'ADJ',\n",
       " 2: 'ADP',\n",
       " 3: 'ADV',\n",
       " 4: 'CONJ',\n",
       " 5: 'DET',\n",
       " 6: 'NOUN',\n",
       " 7: 'NUM',\n",
       " 8: 'PRON',\n",
       " 9: 'PRT',\n",
       " 10: 'VERB',\n",
       " 11: 'X'}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_to_pos_tag = {id: tag for tag, id in pos_tag_to_id.items()}\n",
    "id_to_pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pos_tags(sentence):\n",
    "    tokens = word_tokenize(sentence)\n",
    "    test = list()\n",
    "\n",
    "    null_token = \"[NULL_TOKEN]\"\n",
    "    for i in range(len(tokens)):\n",
    "        padded_sentence = [null_token] * window_size + tokens + [null_token] * window_size\n",
    "        test.append(padded_sentence[i : i + 2*window_size + 1])\n",
    "    \n",
    "    test_embeddings = list()\n",
    "    words_not_present = list()\n",
    "\n",
    "    for words in test:\n",
    "        words_embeddings = list()\n",
    "        for word in words:\n",
    "            words_embeddings.append(get_embeddings(word, word2embedding, words_not_present))\n",
    "        test_embeddings.append(torch.stack(words_embeddings))\n",
    "\n",
    "    test_embeddings = torch.stack(test_embeddings)\n",
    "    \n",
    "    pos_tagger.eval()\n",
    "    with torch.inference_mode():\n",
    "        # 1. Forward pass\n",
    "        test_logits = pos_tagger(test_embeddings.to(device)).squeeze()      # squeeze removes an extra one dimension from a tensor\n",
    "        test_pred = torch.argmax(test_logits, dim=1)\n",
    "        \n",
    "    test_pred = [id_to_pos_tag[id] for id in test_pred.cpu().detach().numpy()]\n",
    "    \n",
    "    df_test = pd.DataFrame.from_dict({\n",
    "        \"tokens\": tokens,\n",
    "        \"POS Tags\": test_pred\n",
    "    })\n",
    "    \n",
    "    display(HTML(df_test.to_html()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample input sentence:\n",
    "\n",
    "Good muffins cost $3.88 in New York. Please buy me ... two od them. Thanks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = input(\"Please enter sentence to perform POS Tagging:\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "      <th>POS Tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Good</td>\n",
       "      <td>ADJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>muffins</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cost</td>\n",
       "      <td>VERB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>$</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.88</td>\n",
       "      <td>NUM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>in</td>\n",
       "      <td>ADP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>New</td>\n",
       "      <td>ADJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>York</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Please</td>\n",
       "      <td>VERB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>buy</td>\n",
       "      <td>VERB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>me</td>\n",
       "      <td>PRON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>...</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>two</td>\n",
       "      <td>NUM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>od</td>\n",
       "      <td>ADP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>them</td>\n",
       "      <td>PRON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Thanks</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "get_pos_tags(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_pos_tags(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Good',\n",
       " 'muffins',\n",
       " 'cost',\n",
       " '$',\n",
       " '3.88',\n",
       " 'in',\n",
       " 'New',\n",
       " 'York',\n",
       " '.',\n",
       " 'Please',\n",
       " 'buy',\n",
       " 'me',\n",
       " '...',\n",
       " 'two',\n",
       " 'of',\n",
       " 'them',\n",
       " '.',\n",
       " 'Thanks',\n",
       " '.']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['[NULL_TOKEN]', 'Good', 'muffins'],\n",
       " ['Good', 'muffins', 'cost'],\n",
       " ['muffins', 'cost', '$'],\n",
       " ['cost', '$', '3.88'],\n",
       " ['$', '3.88', 'in'],\n",
       " ['3.88', 'in', 'New'],\n",
       " ['in', 'New', 'York'],\n",
       " ['New', 'York', '.'],\n",
       " ['York', '.', 'Please'],\n",
       " ['.', 'Please', 'buy'],\n",
       " ['Please', 'buy', 'me'],\n",
       " ['buy', 'me', '...'],\n",
       " ['me', '...', 'two'],\n",
       " ['...', 'two', 'of'],\n",
       " ['two', 'of', 'them'],\n",
       " ['of', 'them', '.'],\n",
       " ['them', '.', 'Thanks'],\n",
       " ['.', 'Thanks', '.'],\n",
       " ['Thanks', '.', '[NULL_TOKEN]']]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = word_tokenize(sentence)\n",
    "window_size = 1\n",
    "test = list()\n",
    "\n",
    "null_token = \"[NULL_TOKEN]\"\n",
    "for i in range(len(tokens)):\n",
    "    padded_sentence = [null_token] * window_size + tokens + [null_token] * window_size\n",
    "    test.append(padded_sentence[i : i + 2*window_size + 1])\n",
    "    \n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_embeddings = list()\n",
    "words_not_present = list()\n",
    "\n",
    "for words in test:\n",
    "    words_embeddings = list()\n",
    "    for word in words:\n",
    "        words_embeddings.append(get_embeddings(word, word2embedding, words_not_present))\n",
    "    test_embeddings.append(torch.stack(words_embeddings))\n",
    "\n",
    "test_embeddings = torch.stack(test_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19, torch.Tensor, 3, torch.Tensor)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_embeddings), type(test_embeddings), len(test_embeddings[0]), type(test_embeddings[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([19, 3, 300])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_tagger.eval()\n",
    "with torch.inference_mode():\n",
    "    # 1. Forward pass\n",
    "    test_logits = pos_tagger(test_embeddings.to(device)).squeeze()      # squeeze removes an extra one dimension from a tensor\n",
    "    test_pred = torch.argmax(torch.sigmoid(test_logits), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1,  6, 10,  6,  7,  2,  1,  6,  0,  9, 10,  8,  0,  7,  2,  8,  0,  6,\n",
       "         0], device='cuda:5')"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '.',\n",
       " 1: 'ADJ',\n",
       " 2: 'ADP',\n",
       " 3: 'ADV',\n",
       " 4: 'CONJ',\n",
       " 5: 'DET',\n",
       " 6: 'NOUN',\n",
       " 7: 'NUM',\n",
       " 8: 'PRON',\n",
       " 9: 'PRT',\n",
       " 10: 'VERB',\n",
       " 11: 'X'}"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_to_pos_tag = {id: tag for tag, id in pos_tag_to_id.items()}\n",
    "id_to_pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1,  6, 10,  6,  7,  2,  1,  6,  0,  9, 10,  8,  0,  7,  2,  8,  0,  6,\n",
       "         0], device='cuda:5')"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ADJ',\n",
       " 'NOUN',\n",
       " 'VERB',\n",
       " 'NOUN',\n",
       " 'NUM',\n",
       " 'ADP',\n",
       " 'ADJ',\n",
       " 'NOUN',\n",
       " '.',\n",
       " 'PRT',\n",
       " 'VERB',\n",
       " 'PRON',\n",
       " '.',\n",
       " 'NUM',\n",
       " 'ADP',\n",
       " 'PRON',\n",
       " '.',\n",
       " 'NOUN',\n",
       " '.']"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred = [id_to_pos_tag[id] for id in test_pred.cpu().detach().numpy()]\n",
    "test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19, 19)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens), len(test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "      <th>POS Tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Good</td>\n",
       "      <td>ADJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>muffins</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cost</td>\n",
       "      <td>VERB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>$</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.88</td>\n",
       "      <td>NUM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>in</td>\n",
       "      <td>ADP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>New</td>\n",
       "      <td>ADJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>York</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Please</td>\n",
       "      <td>PRT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>buy</td>\n",
       "      <td>VERB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>me</td>\n",
       "      <td>PRON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>...</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>two</td>\n",
       "      <td>NUM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>of</td>\n",
       "      <td>ADP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>them</td>\n",
       "      <td>PRON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Thanks</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_test = pd.DataFrame.from_dict({\n",
    "    \"tokens\": tokens,\n",
    "    \"POS Tags\": test_pred\n",
    "})\n",
    "display(HTML(df_test.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stable",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
