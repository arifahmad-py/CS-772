{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown\n",
    "from nltk.corpus import treebank\n",
    "from nltk.corpus import conll2000\n",
    "treebank_corpus = treebank.tagged_sents(tagset='universal')\n",
    "brown_corpus = brown.tagged_sents(tagset='universal')\n",
    "conll_corpus = conll2000.tagged_sents(tagset='universal')\n",
    "tagged_sentences = treebank_corpus + brown_corpus + conll_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [] # store input sequence\n",
    "Y = [] # store output sequence\n",
    "\n",
    "for sentence in tagged_sentences:\n",
    "    X_sentence = []\n",
    "    Y_sentence = []\n",
    "    for entity in sentence:         \n",
    "        X_sentence.append(entity[0])  # entity[0] contains the word\n",
    "        Y_sentence.append(entity[1])  # entity[1] contains corresponding tag\n",
    "        \n",
    "    X.append(X_sentence)\n",
    "    Y.append(Y_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59448\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "num_words = len(set([word.lower() for sentence in X for word in sentence]))\n",
    "num_tags   = len(set([word.lower() for sentence in Y for word in sentence]))\n",
    "print(num_words)\n",
    "print(num_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'x': 0, 'verb': 1, 'conj': 2, 'adj': 3, 'num': 4, 'prt': 5, 'adp': 6, 'det': 7, 'adv': 8, '.': 9, 'pron': 10, 'noun': 11}\n"
     ]
    }
   ],
   "source": [
    "unique_tags = list(set([word.lower() for sentence in Y for word in sentence]))\n",
    "unique_tags_dict = {}\n",
    "index = 0\n",
    "for tag in unique_tags:\n",
    "    unique_tags_dict[tag] = index \n",
    "    index += 1\n",
    "print(unique_tags_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59448\n"
     ]
    }
   ],
   "source": [
    "unique_words = list(set([word.lower() for sentence in X for word in sentence]))\n",
    "unique_words_dict = {}\n",
    "index = 0\n",
    "for word in unique_words:\n",
    "    unique_words_dict[word] = index \n",
    "    index += 1\n",
    "print(len(unique_words_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNTagger(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, target_size):\n",
    "        super(RNNTagger, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim)\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, target_size)\n",
    "        \n",
    "    def forward(self, sentence):\n",
    "        #Input shape: [len(sentence)]\n",
    "        embeds = self.word_embeddings(sentence)  \n",
    "        #embeds shape: [len(sentence), embdeddin_dim]\n",
    "  \n",
    "        \n",
    "        #input shape: [len(sentence),1,embedding_dim] (L,N,Hin​) when batch_first=False)\n",
    "        rnn_out, hidden_state_out = self.rnn(embeds.view(len(sentence), 1, -1)) \n",
    "        #rnn_out shape: [len(sentence),1,hidden_dim] \n",
    "        #hiddsen_state_out shape: [1,1,hidden_shape]\n",
    "\n",
    "        #input shape: [len(sentence),hidden_dim]\n",
    "        tag_space = self.hidden2tag(rnn_out.view(len(sentence), -1))\n",
    "        #tag_shape : (len(sentence),target_size)\n",
    "        \n",
    "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
    "        return tag_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequence(seq, to_ix):\n",
    "    \"\"\"Input: takes in a list of words, and a dictionary containing the index of the words\n",
    "    Output: a tensor containing the indexes of the word\"\"\"\n",
    "    idxs = [to_ix[w.lower()] for w in seq]\n",
    "    return torch.tensor(idxs, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './GoogleNews-vectors-negative300.bin'\n",
    "# load word2vec using the following function present in the gensim library\n",
    "word2vec = KeyedVectors.load_word2vec_format(path, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't assign a numpy.ndarray to a torch.FloatTensor",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 17\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[39mfor\u001b[39;00m word, index \u001b[39min\u001b[39;00m unique_words_dict\u001b[39m.\u001b[39mitems():\n\u001b[0;32m     14\u001b[0m     \u001b[39m#print(word)\u001b[39;00m\n\u001b[0;32m     15\u001b[0m     \u001b[39m#print(index)\u001b[39;00m\n\u001b[0;32m     16\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 17\u001b[0m         embedding_weights[index, :] \u001b[39m=\u001b[39m word2vec[word]\n\u001b[0;32m     18\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m:\n\u001b[0;32m     19\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mwow\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: can't assign a numpy.ndarray to a torch.FloatTensor"
     ]
    }
   ],
   "source": [
    "# assign word vectors from word2vec model\n",
    "\n",
    "EMBEDDING_SIZE  = 300  # each word in word2vec model is represented using a 300 dimensional vector\n",
    "VOCABULARY_SIZE = num_words\n",
    "\n",
    "\n",
    "embedding_weights = torch.zeros((VOCABULARY_SIZE, EMBEDDING_SIZE))\n",
    "\n",
    "for word, index in unique_words_dict.items():\n",
    "    #print(word)\n",
    "    #print(index)\n",
    "    try:\n",
    "        embedding_weights[index, :] = word2vec[word]\n",
    "    except KeyError:\n",
    "        print(\"wow\")\n",
    "        pass\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape: (59448, 300)\n"
     ]
    }
   ],
   "source": [
    "print(\"Embeddings shape: {}\".format(embedding_weights.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 128\n",
    "HIDDEN_DIM = 64\n",
    "model =RNNTagger(EMBEDDING_DIM, HIDDEN_DIM, len(unique_words_dict.keys()), len(unique_tags_dict.keys()))\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 12299/72202 [00:38<03:08, 316.96it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[51], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m sentence_in \u001b[39m=\u001b[39m prepare_sequence(sentence, unique_words_dict)\n\u001b[0;32m     10\u001b[0m targets \u001b[39m=\u001b[39m prepare_sequence(tags, unique_tags_dict)\n\u001b[1;32m---> 11\u001b[0m sentence_in \u001b[39m=\u001b[39m sentence_in\u001b[39m.\u001b[39;49mto(device\u001b[39m=\u001b[39;49mdevice)\n\u001b[0;32m     12\u001b[0m targets \u001b[39m=\u001b[39m targets\u001b[39m.\u001b[39mto(device \u001b[39m=\u001b[39m device)\n\u001b[0;32m     13\u001b[0m \u001b[39m#print(targets)\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"Training Started\")\n",
    "total_length = len(X)\n",
    "for epoch in range(1):\n",
    "    for i in tqdm(range(total_length)):\n",
    "        sentence = X[i]\n",
    "        tags = Y[i]\n",
    "        model.zero_grad()\n",
    "        #print(sentence)\n",
    "        sentence_in = prepare_sequence(sentence, unique_words_dict)\n",
    "        targets = prepare_sequence(tags, unique_tags_dict)\n",
    "        sentence_in = sentence_in.to(device=device)\n",
    "        targets = targets.to(device = device)\n",
    "        #print(targets)\n",
    "        \n",
    "        tag_scores = model(sentence_in)\n",
    "        #print(tag_scores)\n",
    "        \n",
    "        loss = loss_function(tag_scores, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 0, 4])\n",
      "tensor([[-2.1229, -1.7472, -3.3242, -0.9893, -1.2099],\n",
      "        [-1.1042, -2.3910, -3.6942, -1.6905, -1.0005],\n",
      "        [-1.2733, -2.6861, -1.9800, -2.6682, -0.8108]],\n",
      "       grad_fn=<LogSoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "m = nn.LogSoftmax(dim=1)\n",
    "loss = nn.NLLLoss()\n",
    "# input is of size N x C = 3 x 5\n",
    "input = torch.randn(3, 5, requires_grad=True)\n",
    "#each element in target has to have 0 <= value < C\n",
    "target = torch.tensor([1, 0, 4])\n",
    "print(target)\n",
    "print(m(input))\n",
    "output = loss(m(input), target)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
