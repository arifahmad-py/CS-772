{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\sem8\\CS772\\A2_Part1\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import pickle\n",
    "from gensim.models import KeyedVectors\n",
    "from nltk.corpus import brown\n",
    "from nltk.corpus import treebank\n",
    "from nltk.corpus import conll2000\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "treebank_corpus = treebank.tagged_sents(tagset='universal')\n",
    "brown_corpus = brown.tagged_sents(tagset='universal')\n",
    "conll_corpus = conll2000.tagged_sents(tagset='universal')\n",
    "tagged_sentences = treebank_corpus + brown_corpus + conll_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Pierre', 'NOUN'),\n",
       " ('Vinken', 'NOUN'),\n",
       " (',', '.'),\n",
       " ('61', 'NUM'),\n",
       " ('years', 'NOUN'),\n",
       " ('old', 'ADJ'),\n",
       " (',', '.'),\n",
       " ('will', 'VERB'),\n",
       " ('join', 'VERB'),\n",
       " ('the', 'DET'),\n",
       " ('board', 'NOUN'),\n",
       " ('as', 'ADP'),\n",
       " ('a', 'DET'),\n",
       " ('nonexecutive', 'ADJ'),\n",
       " ('director', 'NOUN'),\n",
       " ('Nov.', 'NOUN'),\n",
       " ('29', 'NUM'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged_sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [] # store input sequence\n",
    "Y = [] # store output sequence\n",
    "\n",
    "for sentence in tagged_sentences:\n",
    "    X_sentence = []\n",
    "    Y_sentence = []\n",
    "    for entity in sentence:         \n",
    "        X_sentence.append(entity[0])  # entity[0] contains the word\n",
    "        Y_sentence.append(entity[1])  # entity[1] contains corresponding tag\n",
    "        \n",
    "    X.append(X_sentence)\n",
    "    Y.append(Y_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Pierre',\n",
       " 'Vinken',\n",
       " ',',\n",
       " '61',\n",
       " 'years',\n",
       " 'old',\n",
       " ',',\n",
       " 'will',\n",
       " 'join',\n",
       " 'the',\n",
       " 'board',\n",
       " 'as',\n",
       " 'a',\n",
       " 'nonexecutive',\n",
       " 'director',\n",
       " 'Nov.',\n",
       " '29',\n",
       " '.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59448\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "num_words = len(set([word.lower() for sentence in X for word in sentence]))\n",
    "num_tags   = len(set([word.lower() for sentence in Y for word in sentence]))\n",
    "print(num_words)\n",
    "print(num_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'.': 0, 'adp': 1, 'verb': 2, 'num': 3, 'noun': 4, 'det': 5, 'x': 6, 'conj': 7, 'pron': 8, 'prt': 9, 'adv': 10, 'adj': 11}\n"
     ]
    }
   ],
   "source": [
    "unique_tags = list(set([word.lower() for sentence in Y for word in sentence]))\n",
    "unique_tags_dict = {}\n",
    "index = 0\n",
    "for tag in unique_tags:\n",
    "    unique_tags_dict[tag] = index \n",
    "    index += 1\n",
    "print(unique_tags_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59448\n"
     ]
    }
   ],
   "source": [
    "unique_words = list(set([word.lower() for sentence in X for word in sentence]))\n",
    "unique_words_dict = {}\n",
    "index = 0\n",
    "for word in unique_words:\n",
    "    unique_words_dict[word] = index \n",
    "    index += 1\n",
    "print(len(unique_words_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequence(seq, to_ix):\n",
    "    \"\"\"Input: takes in a list of words, and a dictionary containing the index of the words\n",
    "    Output: a tensor containing the indexes of the word\"\"\"\n",
    "    idxs = [to_ix[w.lower()] for w in seq]\n",
    "    return torch.tensor(idxs, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([59448, 300])\n"
     ]
    }
   ],
   "source": [
    "EMBEDDING_SIZE  = 300  # each word in word2vec model is represented using a 300 dimensional vector\n",
    "VOCABULARY_SIZE = num_words\n",
    "\n",
    "with open('./embedding_weights.pickle', 'rb') as file:\n",
    "    embedding_weights = pickle.load(file)\n",
    "\n",
    "print(embedding_weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_emb_layer(weights_matrix, non_trainable=False):\n",
    "\n",
    "    num_embeddings, embedding_dim = weights_matrix.size()\n",
    "    emb_layer = nn.Embedding(num_embeddings, embedding_dim)\n",
    "    emb_layer.load_state_dict({'weight': weights_matrix})\n",
    "    \n",
    "    if non_trainable:\n",
    "        emb_layer.weight.requires_grad = False\n",
    "\n",
    "    return emb_layer, num_embeddings, embedding_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNTagger_encoder(nn.Module):\n",
    "    def __init__(self, hidden_dim, target_size):\n",
    "        super(RNNTagger_encoder, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.target_size = target_size\n",
    "        \n",
    "        self.word_embeddings, vocab_size, embedding_dim = create_emb_layer(embedding_weights, True)\n",
    "        #self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim)\n",
    "        #self.hidden2tag = nn.Linear(hidden_dim, target_size)\n",
    "        \n",
    "    def forward(self, sentence):\n",
    "        #print(\"ENTERING ENCODER\")\n",
    "\n",
    "        #Input shape: [len(sentence)]\n",
    "        embeds = self.word_embeddings(sentence)  \n",
    "        #embeds shape: [len(sentence), embdeddin_dim]\n",
    "  \n",
    "        \n",
    "        #input shape: [len(sentence),1,embedding_dim] (L,N,Hin​) when batch_first=False)\n",
    "        rnn_out, hidden_state_out = self.rnn(embeds.view(len(sentence), 1, -1))  \n",
    "        #rnn_out shape: [len(sentence),1,hidden_dim] \n",
    "        #hiddsen_state_out shape: [1,1,hidden_shape]  The hidden state corresponding to last time step\n",
    "\n",
    "        #print(\"LEAVING ENCODER\")\n",
    "        \n",
    "        return rnn_out,hidden_state_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNTagger_decoder(nn.Module):\n",
    "    def __init__(self, hidden_dim, target_size):\n",
    "        super(RNNTagger_decoder, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.target_size = target_size\n",
    "        \n",
    "        self.word_embeddings, vocab_size, embedding_dim = create_emb_layer(embedding_weights, True)\n",
    "        #self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim)\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, target_size)\n",
    "        \n",
    "    def forward(self, sentence,hidden):\n",
    "        #print(\"ENTERING DECODER\")\n",
    "\n",
    "\n",
    "        #Input shape: [len(sentence)]  -----HERE len(sentence) = 1\n",
    "        embeds = self.word_embeddings(sentence)  \n",
    "        #embeds shape: [len(sentence), embdeddin_dim]\n",
    "  \n",
    "        \n",
    "        #input shape: [len(sentence),1,embedding_dim] (L,N,Hin​) when batch_first=False)\n",
    "        #input deocder shape: torch.Size([1, 1, 64]\n",
    "        rnn_out, hidden_state_out = self.rnn(embeds.view(len(sentence), 1, -1),hidden) \n",
    "        #rnn_out shape: [len(sentence),1,hidden_dim] \n",
    "        #hiddsen_state_out shape: [1,1,hidden_shape]\n",
    "\n",
    "        #input shape: [len(sentence),hidden_dim]\n",
    "        tag_space = self.hidden2tag(rnn_out.view(len(sentence), -1))\n",
    "        #tag_shape : (len(sentence),target_size)\n",
    "        \n",
    "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
    "\n",
    "        #print(\"LEAVING DECODER\")\n",
    "        return tag_scores,hidden_state_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNTagger_seq2seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, target_size,device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        self.target_size =  target_size\n",
    "        \n",
    "        \n",
    "    def forward(self, sentence, gt_pos_tags):\n",
    "        \n",
    "\n",
    "        outputs = torch.zeros(len(sentence), self.target_size).to(self.device)\n",
    "        \n",
    "        #print(\"for encoder input:\" , sentence.shape)  --- torch.Size([len(sentence)])\n",
    "        encoder_out, hidden = self.encoder(sentence)\n",
    "        #print(\"encoder_out shape:\", encoder_out.shape)  --- torch.Size([len(sentence), 1, hidden_dim])\n",
    "        #print(\"hidden shape:\", hidden.shape)  -- torch.Size([1, 1, hidden_dim])\n",
    "        \n",
    "\n",
    "        index = 0\n",
    "        for token in sentence:\n",
    "            \n",
    "            token = token.unsqueeze(0)\n",
    "            token = token.to(device = self.device)\n",
    "\n",
    "\n",
    "            #print(\"input to decoder:\" ,token.shape)  -- torch.Size([1])\n",
    "            #print(\"hiddden input shape:\", hidden.shape) --torch.Size([1, 1, hidden_dim])\n",
    "            output, hidden = self.decoder(token, hidden)\n",
    "            #print(\"output shape:\", output.shape) --torch.Size([1, target_size])\n",
    "            #print(\"hidden shape:\", hidden.shape) --torch.Size([1, 1, hidden_dim])\n",
    "            \n",
    "            #place predictions in a tensor holding predictions for each token\n",
    "            outputs[index] = output\n",
    "            index += 1\n",
    "\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(model,loss_function,optimizer,device,X,Y):\n",
    "    train_length = len(X)\n",
    "    epoch_train_loss = 0 \n",
    "   \n",
    "    model.train()\n",
    "    for i in tqdm(range(train_length)):\n",
    "        sentence = X[i]\n",
    "        tags = Y[i]\n",
    "\n",
    "        model.zero_grad()\n",
    "\n",
    "        sentence_in = prepare_sequence(sentence, unique_words_dict)\n",
    "        targets = prepare_sequence(tags, unique_tags_dict)\n",
    "        sentence_in = sentence_in.to(device=device)\n",
    "        targets = targets.to(device = device)\n",
    "        #print(sentence_in)\n",
    "        #print(targets)\n",
    "\n",
    "        tag_scores = model(sentence_in,targets)\n",
    "\n",
    "        loss = loss_function(tag_scores, targets)\n",
    "        epoch_train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    return model, epoch_train_loss/train_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_loop(model,loss_function,device,X,Y):\n",
    "    val_length = len(X)\n",
    "    epoch_val_loss = 0 \n",
    "\n",
    "    for i in tqdm(range(val_length)):\n",
    "        sentence = X[i]\n",
    "        tags = Y[i]\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        sentence_in = prepare_sequence(sentence, unique_words_dict)\n",
    "        targets = prepare_sequence(tags, unique_tags_dict)\n",
    "        sentence_in = sentence_in.to(device=device)\n",
    "        targets = targets.to(device = device)\n",
    "\n",
    "        tag_scores = model(sentence_in,targets)\n",
    "        #print(tag_scores)\n",
    "        #print(targets)\n",
    "        #print(tag_scores.shape)\n",
    "        #print(targets.shape)\n",
    "        #tag_scores shape : torch.Size([len(sentence), 12])\n",
    "        #targets shape: torch.Size([len(sentence)])\n",
    "        #CALL A FUNCTION WITH tag_scores and targets, GET PRECISION RECALL FScores        \n",
    "\n",
    "        loss = loss_function(tag_scores, targets)\n",
    "        epoch_val_loss += loss.item()\n",
    "          \n",
    "     \n",
    "    \n",
    "    return epoch_val_loss/val_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "HIDDEN_DIM = 64\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "enc = RNNTagger_encoder(HIDDEN_DIM, len(unique_tags_dict.keys()))\n",
    "dec = RNNTagger_decoder(HIDDEN_DIM,len(unique_tags_dict.keys()))\n",
    "model = RNNTagger_seq2seq(enc, dec, len(unique_tags_dict.keys()),device).to(device=device)\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_SIZE = 0.1\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=TEST_SIZE, random_state=4)\n",
    "\n",
    "VALID_SIZE = 0.15\n",
    "X_train, X_validation, Y_train, Y_validation = train_test_split(X_train, Y_train, test_size=VALID_SIZE, random_state=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 831/55233 [00:14<15:56, 56.87it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[97], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m epochs \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m      2\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epochs):\n\u001b[1;32m----> 3\u001b[0m     model , train_loss \u001b[39m=\u001b[39m train_loop(model,loss_function,optimizer,device,X_train,Y_train)\n\u001b[0;32m      4\u001b[0m     val_loss \u001b[39m=\u001b[39m validation_loop(model,loss_function,device,X_validation,Y_validation)\n\u001b[0;32m      5\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mFor epoch \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, training loss: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, validation loss: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(epoch, train_loss, val_loss))\n",
      "Cell \u001b[1;32mIn[93], line 19\u001b[0m, in \u001b[0;36mtrain_loop\u001b[1;34m(model, loss_function, optimizer, device, X, Y)\u001b[0m\n\u001b[0;32m     15\u001b[0m targets \u001b[39m=\u001b[39m targets\u001b[39m.\u001b[39mto(device \u001b[39m=\u001b[39m device)\n\u001b[0;32m     16\u001b[0m \u001b[39m#print(sentence_in)\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[39m#print(targets)\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m tag_scores \u001b[39m=\u001b[39m model(sentence_in,targets)\n\u001b[0;32m     21\u001b[0m loss \u001b[39m=\u001b[39m loss_function(tag_scores, targets)\n\u001b[0;32m     22\u001b[0m epoch_train_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n",
      "File \u001b[1;32md:\\sem8\\CS772\\A2_Part1\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[92], line 31\u001b[0m, in \u001b[0;36mRNNTagger_seq2seq.forward\u001b[1;34m(self, sentence, gt_pos_tags)\u001b[0m\n\u001b[0;32m     26\u001b[0m token \u001b[39m=\u001b[39m token\u001b[39m.\u001b[39mto(device \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m     29\u001b[0m \u001b[39m#print(\"input to decoder:\" ,token.shape)  -- torch.Size([1])\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[39m#print(\"hiddden input shape:\", hidden.shape) --torch.Size([1, 1, hidden_dim])\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m output, hidden \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdecoder(token, hidden)\n\u001b[0;32m     32\u001b[0m \u001b[39m#print(\"output shape:\", output.shape) --torch.Size([1, target_size])\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \u001b[39m#print(\"hidden shape:\", hidden.shape) --torch.Size([1, 1, hidden_dim])\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \n\u001b[0;32m     35\u001b[0m \u001b[39m#place predictions in a tensor holding predictions for each token\u001b[39;00m\n\u001b[0;32m     36\u001b[0m outputs[index] \u001b[39m=\u001b[39m output\n",
      "File \u001b[1;32md:\\sem8\\CS772\\A2_Part1\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[91], line 30\u001b[0m, in \u001b[0;36mRNNTagger_decoder.forward\u001b[1;34m(self, sentence, hidden)\u001b[0m\n\u001b[0;32m     25\u001b[0m rnn_out, hidden_state_out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrnn(embeds\u001b[39m.\u001b[39mview(\u001b[39mlen\u001b[39m(sentence), \u001b[39m1\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m),hidden) \n\u001b[0;32m     26\u001b[0m \u001b[39m#rnn_out shape: [len(sentence),1,hidden_dim] \u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[39m#hiddsen_state_out shape: [1,1,hidden_shape]\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \n\u001b[0;32m     29\u001b[0m \u001b[39m#input shape: [len(sentence),hidden_dim]\u001b[39;00m\n\u001b[1;32m---> 30\u001b[0m tag_space \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhidden2tag(rnn_out\u001b[39m.\u001b[39;49mview(\u001b[39mlen\u001b[39;49m(sentence), \u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m))\n\u001b[0;32m     31\u001b[0m \u001b[39m#tag_shape : (len(sentence),target_size)\u001b[39;00m\n\u001b[0;32m     33\u001b[0m tag_scores \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mlog_softmax(tag_space, dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32md:\\sem8\\CS772\\A2_Part1\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32md:\\sem8\\CS772\\A2_Part1\\.venv\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 1\n",
    "for epoch in range(epochs):\n",
    "    model , train_loss = train_loop(model,loss_function,optimizer,device,X_train,Y_train)\n",
    "    val_loss = validation_loop(model,loss_function,device,X_validation,Y_validation)\n",
    "    print(\"For epoch {}, training loss: {}, validation loss: {}\".format(epoch, train_loss, val_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.3845, -2.3303, -2.8299, -2.4387, -2.6026, -2.7334, -2.6120, -2.3255,\n",
      "         -2.3966, -2.4169, -2.4293, -2.4559],\n",
      "        [-2.7175, -2.6580, -2.5621, -2.3863, -2.5782, -2.3435, -2.6712, -2.1735,\n",
      "         -2.4823, -2.4160, -2.3058, -2.6970],\n",
      "        [-2.5445, -2.6496, -2.4775, -2.3125, -2.3808, -2.1432, -2.7440, -2.4927,\n",
      "         -2.4781, -2.6070, -2.5448, -2.5911],\n",
      "        [-2.6723, -2.5064, -2.5053, -2.3066, -2.6649, -2.6963, -2.4045, -2.5247,\n",
      "         -2.4774, -2.6542, -2.1041, -2.4725],\n",
      "        [-2.6179, -2.5713, -2.3824, -2.4313, -2.5005, -2.4331, -2.5099, -2.5523,\n",
      "         -2.4298, -2.3233, -2.3330, -2.8411],\n",
      "        [-2.6331, -2.5447, -2.5990, -2.3327, -2.5631, -2.4084, -2.4967, -2.2393,\n",
      "         -2.5961, -2.4588, -2.4920, -2.5318],\n",
      "        [-2.5359, -2.6855, -2.8276, -2.2516, -2.3059, -2.0337, -2.7347, -2.5788,\n",
      "         -2.4444, -2.6241, -2.5422, -2.5387],\n",
      "        [-2.6981, -2.4284, -2.4977, -2.6246, -2.5981, -2.4926, -2.7112, -2.2066,\n",
      "         -2.3961, -2.4557, -2.2733, -2.5735]], device='cuda:0',\n",
      "       grad_fn=<CopySlices>)\n"
     ]
    }
   ],
   "source": [
    "#model = Seq2Seq(enc, dec, device).to(device)\n",
    "\n",
    "# seq1 = \"everybody read the book and ate the food\".split()\n",
    "# inputs = prepare_sequence(seq1, unique_words_dict)\n",
    "# pos_tags = \"adv adv adv\"\n",
    "# tag_scores = model(inputs,pos_tags)\n",
    "# print(tag_scores)\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     for seq in [seq1, seq2]:\n",
    "#         print(seq)\n",
    "#         inputs = prepare_sequence(seq, unique_words_dict)\n",
    "#         #print(\"PRINTING INPUTS:\", inputs)\n",
    "#         tag_scores = model(inputs)\n",
    "#         #print(\"TAG SCORES:\", tag_scores)\n",
    "#         #print(\"TAG SHAPE:\", tag_scores.shape) #Shape is (len(senetence),target_size)\n",
    "\n",
    "\n",
    "#         _, indices = torch.max(tag_scores, 1) \n",
    "#         print(indices) #Returns index of maximum\n",
    "#         ret = []\n",
    "#         for i in range(len(indices)):\n",
    "#             for key, value in unique_tags_dict.items():\n",
    "#                 if indices[i] == value:\n",
    "#                     ret.append((seq[i], key))\n",
    "#         print(ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
