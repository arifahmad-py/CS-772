{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\sem8\\CS772\\A2_Part1\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import pickle\n",
    "from gensim.models import KeyedVectors\n",
    "from nltk.corpus import brown\n",
    "from nltk.corpus import treebank\n",
    "from nltk.corpus import conll2000\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "treebank_corpus = treebank.tagged_sents(tagset='universal')\n",
    "brown_corpus = brown.tagged_sents(tagset='universal')\n",
    "conll_corpus = conll2000.tagged_sents(tagset='universal')\n",
    "tagged_sentences = treebank_corpus + brown_corpus + conll_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Pierre', 'NOUN'),\n",
       " ('Vinken', 'NOUN'),\n",
       " (',', '.'),\n",
       " ('61', 'NUM'),\n",
       " ('years', 'NOUN'),\n",
       " ('old', 'ADJ'),\n",
       " (',', '.'),\n",
       " ('will', 'VERB'),\n",
       " ('join', 'VERB'),\n",
       " ('the', 'DET'),\n",
       " ('board', 'NOUN'),\n",
       " ('as', 'ADP'),\n",
       " ('a', 'DET'),\n",
       " ('nonexecutive', 'ADJ'),\n",
       " ('director', 'NOUN'),\n",
       " ('Nov.', 'NOUN'),\n",
       " ('29', 'NUM'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged_sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [] # store input sequence\n",
    "Y = [] # store output sequence\n",
    "\n",
    "for sentence in tagged_sentences:\n",
    "    X_sentence = []\n",
    "    Y_sentence = []\n",
    "    for entity in sentence:         \n",
    "        X_sentence.append(entity[0])  # entity[0] contains the word\n",
    "        Y_sentence.append(entity[1])  # entity[1] contains corresponding tag\n",
    "        \n",
    "    X.append(X_sentence)\n",
    "    Y.append(Y_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Pierre',\n",
       " 'Vinken',\n",
       " ',',\n",
       " '61',\n",
       " 'years',\n",
       " 'old',\n",
       " ',',\n",
       " 'will',\n",
       " 'join',\n",
       " 'the',\n",
       " 'board',\n",
       " 'as',\n",
       " 'a',\n",
       " 'nonexecutive',\n",
       " 'director',\n",
       " 'Nov.',\n",
       " '29',\n",
       " '.']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59448\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "num_words = len(set([word.lower() for sentence in X for word in sentence]))\n",
    "num_tags   = len(set([word.lower() for sentence in Y for word in sentence]))\n",
    "print(num_words)\n",
    "print(num_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'conj': 1, 'verb': 2, 'adj': 3, 'det': 4, 'prt': 5, 'x': 6, 'pron': 7, 'adv': 8, 'adp': 9, '.': 10, 'num': 11, 'noun': 12}\n"
     ]
    }
   ],
   "source": [
    "unique_tags = list(set([word.lower() for sentence in Y for word in sentence]))\n",
    "unique_tags_dict = {}\n",
    "index = 1\n",
    "for tag in unique_tags:\n",
    "    unique_tags_dict[tag] = index \n",
    "    index += 1\n",
    "print(unique_tags_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59448\n"
     ]
    }
   ],
   "source": [
    "unique_words = list(set([word.lower() for sentence in X for word in sentence]))\n",
    "unique_words_dict = {}\n",
    "index = 1\n",
    "for word in unique_words:\n",
    "    unique_words_dict[word] = index \n",
    "    index += 1\n",
    "print(len(unique_words_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59448\n"
     ]
    }
   ],
   "source": [
    "values = []\n",
    "for i in unique_words_dict.keys():\n",
    "    values.append(unique_words_dict[i])\n",
    "\n",
    "print(max(values))\n",
    "max_value_dict = max(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequence_sentence(seq, to_ix):\n",
    "    \"\"\"Input: takes in a list of words, and a dictionary containing the index of the words\n",
    "    Output: a tensor containing the indexes of the word\"\"\"\n",
    "    idxs = [to_ix[w.lower()] for w in seq]\n",
    "    random_index = random.randint(0,len(idxs)-1)\n",
    "    idxs[random_index] = max_value_dict + 1\n",
    "    return idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequence_tags(seq, to_ix):\n",
    "    \"\"\"Input: takes in a list of words, and a dictionary containing the index of the words\n",
    "    Output: a tensor containing the indexes of the word\"\"\"\n",
    "    idxs = [to_ix[w.lower()] for w in seq]\n",
    "    return idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "from keras_preprocessing.sequence import pad_sequences\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "def split(list_a, batch_size):\n",
    "\n",
    "  for i in range(0, len(list_a), batch_size):\n",
    "    yield list_a[i:i + batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "X_batches = list(split(X, batch_size))\n",
    "Y_batches = list(split(Y,batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_batches_padded = []\n",
    "Y_batches_padded = []\n",
    "max_length_list = []\n",
    "\n",
    "for b_s,b_t in zip(X_batches,Y_batches):\n",
    "    max_seq_length = 0\n",
    "    for sentence in b_s:\n",
    "        if len(sentence) > max_seq_length:\n",
    "            max_seq_length = len(sentence)\n",
    "    \n",
    "    sen_encoded = []\n",
    "    tag_encoded = []\n",
    "    for sentence,tags in zip(b_s,b_t):\n",
    "        sen_encoded.append(prepare_sequence_sentence(sentence, unique_words_dict))\n",
    "        tag_encoded.append(prepare_sequence_tags(tags, unique_tags_dict))\n",
    "    \n",
    "    X_batches_padded.append(pad_sequences(sen_encoded, maxlen=max_seq_length, padding=\"pre\", truncating=\"post\"))\n",
    "    Y_batches_padded.append(pad_sequences(tag_encoded, maxlen=max_seq_length, padding=\"pre\", truncating=\"post\"))\n",
    "    max_length_list.append(max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9026\n",
      "9026\n",
      "9026\n"
     ]
    }
   ],
   "source": [
    "print(len(Y_batches_padded))\n",
    "print(len(X_batches_padded))\n",
    "print(len(max_length_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(Y_batches_padded[0]))\n",
    "len(X_batches_padded[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_final = []\n",
    "Y_final = []\n",
    "\n",
    "for index in range(len(X_batches_padded)):\n",
    "    X_batch_tensor = torch.zeros((batch_size,max_length_list[index]),dtype = int).to(device= device)\n",
    "    Y_batch_tensor = torch.zeros((batch_size,max_length_list[index]), dtype = int).to(device = device)\n",
    "\n",
    "    count = 0\n",
    "    for x, y in zip(X_batches_padded[index],Y_batches_padded[index]):\n",
    "        X_batch_tensor[count] = torch.tensor(x).to(device = device)\n",
    "        Y_batch_tensor[count] = torch.tensor(y).to(device =device)\n",
    "        count += 1\n",
    "    \n",
    "    X_final.append(X_batch_tensor)\n",
    "    Y_final.append(Y_batch_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([59450, 300])\n"
     ]
    }
   ],
   "source": [
    "EMBEDDING_SIZE  = 300  # each word in word2vec model is represented using a 300 dimensional vector\n",
    "VOCABULARY_SIZE = num_words + 2\n",
    "\n",
    "with open('./embedding_weights.pickle', 'rb') as file:\n",
    "    embedding_weights = pickle.load(file)\n",
    "\n",
    "print(embedding_weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_emb_layer(weights_matrix, non_trainable=False):\n",
    "\n",
    "    num_embeddings, embedding_dim = weights_matrix.size()\n",
    "    emb_layer = nn.Embedding(num_embeddings, embedding_dim)\n",
    "    emb_layer.load_state_dict({'weight': weights_matrix})\n",
    "    \n",
    "    if non_trainable:\n",
    "        emb_layer.weight.requires_grad = False\n",
    "\n",
    "    return emb_layer, num_embeddings, embedding_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNTagger(nn.Module):\n",
    "    def __init__(self, hidden_dim, target_size, batch_size,num_layers):\n",
    "        super(RNNTagger, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.word_embeddings, vocab_size, embedding_dim = create_emb_layer(embedding_weights, True)\n",
    "        #self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        self.rnn = nn.GRU(embedding_dim, hidden_dim, batch_first = True, num_layers = num_layers)\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, target_size)\n",
    "        \n",
    "    def forward(self, sentence):\n",
    "        #print(\"REACHONG FORWARD\")\n",
    "\n",
    "        #Input shape: [batch_size,max_length in that batch]\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        #print(\"DONE embeds:\", embeds.shape)  -- torch.Size([batch_Size, max_length_in_that_batch, embedding_dim])\n",
    "  \n",
    "        \n",
    "        #input shape: [len(sentence),1,embedding_dim] (L,N,Hin​) when batch_first=False)\n",
    "    \n",
    "        rnn_out, hidden_state_out = self.rnn(embeds) \n",
    "        #print(rnn_out.shape)  -- torch.Size([8, 45, 64])\n",
    "        #print(hidden_state_out.shape) --torch.Size([1, 8, 64])\n",
    "\n",
    "\n",
    "        #input shape: -- torch.Size([8, 45, 64])\n",
    "        tag_space = self.hidden2tag(rnn_out)\n",
    "        #print(\"DONE LINEAR LAYER: \", tag_space.shape) --torch.Size([8, 45, 13])\n",
    "        \n",
    "        tag_scores = F.log_softmax(tag_space, dim=2)\n",
    "        #print(\"DONE SOFTMAX:\", tag_scores.shape) --torch.Size([8, 45, 13])\n",
    "        \n",
    "        return tag_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(model,loss_function,optimizer,device,X,Y):\n",
    "    train_length = len(X)\n",
    "    epoch_train_loss = 0 \n",
    "   \n",
    "    model.train()\n",
    "    for i in tqdm(range(train_length)):\n",
    "        sentence_batch = X[i]\n",
    "        tags_batch = Y[i]\n",
    "\n",
    "        model.zero_grad()\n",
    "\n",
    "        #print(\"Input shape:\",  sentence_batch.shape) --torch.Size([batch_size, max_length_in_batch]) \n",
    "        tag_scores = model(sentence_batch)\n",
    "\n",
    "       \n",
    "        #print(\"model output shape: \", tag_scores.shape) -- torch.Size([8, 45, 13]) \n",
    "        #print(\"tags batch shape: \", tags_batch.shape) --torch.Size([8, 45])\n",
    "        new1 = tag_scores.transpose(1,2)\n",
    "        #print(\"new input shape:\" , new1.shape) --torch.Size([8, 13, 45])\n",
    "\n",
    "        #print(\"STARTING LOSS FUNCTION\")\n",
    "        loss = loss_function(new1 , tags_batch)\n",
    "        #print(\"DONE WITH LOSS FUNCTION\")\n",
    "        #print(loss)\n",
    "        epoch_train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    return model, epoch_train_loss/train_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_loop(model,loss_function,device,X,Y):\n",
    "    val_length = len(X)\n",
    "    epoch_val_loss = 0 \n",
    "\n",
    "    for i in tqdm(range(val_length)):\n",
    "        sentence_batch = X[i]\n",
    "        tags_batch = Y[i]\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        tag_scores = model(sentence_batch)\n",
    "\n",
    "        #CALL A FUNCTION WITH tag_scores and targets, GET PRECISION RECALL FScores        \n",
    "\n",
    "        new1 = tag_scores.transpose(1,2) \n",
    "        loss = loss_function(new1, tags_batch)\n",
    "        epoch_val_loss += loss.item()\n",
    "          \n",
    "     \n",
    "    \n",
    "    return epoch_val_loss/val_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "HIDDEN_DIM = 64\n",
    "batch_size = 8\n",
    "num_layers =  2\n",
    "model =RNNTagger(HIDDEN_DIM, len(unique_tags_dict.keys())+1, batch_size,num_layers)\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "model = model.to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_SIZE = 0.1\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_final, Y_final, test_size=TEST_SIZE, random_state=4)\n",
    "\n",
    "VALID_SIZE = 0.15\n",
    "X_train, X_validation, Y_train, Y_validation = train_test_split(X_train, Y_train, test_size=VALID_SIZE, random_state=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6904/6904 [00:15<00:00, 436.30it/s]\n",
      "100%|██████████| 1219/1219 [00:00<00:00, 1390.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 0, training loss: 0.4518412434089139, validation loss: 0.3371057066213801\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6904/6904 [00:13<00:00, 519.72it/s]\n",
      "100%|██████████| 1219/1219 [00:00<00:00, 1369.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 1, training loss: 0.3042854023978615, validation loss: 0.2888657215484544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6904/6904 [00:14<00:00, 473.07it/s]\n",
      "100%|██████████| 1219/1219 [00:00<00:00, 1513.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 2, training loss: 0.26870788583156424, validation loss: 0.264800105063616\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6904/6904 [00:12<00:00, 538.07it/s]\n",
      "100%|██████████| 1219/1219 [00:00<00:00, 1393.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 3, training loss: 0.24785828086043582, validation loss: 0.2496536498900023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6904/6904 [00:12<00:00, 566.89it/s]\n",
      "100%|██████████| 1219/1219 [00:00<00:00, 1274.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 4, training loss: 0.23349588425605539, validation loss: 0.2387117083758189\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "    model , train_loss = train_loop(model,loss_function,optimizer,device,X_train,Y_train)\n",
    "    val_loss = validation_loop(model,loss_function,device,X_validation,Y_validation)\n",
    "    print(\"For epoch {}, training loss: {}, validation loss: {}\".format(epoch, train_loss, val_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6904/6904 [00:16<00:00, 421.63it/s]\n",
      "100%|██████████| 1219/1219 [00:01<00:00, 1167.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 0, training loss: 0.5444462966737214, validation loss: 0.35320354160761813\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6904/6904 [00:12<00:00, 547.63it/s]\n",
      "100%|██████████| 1219/1219 [00:00<00:00, 1287.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 1, training loss: 0.31087625902610144, validation loss: 0.2936107037715779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6904/6904 [00:12<00:00, 550.79it/s]\n",
      "100%|██████████| 1219/1219 [00:00<00:00, 1314.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 2, training loss: 0.2672839114433834, validation loss: 0.26517777010443955\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6904/6904 [00:12<00:00, 541.25it/s]\n",
      "100%|██████████| 1219/1219 [00:00<00:00, 1338.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 3, training loss: 0.24279792084027327, validation loss: 0.24739950303663671\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6904/6904 [00:12<00:00, 545.18it/s]\n",
      "100%|██████████| 1219/1219 [00:00<00:00, 1299.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 4, training loss: 0.22620750430038983, validation loss: 0.23453835503085899\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "    model , train_loss = train_loop(model,loss_function,optimizer,device,X_train,Y_train)\n",
    "    val_loss = validation_loop(model,loss_function,device,X_validation,Y_validation)\n",
    "    print(\"For epoch {}, training loss: {}, validation loss: {}\".format(epoch, train_loss, val_loss))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DEMO CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_TESTsequence_sentence(seq, to_ix):\n",
    "    values = []\n",
    "    for i in to_ix.keys():\n",
    "        values.append(to_ix[i])\n",
    "    \n",
    "    max_value_dict = max(values)\n",
    "    \n",
    "    idxs = []\n",
    "\n",
    "    for w in seq:\n",
    "        if w.lower() in to_ix.keys():\n",
    "            idxs.append(to_ix[w.lower()])\n",
    "        else:\n",
    "            idxs.append(max_value_dict+1)\n",
    "    return idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 'conj',\n",
       " 2: 'verb',\n",
       " 3: 'adj',\n",
       " 4: 'det',\n",
       " 5: 'prt',\n",
       " 6: 'x',\n",
       " 7: 'pron',\n",
       " 8: 'adv',\n",
       " 9: 'adp',\n",
       " 10: '.',\n",
       " 11: 'num',\n",
       " 12: 'noun',\n",
       " 0: '0'}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_to_tags = {}\n",
    "for tag in unique_tags_dict:\n",
    "    index_to_tags[unique_tags_dict[tag]] = tag \n",
    "index_to_tags[0] = '0'\n",
    "index_to_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 'conj',\n",
       " 2: 'verb',\n",
       " 3: 'adj',\n",
       " 4: 'det',\n",
       " 5: 'prt',\n",
       " 6: 'x',\n",
       " 7: 'pron',\n",
       " 8: 'adv',\n",
       " 9: 'adp',\n",
       " 10: '.',\n",
       " 11: 'num',\n",
       " 12: 'noun',\n",
       " 0: '0'}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_to_tags = {}\n",
    "for tag in unique_tags_dict:\n",
    "    index_to_tags[unique_tags_dict[tag]] = tag \n",
    "index_to_tags[0] = '0'\n",
    "index_to_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bottle', 'is', '5', 'years', 'old', ',', 'non', 'executive', 'director', 'of', 'CSK']\n",
      "[53260, 45938, 42439, 1493, 49622, 25353, 2998, 34501, 34780, 7540, 59449]\n",
      "bottle noun\n",
      "is verb\n",
      "5 num\n",
      "years noun\n",
      "old adj\n",
      ", .\n",
      "non noun\n",
      "executive noun\n",
      "director noun\n",
      "of adp\n",
      "CSK det\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "text = \"bottle is 5 years old, non executive director of CSK\"\n",
    "tokenized_seq = word_tokenize(text)\n",
    "print(tokenized_seq)\n",
    "\n",
    "tokens= prepare_TESTsequence_sentence(tokenized_seq,unique_words_dict)\n",
    "print(tokens) \n",
    "\n",
    "tokens = torch.tensor(tokens).to(device = device)\n",
    "tokens = tokens.unsqueeze(0)\n",
    "output = model(tokens)\n",
    "output = output.squeeze(0)\n",
    "tag_index = torch.argmax(output,dim=1)\n",
    "#print(tag_index.shape)\n",
    "#print(output.shape)\n",
    "for predicted_tag,word in zip(tag_index,tokenized_seq):\n",
    "    predicted_tag = predicted_tag.item()\n",
    "    predicted_tag = index_to_tags[predicted_tag]\n",
    "    print(word, predicted_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
