{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\sem8\\CS772\\A2_Part1\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import pickle\n",
    "from gensim.models import KeyedVectors\n",
    "from nltk.corpus import brown\n",
    "from nltk.corpus import treebank\n",
    "from nltk.corpus import conll2000\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "treebank_corpus = treebank.tagged_sents(tagset='universal')\n",
    "brown_corpus = brown.tagged_sents(tagset='universal')\n",
    "conll_corpus = conll2000.tagged_sents(tagset='universal')\n",
    "tagged_sentences = treebank_corpus + brown_corpus + conll_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Pierre', 'NOUN'),\n",
       " ('Vinken', 'NOUN'),\n",
       " (',', '.'),\n",
       " ('61', 'NUM'),\n",
       " ('years', 'NOUN'),\n",
       " ('old', 'ADJ'),\n",
       " (',', '.'),\n",
       " ('will', 'VERB'),\n",
       " ('join', 'VERB'),\n",
       " ('the', 'DET'),\n",
       " ('board', 'NOUN'),\n",
       " ('as', 'ADP'),\n",
       " ('a', 'DET'),\n",
       " ('nonexecutive', 'ADJ'),\n",
       " ('director', 'NOUN'),\n",
       " ('Nov.', 'NOUN'),\n",
       " ('29', 'NUM'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged_sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [] # store input sequence\n",
    "Y = [] # store output sequence\n",
    "\n",
    "for sentence in tagged_sentences:\n",
    "    X_sentence = []\n",
    "    Y_sentence = []\n",
    "    for entity in sentence:         \n",
    "        X_sentence.append(entity[0])  # entity[0] contains the word\n",
    "        Y_sentence.append(entity[1])  # entity[1] contains corresponding tag\n",
    "        \n",
    "    X.append(X_sentence)\n",
    "    Y.append(Y_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Pierre',\n",
       " 'Vinken',\n",
       " ',',\n",
       " '61',\n",
       " 'years',\n",
       " 'old',\n",
       " ',',\n",
       " 'will',\n",
       " 'join',\n",
       " 'the',\n",
       " 'board',\n",
       " 'as',\n",
       " 'a',\n",
       " 'nonexecutive',\n",
       " 'director',\n",
       " 'Nov.',\n",
       " '29',\n",
       " '.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59448\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "num_words = len(set([word.lower() for sentence in X for word in sentence]))\n",
    "num_tags   = len(set([word.lower() for sentence in Y for word in sentence]))\n",
    "print(num_words)\n",
    "print(num_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'.': 1, 'adv': 2, 'num': 3, 'adj': 4, 'x': 5, 'pron': 6, 'det': 7, 'conj': 8, 'adp': 9, 'verb': 10, 'prt': 11, 'noun': 12}\n"
     ]
    }
   ],
   "source": [
    "unique_tags = list(set([word.lower() for sentence in Y for word in sentence]))\n",
    "unique_tags_dict = {}\n",
    "index = 1\n",
    "for tag in unique_tags:\n",
    "    unique_tags_dict[tag] = index \n",
    "    index += 1\n",
    "print(unique_tags_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59448\n"
     ]
    }
   ],
   "source": [
    "unique_words = list(set([word.lower() for sentence in X for word in sentence]))\n",
    "unique_words_dict = {}\n",
    "index = 1\n",
    "for word in unique_words:\n",
    "    unique_words_dict[word] = index \n",
    "    index += 1\n",
    "print(len(unique_words_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequence(seq, to_ix):\n",
    "    \"\"\"Input: takes in a list of words, and a dictionary containing the index of the words\n",
    "    Output: a tensor containing the indexes of the word\"\"\"\n",
    "    idxs = [to_ix[w.lower()] for w in seq]\n",
    "    return idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "from keras_preprocessing.sequence import pad_sequences\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "def split(list_a, batch_size):\n",
    "\n",
    "  for i in range(0, len(list_a), batch_size):\n",
    "    yield list_a[i:i + batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "X_batches = list(split(X, batch_size))\n",
    "Y_batches = list(split(Y,batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_batches_padded = []\n",
    "Y_batches_padded = []\n",
    "max_length_list = []\n",
    "\n",
    "for b_s,b_t in zip(X_batches,Y_batches):\n",
    "    max_seq_length = 0\n",
    "    for sentence in b_s:\n",
    "        if len(sentence) > max_seq_length:\n",
    "            max_seq_length = len(sentence)\n",
    "    \n",
    "    sen_encoded = []\n",
    "    tag_encoded = []\n",
    "    for sentence,tags in zip(b_s,b_t):\n",
    "        sen_encoded.append(prepare_sequence(sentence, unique_words_dict))\n",
    "        tag_encoded.append(prepare_sequence(tags, unique_tags_dict))\n",
    "    \n",
    "    X_batches_padded.append(pad_sequences(sen_encoded, maxlen=max_seq_length, padding=\"pre\", truncating=\"post\"))\n",
    "    Y_batches_padded.append(pad_sequences(tag_encoded, maxlen=max_seq_length, padding=\"pre\", truncating=\"post\"))\n",
    "    max_length_list.append(max_seq_length)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9026\n",
      "9026\n",
      "9026\n"
     ]
    }
   ],
   "source": [
    "print(len(Y_batches_padded))\n",
    "print(len(X_batches_padded))\n",
    "print(len(max_length_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(Y_batches_padded[0]))\n",
    "len(X_batches_padded[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 12\n",
      "  12  1  3 12  4  1 10 10  7 12  9  7  4 12 12  3  1]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0 12 12 10 12  9 12 12  1  7 12 10 12  1]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0 12 12  1  3 12  4  8  4 12  9\n",
      "  12 12 12 12  1 10 10  5  7  4 12  9  7  4  4 12  1]\n",
      " [ 7 12  9 12  2 10  5  5 11 10 12 12 12 10 10  7  4 12  9 12 12  9  7 12\n",
      "   9 12 10  5 11  6  2  9  3 12  9  1 12 10  5  5  1]\n",
      " [ 0  0  0  0  0  0  7 12 12  1 12  1 10  2  4  9  6 10  7 12  1  9  2  4\n",
      "  12 11  6 10 12  7  5 10 11 12  4  1 12 10  5  5  1]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0 12 12  1  7 12  9  4  4 12 12\n",
      "   7  5 10 12 12  1 10 10 12  9  6 12 12 12  9  3  1]\n",
      " [ 0  0  0  0  9  4 12 10 10  5  2  9  7 12  9  1  7  4 12 10  9 12 11 12\n",
      "  12 12  9 12  1  7 12  4  5 11 10  4 12 11  7 12  1]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  7 12 12 10  1  1  7 10  7  4 12  1]]\n",
      "[[    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0  9785\n",
      "  46026 10989 34540  6101 39308 10989 35685 31554 40706 38909 11132 20330\n",
      "  46669 49264 10005  6551 25158]\n",
      " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0 58022 46026 51213 21504 17399 36819 10432 10989\n",
      "  40706 24412 56650 25659 25158]\n",
      " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0 10027 18005 10989 45004  6101 39308 12198 23697 21504 17399\n",
      "  51843 33092 56958 21507 10989 51735 33768 58899 20330 46669 49264 17399\n",
      "  51295 33645 25588 46900 25158]\n",
      " [20330 41862 17399 51184 47140 30809 33078 33078 22995 53413 18968  6492\n",
      "  35000 27354 45610 20330 39664  3367 17399 14924 34195 50054 20330 25659\n",
      "  17399 34762 45912 33078 22995 20487 11714 42367   346  6101 17304 10989\n",
      "   5188 49961 12223 10110 25158]\n",
      " [    0     0     0     0     0     0 40706 51184  4608 10989 32950 10989\n",
      "  51213 37001 44114 47140 20487 48995 40706 46979 10989 45427 33989 11889\n",
      "   7724 22995 20487 45357 31708  4679 10110 57074 19555 49186 29128 10989\n",
      "   5188 29244 12223 43987 25158]\n",
      " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0 12449  8905 10989 40706 51656 17399  3434 54724 55320 25179\n",
      "   4679 43987 46526 18968 51103 10989 52362 51163 32950 20403 27002 31168\n",
      "   6492 35000 20403 58715 25158]\n",
      " [    0     0     0     0   773 41493 50170 21834 49961 55910 11714 42367\n",
      "  20330 15057 17304 10989 40706 11623 30097 56525 20403 39125 36823  3434\n",
      "  49129 38705 17399 12528 10989 20330 57352 41598 33078 22995 24013  3434\n",
      "  30711 22995 40706 58226 25158]\n",
      " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0 20330 12449 38786 29244 10989 27557 51295\n",
      "  51213 43428 39308 50065 25158]]\n",
      "41\n"
     ]
    }
   ],
   "source": [
    "print(Y_batches_padded[0])\n",
    "print(X_batches_padded[0])\n",
    "print(max_length_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_final = []\n",
    "Y_final = []\n",
    "\n",
    "for index in range(len(X_batches_padded)):\n",
    "    X_batch_tensor = torch.zeros((batch_size,max_length_list[index]),dtype = int).to(device= device)\n",
    "    Y_batch_tensor = torch.zeros((batch_size,max_length_list[index]), dtype = int).to(device = device)\n",
    "\n",
    "    count = 0\n",
    "    for x, y in zip(X_batches_padded[index],Y_batches_padded[index]):\n",
    "        X_batch_tensor[count] = torch.tensor(x).to(device = device)\n",
    "        Y_batch_tensor[count] = torch.tensor(y).to(device =device)\n",
    "        count += 1\n",
    "    \n",
    "    X_final.append(X_batch_tensor)\n",
    "    Y_final.append(Y_batch_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9026\n",
      "9026\n",
      "torch.Size([8, 41])\n",
      "torch.Size([8, 41])\n",
      "torch.Size([41])\n",
      "torch.Size([41])\n",
      "torch.Size([8, 38])\n",
      "torch.Size([8, 38])\n",
      "tensor([[    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,  9785, 46026, 10989, 34540,  6101, 39308, 10989,\n",
      "         35685, 31554, 40706, 38909, 11132, 20330, 46669, 49264, 10005,  6551,\n",
      "         25158],\n",
      "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0, 58022, 46026,\n",
      "         51213, 21504, 17399, 36819, 10432, 10989, 40706, 24412, 56650, 25659,\n",
      "         25158],\n",
      "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0, 10027, 18005, 10989, 45004,  6101, 39308,\n",
      "         12198, 23697, 21504, 17399, 51843, 33092, 56958, 21507, 10989, 51735,\n",
      "         33768, 58899, 20330, 46669, 49264, 17399, 51295, 33645, 25588, 46900,\n",
      "         25158],\n",
      "        [20330, 41862, 17399, 51184, 47140, 30809, 33078, 33078, 22995, 53413,\n",
      "         18968,  6492, 35000, 27354, 45610, 20330, 39664,  3367, 17399, 14924,\n",
      "         34195, 50054, 20330, 25659, 17399, 34762, 45912, 33078, 22995, 20487,\n",
      "         11714, 42367,   346,  6101, 17304, 10989,  5188, 49961, 12223, 10110,\n",
      "         25158],\n",
      "        [    0,     0,     0,     0,     0,     0, 40706, 51184,  4608, 10989,\n",
      "         32950, 10989, 51213, 37001, 44114, 47140, 20487, 48995, 40706, 46979,\n",
      "         10989, 45427, 33989, 11889,  7724, 22995, 20487, 45357, 31708,  4679,\n",
      "         10110, 57074, 19555, 49186, 29128, 10989,  5188, 29244, 12223, 43987,\n",
      "         25158],\n",
      "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0, 12449,  8905, 10989, 40706, 51656, 17399,\n",
      "          3434, 54724, 55320, 25179,  4679, 43987, 46526, 18968, 51103, 10989,\n",
      "         52362, 51163, 32950, 20403, 27002, 31168,  6492, 35000, 20403, 58715,\n",
      "         25158],\n",
      "        [    0,     0,     0,     0,   773, 41493, 50170, 21834, 49961, 55910,\n",
      "         11714, 42367, 20330, 15057, 17304, 10989, 40706, 11623, 30097, 56525,\n",
      "         20403, 39125, 36823,  3434, 49129, 38705, 17399, 12528, 10989, 20330,\n",
      "         57352, 41598, 33078, 22995, 24013,  3434, 30711, 22995, 40706, 58226,\n",
      "         25158],\n",
      "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0, 20330,\n",
      "         12449, 38786, 29244, 10989, 27557, 51295, 51213, 43428, 39308, 50065,\n",
      "         25158]], device='cuda:0')\n",
      "tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0, 12, 12,  1,  3, 12,  4,  1, 10, 10,  7, 12,  9,  7,\n",
      "          4, 12, 12,  3,  1],\n",
      "        [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 12, 12, 10, 12,  9, 12, 12,  1,\n",
      "          7, 12, 10, 12,  1],\n",
      "        [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 12, 12,  1,  3,\n",
      "         12,  4,  8,  4, 12,  9, 12, 12, 12, 12,  1, 10, 10,  5,  7,  4, 12,  9,\n",
      "          7,  4,  4, 12,  1],\n",
      "        [ 7, 12,  9, 12,  2, 10,  5,  5, 11, 10, 12, 12, 12, 10, 10,  7,  4, 12,\n",
      "          9, 12, 12,  9,  7, 12,  9, 12, 10,  5, 11,  6,  2,  9,  3, 12,  9,  1,\n",
      "         12, 10,  5,  5,  1],\n",
      "        [ 0,  0,  0,  0,  0,  0,  7, 12, 12,  1, 12,  1, 10,  2,  4,  9,  6, 10,\n",
      "          7, 12,  1,  9,  2,  4, 12, 11,  6, 10, 12,  7,  5, 10, 11, 12,  4,  1,\n",
      "         12, 10,  5,  5,  1],\n",
      "        [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 12, 12,  1,  7,\n",
      "         12,  9,  4,  4, 12, 12,  7,  5, 10, 12, 12,  1, 10, 10, 12,  9,  6, 12,\n",
      "         12, 12,  9,  3,  1],\n",
      "        [ 0,  0,  0,  0,  9,  4, 12, 10, 10,  5,  2,  9,  7, 12,  9,  1,  7,  4,\n",
      "         12, 10,  9, 12, 11, 12, 12, 12,  9, 12,  1,  7, 12,  4,  5, 11, 10,  4,\n",
      "         12, 11,  7, 12,  1],\n",
      "        [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  7, 12, 12, 10,  1,  1,  7,\n",
      "         10,  7,  4, 12,  1]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(len(X_final))\n",
    "print(len(Y_final))\n",
    "print(X_final[0].shape)\n",
    "print(Y_final[0].shape)\n",
    "print(X_final[0][0].shape)\n",
    "print(Y_final[0][0].shape)\n",
    "print(X_final[1].shape)\n",
    "print(Y_final[1].shape)\n",
    "print(X_final[0])\n",
    "print(Y_final[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_batches = []\n",
    "# Y_batches = []\n",
    "# batch_size = 8\n",
    "\n",
    "\n",
    "# for sentence in X:\n",
    "#     X_encoded.append(prepare_sequence(sentence, unique_words_dict))\n",
    "# for tags in Y:\n",
    "#     Y_encoded.append(prepare_sequence(tags, unique_tags_dict))\n",
    "\n",
    "# MAX_SEQ_LENGTH = 100  # sequences greater than 100 in length will be truncated\n",
    "\n",
    "# X_padded = pad_sequences(X_encoded, maxlen=MAX_SEQ_LENGTH, padding=\"pre\", truncating=\"post\")\n",
    "# Y_padded = pad_sequences(Y_encoded, maxlen=MAX_SEQ_LENGTH, padding=\"pre\", truncating=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(X_padded))\n",
    "# X_padded[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(Y_encoded))\n",
    "# print(Y_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([59449, 300])\n"
     ]
    }
   ],
   "source": [
    "EMBEDDING_SIZE  = 300  # each word in word2vec model is represented using a 300 dimensional vector\n",
    "VOCABULARY_SIZE = num_words + 1\n",
    "\n",
    "with open('./embedding_weights.pickle', 'rb') as file:\n",
    "    embedding_weights = pickle.load(file)\n",
    "\n",
    "print(embedding_weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_emb_layer(weights_matrix, non_trainable=False):\n",
    "\n",
    "    num_embeddings, embedding_dim = weights_matrix.size()\n",
    "    emb_layer = nn.Embedding(num_embeddings, embedding_dim)\n",
    "    emb_layer.load_state_dict({'weight': weights_matrix})\n",
    "    \n",
    "    if non_trainable:\n",
    "        emb_layer.weight.requires_grad = False\n",
    "\n",
    "    return emb_layer, num_embeddings, embedding_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNTagger(nn.Module):\n",
    "    def __init__(self, hidden_dim, target_size, batch_size):\n",
    "        super(RNNTagger, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.word_embeddings, vocab_size, embedding_dim = create_emb_layer(embedding_weights, True)\n",
    "        #self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim, batch_first = True)\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, target_size)\n",
    "        \n",
    "    def forward(self, sentence):\n",
    "        #print(\"REACHONG FORWARD\")\n",
    "\n",
    "        #Input shape: [batch_size,max_length in that batch]\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        #print(\"DONE embeds:\", embeds.shape)  -- torch.Size([batch_Size, max_length_in_that_batch, embedding_dim])\n",
    "  \n",
    "        \n",
    "        #input shape: [len(sentence),1,embedding_dim] (L,N,Hin​) when batch_first=False)\n",
    "    \n",
    "        rnn_out, hidden_state_out = self.rnn(embeds) \n",
    "        #print(rnn_out.shape)  -- torch.Size([8, 45, 64])\n",
    "        #print(hidden_state_out.shape) --torch.Size([1, 8, 64])\n",
    "\n",
    "\n",
    "        #input shape: -- torch.Size([8, 45, 64])\n",
    "        tag_space = self.hidden2tag(rnn_out)\n",
    "        #print(\"DONE LINEAR LAYER: \", tag_space.shape) --torch.Size([8, 45, 13])\n",
    "        \n",
    "        tag_scores = F.log_softmax(tag_space, dim=2)\n",
    "        #print(\"DONE SOFTMAX:\", tag_scores.shape) --torch.Size([8, 45, 13])\n",
    "        \n",
    "        return tag_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(model,loss_function,optimizer,device,X,Y):\n",
    "    train_length = len(X)\n",
    "    epoch_train_loss = 0 \n",
    "   \n",
    "    model.train()\n",
    "    for i in tqdm(range(train_length)):\n",
    "        sentence_batch = X[i]\n",
    "        tags_batch = Y[i]\n",
    "\n",
    "        model.zero_grad()\n",
    "\n",
    "        #print(\"Input shape:\",  sentence_batch.shape) --torch.Size([batch_size, max_length_in_batch]) \n",
    "        tag_scores = model(sentence_batch)\n",
    "\n",
    "       \n",
    "        #print(\"model output shape: \", tag_scores.shape) -- torch.Size([8, 45, 13]) \n",
    "        #print(\"tags batch shape: \", tags_batch.shape) --torch.Size([8, 45])\n",
    "        new1 = tag_scores.transpose(1,2)\n",
    "        #print(\"new input shape:\" , new1.shape) --torch.Size([8, 13, 45])\n",
    "\n",
    "        #print(\"STARTING LOSS FUNCTION\")\n",
    "        loss = loss_function(new1 , tags_batch)\n",
    "        #print(\"DONE WITH LOSS FUNCTION\")\n",
    "        #print(loss)\n",
    "        epoch_train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    return model, epoch_train_loss/train_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_loop(model,loss_function,device,X,Y):\n",
    "    val_length = len(X)\n",
    "    epoch_val_loss = 0 \n",
    "\n",
    "    for i in tqdm(range(val_length)):\n",
    "        sentence_batch = X[i]\n",
    "        tags_batch = Y[i]\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        tag_scores = model(sentence_batch)\n",
    "\n",
    "        #CALL A FUNCTION WITH tag_scores and targets, GET PRECISION RECALL FScores        \n",
    "\n",
    "        new1 = tag_scores.transpose(1,2) \n",
    "        loss = loss_function(new1, tags_batch)\n",
    "        epoch_val_loss += loss.item()\n",
    "          \n",
    "     \n",
    "    \n",
    "    return epoch_val_loss/val_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "HIDDEN_DIM = 64\n",
    "batch_size = 4\n",
    "model =RNNTagger(HIDDEN_DIM, len(unique_tags_dict.keys())+1, batch_size)\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "model = model.to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_SIZE = 0.1\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_final, Y_final, test_size=TEST_SIZE, random_state=4)\n",
    "\n",
    "VALID_SIZE = 0.15\n",
    "X_train, X_validation, Y_train, Y_validation = train_test_split(X_train, Y_train, test_size=VALID_SIZE, random_state=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  0,  0,  0,  0,  0,  7, 12, 12, 10,  7,  4, 12,  9, 12,  9, 12,  8,\n",
       "         12, 11,  4, 12, 12, 10,  9,  7, 12, 10, 10,  1],\n",
       "        [ 0,  0,  0,  0,  0,  0,  1,  9,  7,  4, 12,  1,  6, 10,  2, 10, 12,  1,\n",
       "         11, 10, 12, 11,  7, 12,  1, 10,  7, 12, 12,  1],\n",
       "        [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  6, 10,  4, 11,\n",
       "         12, 10,  9,  6, 10,  2, 10,  6, 12,  9,  1,  1],\n",
       "        [ 1,  6, 10,  9,  6, 10,  6,  9,  7, 12,  9,  7, 12,  8, 10,  6,  7, 10,\n",
       "          3, 12, 11, 10,  1,  1, 10,  3,  4, 12, 12,  1],\n",
       "        [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  2,  6, 10,  1,  1,  9,\n",
       "          7, 12,  1,  3,  9,  7, 12, 10, 10,  1,  1,  1],\n",
       "        [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  7, 12, 12,  2, 10, 11, 12,  1],\n",
       "        [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0, 12, 12, 10,  7,  4, 12,  1],\n",
       "        [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 10,  9,  7, 12, 12,  1,\n",
       "          7, 12, 12, 10, 10,  9,  2,  1,  3,  7, 12,  1]], device='cuda:0')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    0,     0,     0,     0,     0,     0, 40706,  9997, 11719, 45610,\n",
       "         20330, 14802, 48138, 20403, 40956, 20403, 57926, 28629, 11749, 36823,\n",
       "         28446, 19444, 52177, 20809, 11132, 40706, 38278, 21834, 33386, 25158],\n",
       "        [    0,     0,     0,     0,     0,     0, 27557, 45427, 40706, 23607,\n",
       "         58336, 10989, 17660, 51960, 51269, 25802, 48913, 10120, 22995,  3927,\n",
       "         52455, 22995, 40706, 34050, 10989, 21440, 20330, 26815, 40038, 25158],\n",
       "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0, 27557, 20487, 51735, 32999, 22995,  3476, 59002,\n",
       "          4679, 30534, 51960, 10995, 49776,  1085, 25794, 46260, 25158, 10120],\n",
       "        [27557, 20487, 51735, 41936, 30534,  5178, 17660, 20403, 20330,  8965,\n",
       "         45427, 20330, 28018, 12198, 28309, 17660, 14836, 21834,  3812, 54173,\n",
       "         22995,   716, 10989, 10120, 29244, 49756, 40481, 52177, 39465, 25158],\n",
       "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0, 27557,  9909, 30534, 29244, 10989, 17485, 38264, 40706, 26548,\n",
       "         10989, 33215, 17399, 40706, 54173,  4167, 48111, 25158, 26726, 10120],\n",
       "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0, 40706, 13118,  7457, 43587, 45261, 22995, 10096, 25158],\n",
       "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0, 29254, 25179, 51213, 20330,  2327, 24206, 25158],\n",
       "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0, 24849, 11132, 20330, 28675, 59376, 10989, 40706,  1483,\n",
       "         20908, 51735, 40956, 12530, 20207, 10425, 10728, 20330, 22684, 25158]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6904\n",
      "6904\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train))\n",
    "print(len(Y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6904/6904 [00:26<00:00, 255.75it/s]\n",
      "100%|██████████| 1219/1219 [00:01<00:00, 625.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 0, training loss: 0.415888161280841, validation loss: 0.31407634555755626\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6904/6904 [00:21<00:00, 324.15it/s]\n",
      "100%|██████████| 1219/1219 [00:02<00:00, 515.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 1, training loss: 0.28070195092412786, validation loss: 0.26505546522089163\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6904/6904 [00:15<00:00, 439.34it/s]\n",
      "100%|██████████| 1219/1219 [00:00<00:00, 1308.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 2, training loss: 0.24588404443149528, validation loss: 0.2426609759806511\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6904/6904 [00:11<00:00, 598.50it/s]\n",
      "100%|██████████| 1219/1219 [00:00<00:00, 1440.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 3, training loss: 0.22632491197663673, validation loss: 0.2281126891361386\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6904/6904 [00:12<00:00, 567.92it/s]\n",
      "100%|██████████| 1219/1219 [00:00<00:00, 1572.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 4, training loss: 0.21305462160565364, validation loss: 0.21741873820765376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "    model , train_loss = train_loop(model,loss_function,optimizer,device,X_train,Y_train)\n",
    "    val_loss = validation_loop(model,loss_function,device,X_validation,Y_validation)\n",
    "    print(\"For epoch {}, training loss: {}, validation loss: {}\".format(epoch, train_loss, val_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[5, 6, 8]])\n",
      "tensor([1, 2, 3])\n",
      "tensor([[5, 6, 8],\n",
      "        [1, 2, 3]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 300])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#weight = torch.FloatTensor([[1, 2.3, 3], [4, 5.1, 6.3]])\n",
    "embedding = nn.Embedding.from_pretrained(embedding_weights)\n",
    "# Get embeddings for index 1\n",
    "list1 = [5,6,8],\n",
    "list1 = torch.tensor(list1)\n",
    "list2 = [1,2,3]\n",
    "list2 = torch.tensor(list2)\n",
    "list3 = torch.zeros((2,3), dtype = int)\n",
    "print(list1)\n",
    "print(list2)\n",
    "list3[0] = list1 \n",
    "list3[1] = list2\n",
    "print(list3)\n",
    "a = embedding(list3)\n",
    "a.shape -- batch_size,L_embedding_dim\n",
    "#for i in list3:\n",
    "#    print(i)\n",
    "#    a = embedding(i)\n",
    "#    print(a.shape)\n",
    "#print(list1)\n",
    "#input = torch.LongTensor([1])\n",
    "#a = embedding(list3)\n",
    "#a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
