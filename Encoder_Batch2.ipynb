{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import pickle\n",
    "from gensim.models import KeyedVectors\n",
    "from nltk.corpus import brown\n",
    "from nltk.corpus import treebank\n",
    "from nltk.corpus import conll2000\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "treebank_corpus = treebank.tagged_sents(tagset='universal')\n",
    "brown_corpus = brown.tagged_sents(tagset='universal')\n",
    "conll_corpus = conll2000.tagged_sents(tagset='universal')\n",
    "tagged_sentences = treebank_corpus + brown_corpus + conll_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Pierre', 'NOUN'),\n",
       " ('Vinken', 'NOUN'),\n",
       " (',', '.'),\n",
       " ('61', 'NUM'),\n",
       " ('years', 'NOUN'),\n",
       " ('old', 'ADJ'),\n",
       " (',', '.'),\n",
       " ('will', 'VERB'),\n",
       " ('join', 'VERB'),\n",
       " ('the', 'DET'),\n",
       " ('board', 'NOUN'),\n",
       " ('as', 'ADP'),\n",
       " ('a', 'DET'),\n",
       " ('nonexecutive', 'ADJ'),\n",
       " ('director', 'NOUN'),\n",
       " ('Nov.', 'NOUN'),\n",
       " ('29', 'NUM'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged_sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [] # store input sequence\n",
    "Y = [] # store output sequence\n",
    "\n",
    "for sentence in tagged_sentences:\n",
    "    X_sentence = []\n",
    "    Y_sentence = []\n",
    "    for entity in sentence:         \n",
    "        X_sentence.append(entity[0])  # entity[0] contains the word\n",
    "        Y_sentence.append(entity[1])  # entity[1] contains corresponding tag\n",
    "        \n",
    "    X.append(X_sentence)\n",
    "    Y.append(Y_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Pierre',\n",
       " 'Vinken',\n",
       " ',',\n",
       " '61',\n",
       " 'years',\n",
       " 'old',\n",
       " ',',\n",
       " 'will',\n",
       " 'join',\n",
       " 'the',\n",
       " 'board',\n",
       " 'as',\n",
       " 'a',\n",
       " 'nonexecutive',\n",
       " 'director',\n",
       " 'Nov.',\n",
       " '29',\n",
       " '.']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NOUN',\n",
       " 'NOUN',\n",
       " '.',\n",
       " 'NUM',\n",
       " 'NOUN',\n",
       " 'ADJ',\n",
       " '.',\n",
       " 'VERB',\n",
       " 'VERB',\n",
       " 'DET',\n",
       " 'NOUN',\n",
       " 'ADP',\n",
       " 'DET',\n",
       " 'ADJ',\n",
       " 'NOUN',\n",
       " 'NOUN',\n",
       " 'NUM',\n",
       " '.']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59448\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "num_words = len(set([word.lower() for sentence in X for word in sentence]))\n",
    "num_tags   = len(set([word.lower() for sentence in Y for word in sentence]))\n",
    "print(num_words)\n",
    "print(num_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'adp': 1, 'prt': 2, 'adv': 3, 'det': 4, 'pron': 5, 'conj': 6, 'x': 7, 'noun': 8, 'verb': 9, 'num': 10, '.': 11, 'adj': 12}\n"
     ]
    }
   ],
   "source": [
    "unique_tags = list(set([word.lower() for sentence in Y for word in sentence]))\n",
    "unique_tags_dict = {}\n",
    "index = 1\n",
    "for tag in unique_tags:\n",
    "    unique_tags_dict[tag] = index \n",
    "    index += 1\n",
    "print(unique_tags_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59448\n"
     ]
    }
   ],
   "source": [
    "unique_words = list(set([word.lower() for sentence in X for word in sentence]))\n",
    "unique_words_dict = {}\n",
    "index = 1\n",
    "for word in unique_words:\n",
    "    unique_words_dict[word] = index \n",
    "    index += 1\n",
    "print(len(unique_words_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59448\n"
     ]
    }
   ],
   "source": [
    "values = []\n",
    "for i in unique_words_dict.keys():\n",
    "    values.append(unique_words_dict[i])\n",
    "\n",
    "print(max(values))\n",
    "max_value_dict = max(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequence_sentence(seq, to_ix):\n",
    "    \"\"\"Input: takes in a list of words, and a dictionary containing the index of the words\n",
    "    Output: a tensor containing the indexes of the word\"\"\"\n",
    "    idxs = [to_ix[w.lower()] for w in seq]\n",
    "    random_index = random.randint(0,len(idxs)-1)\n",
    "    idxs[random_index] = max_value_dict + 1\n",
    "    return idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequence_tags(seq, to_ix):\n",
    "    \"\"\"Input: takes in a list of words, and a dictionary containing the index of the words\n",
    "    Output: a tensor containing the indexes of the word\"\"\"\n",
    "    idxs = [to_ix[w.lower()] for w in seq]\n",
    "    return idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "from keras_preprocessing.sequence import pad_sequences\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "def split(list_a, batch_size):\n",
    "\n",
    "  for i in range(0, len(list_a), batch_size):\n",
    "    yield list_a[i:i + batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "X_batches = list(split(X, batch_size))\n",
    "Y_batches = list(split(Y,batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_batches_padded = []\n",
    "Y_batches_padded = []\n",
    "max_length_list = []\n",
    "\n",
    "for b_s,b_t in zip(X_batches,Y_batches):\n",
    "    max_seq_length = 0\n",
    "    for sentence in b_s:\n",
    "        if len(sentence) > max_seq_length:\n",
    "            max_seq_length = len(sentence)\n",
    "    \n",
    "    sen_encoded = []\n",
    "    tag_encoded = []\n",
    "    for sentence,tags in zip(b_s,b_t):\n",
    "        sen_encoded.append(prepare_sequence_sentence(sentence, unique_words_dict))\n",
    "        tag_encoded.append(prepare_sequence_tags(tags, unique_tags_dict))\n",
    "    \n",
    "    X_batches_padded.append(pad_sequences(sen_encoded, maxlen=max_seq_length, padding=\"pre\", truncating=\"post\"))\n",
    "    Y_batches_padded.append(pad_sequences(tag_encoded, maxlen=max_seq_length, padding=\"pre\", truncating=\"post\"))\n",
    "    max_length_list.append(max_seq_length)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9026\n",
      "9026\n",
      "9026\n"
     ]
    }
   ],
   "source": [
    "print(len(Y_batches_padded))\n",
    "print(len(X_batches_padded))\n",
    "print(len(max_length_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(Y_batches_padded[0]))\n",
    "len(X_batches_padded[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  8\n",
      "   8 11 10  8 12 11  9  9  4  8  1  4 12  8  8 10 11]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  8  8  9  8  1  8  8 11  4  8  9  8 11]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  8  8 11 10  8 12  6 12  8  1\n",
      "   8  8  8  8 11  9  9  7  4 12  8  1  4 12 12  8 11]\n",
      " [ 4  8  1  8  3  9  7  7  2  9  8  8  8  9  9  4 12  8  1  8  8  1  4  8\n",
      "   1  8  9  7  2  5  3  1 10  8  1 11  8  9  7  7 11]\n",
      " [ 0  0  0  0  0  0  4  8  8 11  8 11  9  3 12  1  5  9  4  8 11  1  3 12\n",
      "   8  2  5  9  8  4  7  9  2  8 12 11  8  9  7  7 11]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  8  8 11  4  8  1 12 12  8  8\n",
      "   4  7  9  8  8 11  9  9  8  1  5  8  8  8  1 10 11]\n",
      " [ 0  0  0  0  1 12  8  9  9  7  3  1  4  8  1 11  4 12  8  9  1  8  2  8\n",
      "   8  8  1  8 11  4  8 12  7  2  9 12  8  2  4  8 11]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  4  8  8  9 11 11  4  9  4 12  8 11]]\n",
      "[[    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0 32437\n",
      "  14675 55964 42148 48419  3949 55964 59449 12641 38218 39032 53034 48521\n",
      "  58064  4580 50227  1928 45147]\n",
      " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0 35081 14675  6610 59088 58469 59353 32350 55964\n",
      "  59449 14694 31562 48546 45147]\n",
      " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0 40508 51290 55964 47716 48419  3949 20273 51420 59088 58469\n",
      "  57302 43680 11056 59449 55964 48077 52633 20111 48521 58064  4580 58469\n",
      "  35931 27702 57154 48334 45147]\n",
      " [48521  1811 58469 53500 47833 55635 26670 26670  3972  2317 45161 27735\n",
      "  57406 11627  4353 48521 43294  9759 58469 25743 23649 46541 48521 48546\n",
      "  58469  3490 39019 59449  3972 15403 27680  2937 52808 48419 20209 55964\n",
      "  21062 33084 54991 37311 45147]\n",
      " [    0     0     0     0     0     0 38218 53500 38396 55964 59449 55964\n",
      "   6610 26714 25100 47833 15403 22816 38218 30069 55964 17189 50477 57822\n",
      "  22257  3972 15403 31633  6905 38800 37311 45543 38549 32317  1835 55964\n",
      "  21062 22102 54991 59315 45147]\n",
      " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0 58187 36577 55964 38218  9137 58469 44167 28897 39101 37889\n",
      "  38800 59315 44613 45161  3606 55964 16765 19414 24993  7984 14929 56207\n",
      "  27735 57406  7984 38159 59449]\n",
      " [    0     0     0     0 59449 10538 11401  4957 33084 13443 27680  2937\n",
      "  48521 24992 20209 55964 38218 56919 41320 47354  7984  4425 37009 44167\n",
      "  32248 21745 58469 44549 55964 48521 46620 18597 26670  3972 20965 44167\n",
      "  53171  3972 38218 49617 45147]\n",
      " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0 48521 58187 59259 22102 55964 37652 35931\n",
      "   6610 20959  3949  2218 59449]]\n",
      "41\n"
     ]
    }
   ],
   "source": [
    "print(Y_batches_padded[0])\n",
    "print(X_batches_padded[0])\n",
    "print(max_length_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_final = []\n",
    "Y_final = []\n",
    "\n",
    "for index in range(len(X_batches_padded)):\n",
    "    X_batch_tensor = torch.zeros((batch_size,max_length_list[index]),dtype = int).to(device= device)\n",
    "    Y_batch_tensor = torch.zeros((batch_size,max_length_list[index]), dtype = int).to(device = device)\n",
    "\n",
    "    count = 0\n",
    "    for x, y in zip(X_batches_padded[index],Y_batches_padded[index]):\n",
    "        X_batch_tensor[count] = torch.tensor(x).to(device = device)\n",
    "        Y_batch_tensor[count] = torch.tensor(y).to(device =device)\n",
    "        count += 1\n",
    "    \n",
    "    X_final.append(X_batch_tensor)\n",
    "    Y_final.append(Y_batch_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9026\n",
      "9026\n",
      "torch.Size([8, 41])\n",
      "torch.Size([8, 41])\n",
      "torch.Size([41])\n",
      "torch.Size([41])\n",
      "torch.Size([8, 38])\n",
      "torch.Size([8, 38])\n",
      "tensor([[    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0, 32437, 14675, 55964, 42148, 48419,  3949, 55964,\n",
      "         59449, 12641, 38218, 39032, 53034, 48521, 58064,  4580, 50227,  1928,\n",
      "         45147],\n",
      "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0, 35081, 14675,\n",
      "          6610, 59088, 58469, 59353, 32350, 55964, 59449, 14694, 31562, 48546,\n",
      "         45147],\n",
      "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0, 40508, 51290, 55964, 47716, 48419,  3949,\n",
      "         20273, 51420, 59088, 58469, 57302, 43680, 11056, 59449, 55964, 48077,\n",
      "         52633, 20111, 48521, 58064,  4580, 58469, 35931, 27702, 57154, 48334,\n",
      "         45147],\n",
      "        [48521,  1811, 58469, 53500, 47833, 55635, 26670, 26670,  3972,  2317,\n",
      "         45161, 27735, 57406, 11627,  4353, 48521, 43294,  9759, 58469, 25743,\n",
      "         23649, 46541, 48521, 48546, 58469,  3490, 39019, 59449,  3972, 15403,\n",
      "         27680,  2937, 52808, 48419, 20209, 55964, 21062, 33084, 54991, 37311,\n",
      "         45147],\n",
      "        [    0,     0,     0,     0,     0,     0, 38218, 53500, 38396, 55964,\n",
      "         59449, 55964,  6610, 26714, 25100, 47833, 15403, 22816, 38218, 30069,\n",
      "         55964, 17189, 50477, 57822, 22257,  3972, 15403, 31633,  6905, 38800,\n",
      "         37311, 45543, 38549, 32317,  1835, 55964, 21062, 22102, 54991, 59315,\n",
      "         45147],\n",
      "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0, 58187, 36577, 55964, 38218,  9137, 58469,\n",
      "         44167, 28897, 39101, 37889, 38800, 59315, 44613, 45161,  3606, 55964,\n",
      "         16765, 19414, 24993,  7984, 14929, 56207, 27735, 57406,  7984, 38159,\n",
      "         59449],\n",
      "        [    0,     0,     0,     0, 59449, 10538, 11401,  4957, 33084, 13443,\n",
      "         27680,  2937, 48521, 24992, 20209, 55964, 38218, 56919, 41320, 47354,\n",
      "          7984,  4425, 37009, 44167, 32248, 21745, 58469, 44549, 55964, 48521,\n",
      "         46620, 18597, 26670,  3972, 20965, 44167, 53171,  3972, 38218, 49617,\n",
      "         45147],\n",
      "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0, 48521,\n",
      "         58187, 59259, 22102, 55964, 37652, 35931,  6610, 20959,  3949,  2218,\n",
      "         59449]], device='cuda:0')\n",
      "tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  8,  8, 11, 10,  8, 12, 11,  9,  9,  4,  8,  1,  4,\n",
      "         12,  8,  8, 10, 11],\n",
      "        [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  8,  8,  9,  8,  1,  8,  8, 11,\n",
      "          4,  8,  9,  8, 11],\n",
      "        [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  8,  8, 11, 10,\n",
      "          8, 12,  6, 12,  8,  1,  8,  8,  8,  8, 11,  9,  9,  7,  4, 12,  8,  1,\n",
      "          4, 12, 12,  8, 11],\n",
      "        [ 4,  8,  1,  8,  3,  9,  7,  7,  2,  9,  8,  8,  8,  9,  9,  4, 12,  8,\n",
      "          1,  8,  8,  1,  4,  8,  1,  8,  9,  7,  2,  5,  3,  1, 10,  8,  1, 11,\n",
      "          8,  9,  7,  7, 11],\n",
      "        [ 0,  0,  0,  0,  0,  0,  4,  8,  8, 11,  8, 11,  9,  3, 12,  1,  5,  9,\n",
      "          4,  8, 11,  1,  3, 12,  8,  2,  5,  9,  8,  4,  7,  9,  2,  8, 12, 11,\n",
      "          8,  9,  7,  7, 11],\n",
      "        [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  8,  8, 11,  4,\n",
      "          8,  1, 12, 12,  8,  8,  4,  7,  9,  8,  8, 11,  9,  9,  8,  1,  5,  8,\n",
      "          8,  8,  1, 10, 11],\n",
      "        [ 0,  0,  0,  0,  1, 12,  8,  9,  9,  7,  3,  1,  4,  8,  1, 11,  4, 12,\n",
      "          8,  9,  1,  8,  2,  8,  8,  8,  1,  8, 11,  4,  8, 12,  7,  2,  9, 12,\n",
      "          8,  2,  4,  8, 11],\n",
      "        [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  4,  8,  8,  9, 11, 11,  4,\n",
      "          9,  4, 12,  8, 11]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(len(X_final))\n",
    "print(len(Y_final))\n",
    "print(X_final[0].shape)\n",
    "print(Y_final[0].shape)\n",
    "print(X_final[0][0].shape)\n",
    "print(Y_final[0][0].shape)\n",
    "print(X_final[1].shape)\n",
    "print(Y_final[1].shape)\n",
    "print(X_final[0])\n",
    "print(Y_final[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_batches = []\n",
    "# Y_batches = []\n",
    "# batch_size = 8\n",
    "\n",
    "\n",
    "# for sentence in X:\n",
    "#     X_encoded.append(prepare_sequence(sentence, unique_words_dict))\n",
    "# for tags in Y:\n",
    "#     Y_encoded.append(prepare_sequence(tags, unique_tags_dict))\n",
    "\n",
    "# MAX_SEQ_LENGTH = 100  # sequences greater than 100 in length will be truncated\n",
    "\n",
    "# X_padded = pad_sequences(X_encoded, maxlen=MAX_SEQ_LENGTH, padding=\"pre\", truncating=\"post\")\n",
    "# Y_padded = pad_sequences(Y_encoded, maxlen=MAX_SEQ_LENGTH, padding=\"pre\", truncating=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(X_padded))\n",
    "# X_padded[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(Y_encoded))\n",
    "# print(Y_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([59450, 300])\n"
     ]
    }
   ],
   "source": [
    "EMBEDDING_SIZE  = 300  # each word in word2vec model is represented using a 300 dimensional vector\n",
    "VOCABULARY_SIZE = num_words + 2\n",
    "\n",
    "with open('./embedding_weights.pickle', 'rb') as file:\n",
    "    embedding_weights = pickle.load(file)\n",
    "\n",
    "print(embedding_weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_emb_layer(weights_matrix, non_trainable=False):\n",
    "\n",
    "    num_embeddings, embedding_dim = weights_matrix.size()\n",
    "    emb_layer = nn.Embedding(num_embeddings, embedding_dim)\n",
    "    emb_layer.load_state_dict({'weight': weights_matrix})\n",
    "    \n",
    "    if non_trainable:\n",
    "        emb_layer.weight.requires_grad = False\n",
    "\n",
    "    return emb_layer, num_embeddings, embedding_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNTagger(nn.Module):\n",
    "    def __init__(self, hidden_dim, target_size, batch_size):\n",
    "        super(RNNTagger, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.word_embeddings, vocab_size, embedding_dim = create_emb_layer(embedding_weights, True)\n",
    "        #self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim, batch_first = True)\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, target_size)\n",
    "        \n",
    "    def forward(self, sentence):\n",
    "        #print(\"REACHONG FORWARD\")\n",
    "\n",
    "        #Input shape: [batch_size,max_length in that batch]\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        #print(\"DONE embeds:\", embeds.shape)  -- torch.Size([batch_Size, max_length_in_that_batch, embedding_dim])\n",
    "  \n",
    "        \n",
    "        #input shape: [len(sentence),1,embedding_dim] (L,N,Hin​) when batch_first=False)\n",
    "    \n",
    "        rnn_out, hidden_state_out = self.rnn(embeds) \n",
    "        #print(rnn_out.shape)  -- torch.Size([8, 45, 64])\n",
    "        #print(hidden_state_out.shape) --torch.Size([1, 8, 64])\n",
    "\n",
    "\n",
    "        #input shape: -- torch.Size([8, 45, 64])\n",
    "        tag_space = self.hidden2tag(rnn_out)\n",
    "        #print(\"DONE LINEAR LAYER: \", tag_space.shape) --torch.Size([8, 45, 13])\n",
    "        \n",
    "        tag_scores = F.log_softmax(tag_space, dim=2)\n",
    "        #print(\"DONE SOFTMAX:\", tag_scores.shape) --torch.Size([8, 45, 13])\n",
    "        \n",
    "        return tag_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(model,loss_function,optimizer,device,X,Y):\n",
    "    train_length = len(X)\n",
    "    epoch_train_loss = 0 \n",
    "   \n",
    "    model.train()\n",
    "    for i in tqdm(range(train_length)):\n",
    "        sentence_batch = X[i]\n",
    "        tags_batch = Y[i]\n",
    "\n",
    "        model.zero_grad()\n",
    "\n",
    "        #print(\"Input shape:\",  sentence_batch.shape) --torch.Size([batch_size, max_length_in_batch]) \n",
    "        tag_scores = model(sentence_batch)\n",
    "\n",
    "       \n",
    "        #print(\"model output shape: \", tag_scores.shape) -- torch.Size([8, 45, 13]) \n",
    "        #print(\"tags batch shape: \", tags_batch.shape) --torch.Size([8, 45])\n",
    "        new1 = tag_scores.transpose(1,2)\n",
    "        #print(\"new input shape:\" , new1.shape) --torch.Size([8, 13, 45])\n",
    "\n",
    "        #print(\"STARTING LOSS FUNCTION\")\n",
    "        loss = loss_function(new1 , tags_batch)\n",
    "        #print(\"DONE WITH LOSS FUNCTION\")\n",
    "        #print(loss)\n",
    "        epoch_train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    return model, epoch_train_loss/train_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_loop(model,loss_function,device,X,Y):\n",
    "    val_length = len(X)\n",
    "    epoch_val_loss = 0 \n",
    "\n",
    "    for i in tqdm(range(val_length)):\n",
    "        sentence_batch = X[i]\n",
    "        tags_batch = Y[i]\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        tag_scores = model(sentence_batch)\n",
    "\n",
    "        #CALL A FUNCTION WITH tag_scores and targets, GET PRECISION RECALL FScores        \n",
    "\n",
    "        new1 = tag_scores.transpose(1,2) \n",
    "        loss = loss_function(new1, tags_batch)\n",
    "        epoch_val_loss += loss.item()\n",
    "          \n",
    "     \n",
    "    \n",
    "    return epoch_val_loss/val_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "HIDDEN_DIM = 64\n",
    "batch_size = 4\n",
    "model =RNNTagger(HIDDEN_DIM, len(unique_tags_dict.keys())+1, batch_size)\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "model = model.to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_SIZE = 0.1\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_final, Y_final, test_size=TEST_SIZE, random_state=4)\n",
    "\n",
    "VALID_SIZE = 0.15\n",
    "X_train, X_validation, Y_train, Y_validation = train_test_split(X_train, Y_train, test_size=VALID_SIZE, random_state=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  0,  0,  0,  0,  0,  7, 12, 12, 10,  7,  4, 12,  9, 12,  9, 12,  8,\n",
       "         12, 11,  4, 12, 12, 10,  9,  7, 12, 10, 10,  1],\n",
       "        [ 0,  0,  0,  0,  0,  0,  1,  9,  7,  4, 12,  1,  6, 10,  2, 10, 12,  1,\n",
       "         11, 10, 12, 11,  7, 12,  1, 10,  7, 12, 12,  1],\n",
       "        [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  6, 10,  4, 11,\n",
       "         12, 10,  9,  6, 10,  2, 10,  6, 12,  9,  1,  1],\n",
       "        [ 1,  6, 10,  9,  6, 10,  6,  9,  7, 12,  9,  7, 12,  8, 10,  6,  7, 10,\n",
       "          3, 12, 11, 10,  1,  1, 10,  3,  4, 12, 12,  1],\n",
       "        [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  2,  6, 10,  1,  1,  9,\n",
       "          7, 12,  1,  3,  9,  7, 12, 10, 10,  1,  1,  1],\n",
       "        [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  7, 12, 12,  2, 10, 11, 12,  1],\n",
       "        [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0, 12, 12, 10,  7,  4, 12,  1],\n",
       "        [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 10,  9,  7, 12, 12,  1,\n",
       "          7, 12, 12, 10, 10,  9,  2,  1,  3,  7, 12,  1]], device='cuda:0')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    0,     0,     0,     0,     0,     0, 40706,  9997, 11719, 45610,\n",
       "         20330, 14802, 48138, 20403, 40956, 20403, 57926, 28629, 11749, 36823,\n",
       "         28446, 19444, 52177, 20809, 11132, 40706, 38278, 21834, 33386, 25158],\n",
       "        [    0,     0,     0,     0,     0,     0, 27557, 45427, 40706, 23607,\n",
       "         58336, 10989, 17660, 51960, 51269, 25802, 48913, 10120, 22995,  3927,\n",
       "         52455, 22995, 40706, 34050, 10989, 21440, 20330, 26815, 40038, 25158],\n",
       "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0, 27557, 20487, 51735, 32999, 22995,  3476, 59002,\n",
       "          4679, 30534, 51960, 10995, 49776,  1085, 25794, 46260, 25158, 10120],\n",
       "        [27557, 20487, 51735, 41936, 30534,  5178, 17660, 20403, 20330,  8965,\n",
       "         45427, 20330, 28018, 12198, 28309, 17660, 14836, 21834,  3812, 54173,\n",
       "         22995,   716, 10989, 10120, 29244, 49756, 40481, 52177, 39465, 25158],\n",
       "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0, 27557,  9909, 30534, 29244, 10989, 17485, 38264, 40706, 26548,\n",
       "         10989, 33215, 17399, 40706, 54173,  4167, 48111, 25158, 26726, 10120],\n",
       "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0, 40706, 13118,  7457, 43587, 45261, 22995, 10096, 25158],\n",
       "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0, 29254, 25179, 51213, 20330,  2327, 24206, 25158],\n",
       "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0, 24849, 11132, 20330, 28675, 59376, 10989, 40706,  1483,\n",
       "         20908, 51735, 40956, 12530, 20207, 10425, 10728, 20330, 22684, 25158]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6904\n",
      "6904\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train))\n",
    "print(len(Y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6904/6904 [00:43<00:00, 159.87it/s]\n",
      "100%|██████████| 1219/1219 [00:02<00:00, 516.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 0, training loss: 0.47539491772215686, validation loss: 0.373599452425751\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6904/6904 [00:33<00:00, 204.47it/s]\n",
      "100%|██████████| 1219/1219 [00:02<00:00, 468.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 1, training loss: 0.34039641293316114, validation loss: 0.3213791811718033\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6904/6904 [00:32<00:00, 211.20it/s]\n",
      "100%|██████████| 1219/1219 [00:02<00:00, 590.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 2, training loss: 0.30288447106962857, validation loss: 0.297107857860582\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6904/6904 [00:34<00:00, 200.76it/s]\n",
      "100%|██████████| 1219/1219 [00:02<00:00, 425.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 3, training loss: 0.28159749749853674, validation loss: 0.28230304639538945\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6904/6904 [00:23<00:00, 289.84it/s]\n",
      "100%|██████████| 1219/1219 [00:01<00:00, 763.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 4, training loss: 0.2670571006022972, validation loss: 0.2720070388397298\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "    model , train_loss = train_loop(model,loss_function,optimizer,device,X_train,Y_train)\n",
    "    val_loss = validation_loop(model,loss_function,device,X_validation,Y_validation)\n",
    "    print(\"For epoch {}, training loss: {}, validation loss: {}\".format(epoch, train_loss, val_loss))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DEMO CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\prana\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_TESTsequence_sentence(seq, to_ix):\n",
    "    values = []\n",
    "    for i in to_ix.keys():\n",
    "        values.append(to_ix[i])\n",
    "    \n",
    "    max_value_dict = max(values)\n",
    "    \n",
    "    idxs = []\n",
    "\n",
    "    for w in seq:\n",
    "        if w.lower() in to_ix.keys():\n",
    "            idxs.append(to_ix[w.lower()])\n",
    "        else:\n",
    "            idxs.append(max_value_dict+1)\n",
    "    return idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 'adp',\n",
       " 2: 'prt',\n",
       " 3: 'adv',\n",
       " 4: 'det',\n",
       " 5: 'pron',\n",
       " 6: 'conj',\n",
       " 7: 'x',\n",
       " 8: 'noun',\n",
       " 9: 'verb',\n",
       " 10: 'num',\n",
       " 11: '.',\n",
       " 12: 'adj'}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_to_tags = {}\n",
    "for tag in unique_tags_dict:\n",
    "    index_to_tags[unique_tags_dict[tag]] = tag \n",
    "index_to_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bottle', 'is', '5', 'years', 'old', ',', 'non', 'executive', 'director', 'of', 'CSK']\n",
      "[56251, 6610, 58950, 48419, 3949, 55964, 14933, 22254, 4580, 58469, 59449]\n",
      "bottle noun\n",
      "is verb\n",
      "5 num\n",
      "years noun\n",
      "old adj\n",
      ", .\n",
      "non noun\n",
      "executive noun\n",
      "director noun\n",
      "of adp\n",
      "CSK det\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "text = \"bottle is 5 years old, non executive director of CSK\"\n",
    "tokenized_seq = word_tokenize(text)\n",
    "print(tokenized_seq)\n",
    "\n",
    "tokens= prepare_TESTsequence_sentence(tokenized_seq,unique_words_dict)\n",
    "print(tokens) \n",
    "\n",
    "tokens = torch.tensor(tokens).to(device = device)\n",
    "tokens = tokens.unsqueeze(0)\n",
    "output = model(tokens)\n",
    "output = output.squeeze(0)\n",
    "tag_index = torch.argmax(output,dim=1)\n",
    "#print(tag_index.shape)\n",
    "#print(output.shape)\n",
    "for predicted_tag,word in zip(tag_index,tokenized_seq):\n",
    "    predicted_tag = predicted_tag.item()\n",
    "    predicted_tag = index_to_tags[predicted_tag]\n",
    "    print(word, predicted_tag)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[5, 6, 8]])\n",
      "tensor([1, 2, 3])\n",
      "tensor([[5, 6, 8],\n",
      "        [1, 2, 3]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 300])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#weight = torch.FloatTensor([[1, 2.3, 3], [4, 5.1, 6.3]])\n",
    "embedding = nn.Embedding.from_pretrained(embedding_weights)\n",
    "# Get embeddings for index 1\n",
    "list1 = [5,6,8],\n",
    "list1 = torch.tensor(list1)\n",
    "list2 = [1,2,3]\n",
    "list2 = torch.tensor(list2)\n",
    "list3 = torch.zeros((2,3), dtype = int)\n",
    "print(list1)\n",
    "print(list2)\n",
    "list3[0] = list1 \n",
    "list3[1] = list2\n",
    "print(list3)\n",
    "a = embedding(list3)\n",
    "a.shape -- batch_size,L_embedding_dim\n",
    "#for i in list3:\n",
    "#    print(i)\n",
    "#    a = embedding(i)\n",
    "#    print(a.shape)\n",
    "#print(list1)\n",
    "#input = torch.LongTensor([1])\n",
    "#a = embedding(list3)\n",
    "#a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
