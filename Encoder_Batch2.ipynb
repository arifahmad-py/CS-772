{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\sem8\\CS772\\A2_Part1\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import pickle\n",
    "from gensim.models import KeyedVectors\n",
    "from nltk.corpus import brown\n",
    "from nltk.corpus import treebank\n",
    "from nltk.corpus import conll2000\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "treebank_corpus = treebank.tagged_sents(tagset='universal')\n",
    "brown_corpus = brown.tagged_sents(tagset='universal')\n",
    "conll_corpus = conll2000.tagged_sents(tagset='universal')\n",
    "tagged_sentences = treebank_corpus + brown_corpus + conll_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Pierre', 'NOUN'),\n",
       " ('Vinken', 'NOUN'),\n",
       " (',', '.'),\n",
       " ('61', 'NUM'),\n",
       " ('years', 'NOUN'),\n",
       " ('old', 'ADJ'),\n",
       " (',', '.'),\n",
       " ('will', 'VERB'),\n",
       " ('join', 'VERB'),\n",
       " ('the', 'DET'),\n",
       " ('board', 'NOUN'),\n",
       " ('as', 'ADP'),\n",
       " ('a', 'DET'),\n",
       " ('nonexecutive', 'ADJ'),\n",
       " ('director', 'NOUN'),\n",
       " ('Nov.', 'NOUN'),\n",
       " ('29', 'NUM'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged_sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [] # store input sequence\n",
    "Y = [] # store output sequence\n",
    "\n",
    "for sentence in tagged_sentences:\n",
    "    X_sentence = []\n",
    "    Y_sentence = []\n",
    "    for entity in sentence:         \n",
    "        X_sentence.append(entity[0])  # entity[0] contains the word\n",
    "        Y_sentence.append(entity[1])  # entity[1] contains corresponding tag\n",
    "        \n",
    "    X.append(X_sentence)\n",
    "    Y.append(Y_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Pierre',\n",
       " 'Vinken',\n",
       " ',',\n",
       " '61',\n",
       " 'years',\n",
       " 'old',\n",
       " ',',\n",
       " 'will',\n",
       " 'join',\n",
       " 'the',\n",
       " 'board',\n",
       " 'as',\n",
       " 'a',\n",
       " 'nonexecutive',\n",
       " 'director',\n",
       " 'Nov.',\n",
       " '29',\n",
       " '.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NOUN',\n",
       " 'NOUN',\n",
       " '.',\n",
       " 'NUM',\n",
       " 'NOUN',\n",
       " 'ADJ',\n",
       " '.',\n",
       " 'VERB',\n",
       " 'VERB',\n",
       " 'DET',\n",
       " 'NOUN',\n",
       " 'ADP',\n",
       " 'DET',\n",
       " 'ADJ',\n",
       " 'NOUN',\n",
       " 'NOUN',\n",
       " 'NUM',\n",
       " '.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59448\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "num_words = len(set([word.lower() for sentence in X for word in sentence]))\n",
    "num_tags   = len(set([word.lower() for sentence in Y for word in sentence]))\n",
    "print(num_words)\n",
    "print(num_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'adj': 1, 'conj': 2, 'adv': 3, 'pron': 4, '.': 5, 'num': 6, 'prt': 7, 'adp': 8, 'x': 9, 'noun': 10, 'verb': 11, 'det': 12}\n"
     ]
    }
   ],
   "source": [
    "unique_tags = list(set([word.lower() for sentence in Y for word in sentence]))\n",
    "unique_tags_dict = {}\n",
    "index = 1\n",
    "for tag in unique_tags:\n",
    "    unique_tags_dict[tag] = index \n",
    "    index += 1\n",
    "print(unique_tags_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59448\n"
     ]
    }
   ],
   "source": [
    "unique_words = list(set([word.lower() for sentence in X for word in sentence]))\n",
    "unique_words_dict = {}\n",
    "index = 1\n",
    "for word in unique_words:\n",
    "    unique_words_dict[word] = index \n",
    "    index += 1\n",
    "print(len(unique_words_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59448\n"
     ]
    }
   ],
   "source": [
    "values = []\n",
    "for i in unique_words_dict.keys():\n",
    "    values.append(unique_words_dict[i])\n",
    "\n",
    "print(max(values))\n",
    "max_value_dict = max(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequence_sentence(seq, to_ix):\n",
    "    \"\"\"Input: takes in a list of words, and a dictionary containing the index of the words\n",
    "    Output: a tensor containing the indexes of the word\"\"\"\n",
    "    idxs = [to_ix[w.lower()] for w in seq]\n",
    "    random_index = random.randint(0,len(idxs)-1)\n",
    "    idxs[random_index] = max_value_dict + 1\n",
    "    return idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequence_tags(seq, to_ix):\n",
    "    \"\"\"Input: takes in a list of words, and a dictionary containing the index of the words\n",
    "    Output: a tensor containing the indexes of the word\"\"\"\n",
    "    idxs = [to_ix[w.lower()] for w in seq]\n",
    "    return idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "from keras_preprocessing.sequence import pad_sequences\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "def split(list_a, batch_size):\n",
    "\n",
    "  for i in range(0, len(list_a), batch_size):\n",
    "    yield list_a[i:i + batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "X_batches = list(split(X, batch_size))\n",
    "Y_batches = list(split(Y,batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_batches_padded = []\n",
    "Y_batches_padded = []\n",
    "max_length_list = []\n",
    "\n",
    "for b_s,b_t in zip(X_batches,Y_batches):\n",
    "    max_seq_length = 0\n",
    "    for sentence in b_s:\n",
    "        if len(sentence) > max_seq_length:\n",
    "            max_seq_length = len(sentence)\n",
    "    \n",
    "    sen_encoded = []\n",
    "    tag_encoded = []\n",
    "    for sentence,tags in zip(b_s,b_t):\n",
    "        sen_encoded.append(prepare_sequence_sentence(sentence, unique_words_dict))\n",
    "        tag_encoded.append(prepare_sequence_tags(tags, unique_tags_dict))\n",
    "    \n",
    "    X_batches_padded.append(pad_sequences(sen_encoded, maxlen=max_seq_length, padding=\"pre\", truncating=\"post\"))\n",
    "    Y_batches_padded.append(pad_sequences(tag_encoded, maxlen=max_seq_length, padding=\"pre\", truncating=\"post\"))\n",
    "    max_length_list.append(max_seq_length)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9026\n",
      "9026\n",
      "9026\n"
     ]
    }
   ],
   "source": [
    "print(len(Y_batches_padded))\n",
    "print(len(X_batches_padded))\n",
    "print(len(max_length_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(Y_batches_padded[0]))\n",
    "len(X_batches_padded[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 10\n",
      "  10  5  6 10  1  5 11 11 12 10  8 12  1 10 10  6  5]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0 10 10 11 10  8 10 10  5 12 10 11 10  5]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0 10 10  5  6 10  1  2  1 10  8\n",
      "  10 10 10 10  5 11 11  9 12  1 10  8 12  1  1 10  5]\n",
      " [12 10  8 10  3 11  9  9  7 11 10 10 10 11 11 12  1 10  8 10 10  8 12 10\n",
      "   8 10 11  9  7  4  3  8  6 10  8  5 10 11  9  9  5]\n",
      " [ 0  0  0  0  0  0 12 10 10  5 10  5 11  3  1  8  4 11 12 10  5  8  3  1\n",
      "  10  7  4 11 10 12  9 11  7 10  1  5 10 11  9  9  5]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0 10 10  5 12 10  8  1  1 10 10\n",
      "  12  9 11 10 10  5 11 11 10  8  4 10 10 10  8  6  5]\n",
      " [ 0  0  0  0  8  1 10 11 11  9  3  8 12 10  8  5 12  1 10 11  8 10  7 10\n",
      "  10 10  8 10  5 12 10  1  9  7 11  1 10  7 12 10  5]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0 12 10 10 11  5  5 12 11 12  1 10  5]]\n",
      "[[    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0 42399\n",
      "  46560  9529 30096 18958 11126  9529 34297 58179 37456   100 29634  7786\n",
      "  59449 46726 42324 26382 36207]\n",
      " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0 26279 46560 44323 49559 21652 39255   820  9529\n",
      "  59449 27632  7584 51603 36207]\n",
      " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0 31842 12907  9529  6106 18958 11126 50302 58189 49559 21652\n",
      "    352 29618 58107 17483  9529  4491 24190 53316  7786 35320 46726 21652\n",
      "  46407   441 59449 54550 36207]\n",
      " [ 7786 34968 21652 31583 20151 40289 26567 26567 42266 11987 11285 22682\n",
      "  30864 20326 55439  7786 37737 55787 21652 59449 11418 14121  7786 51603\n",
      "  21652 33091 25044 26567 42266 32789 55877 29590 14152 18958 12654  9529\n",
      "   8428 25766 11620 40745 36207]\n",
      " [    0     0     0     0     0     0 37456 31583  2066  9529 27002  9529\n",
      "  59449  2231 46964 20151 32789 58059 37456 27848  9529  1176   239 22138\n",
      "  49625 42266 32789 57983 53597 13338 40745 38475 20403 47967  2285  9529\n",
      "   8428  2602 11620 25734 36207]\n",
      " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0  6183  3987 59449 37456 18120 21652 17343 16455 48066 54579\n",
      "  13338 25734 54987 11285 35356  9529  5293 38238 27002 38802 57045 37839\n",
      "  22682 30864 38802 15675 36207]\n",
      " [    0     0     0     0  9539 22308  6053 49699 25766 57342 55877 29590\n",
      "   7786 51451 12654  9529 37456 36711  4572 45830 38802 44520  4565 17343\n",
      "  56188 52134 21652 42095  9529  7786 15535 11690 26567 42266 59449 17343\n",
      "  36280 42266 37456 42458 36207]\n",
      " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0  7786  6183 35584  2602  9529 50973 46407\n",
      "  44323 57773 59449 51946 36207]]\n",
      "41\n"
     ]
    }
   ],
   "source": [
    "print(Y_batches_padded[0])\n",
    "print(X_batches_padded[0])\n",
    "print(max_length_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_final = []\n",
    "Y_final = []\n",
    "\n",
    "for index in range(len(X_batches_padded)):\n",
    "    X_batch_tensor = torch.zeros((batch_size,max_length_list[index]),dtype = int).to(device= device)\n",
    "    Y_batch_tensor = torch.zeros((batch_size,max_length_list[index]), dtype = int).to(device = device)\n",
    "\n",
    "    count = 0\n",
    "    for x, y in zip(X_batches_padded[index],Y_batches_padded[index]):\n",
    "        X_batch_tensor[count] = torch.tensor(x).to(device = device)\n",
    "        Y_batch_tensor[count] = torch.tensor(y).to(device =device)\n",
    "        count += 1\n",
    "    \n",
    "    X_final.append(X_batch_tensor)\n",
    "    Y_final.append(Y_batch_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9026\n",
      "9026\n",
      "torch.Size([8, 41])\n",
      "torch.Size([8, 41])\n",
      "torch.Size([41])\n",
      "torch.Size([41])\n",
      "torch.Size([8, 38])\n",
      "torch.Size([8, 38])\n",
      "tensor([[    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0, 42399, 46560,  9529, 30096, 18958, 11126,  9529,\n",
      "         34297, 58179, 37456,   100, 29634,  7786, 59449, 46726, 42324, 26382,\n",
      "         36207],\n",
      "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0, 26279, 46560,\n",
      "         44323, 49559, 21652, 39255,   820,  9529, 59449, 27632,  7584, 51603,\n",
      "         36207],\n",
      "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0, 31842, 12907,  9529,  6106, 18958, 11126,\n",
      "         50302, 58189, 49559, 21652,   352, 29618, 58107, 17483,  9529,  4491,\n",
      "         24190, 53316,  7786, 35320, 46726, 21652, 46407,   441, 59449, 54550,\n",
      "         36207],\n",
      "        [ 7786, 34968, 21652, 31583, 20151, 40289, 26567, 26567, 42266, 11987,\n",
      "         11285, 22682, 30864, 20326, 55439,  7786, 37737, 55787, 21652, 59449,\n",
      "         11418, 14121,  7786, 51603, 21652, 33091, 25044, 26567, 42266, 32789,\n",
      "         55877, 29590, 14152, 18958, 12654,  9529,  8428, 25766, 11620, 40745,\n",
      "         36207],\n",
      "        [    0,     0,     0,     0,     0,     0, 37456, 31583,  2066,  9529,\n",
      "         27002,  9529, 59449,  2231, 46964, 20151, 32789, 58059, 37456, 27848,\n",
      "          9529,  1176,   239, 22138, 49625, 42266, 32789, 57983, 53597, 13338,\n",
      "         40745, 38475, 20403, 47967,  2285,  9529,  8428,  2602, 11620, 25734,\n",
      "         36207],\n",
      "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,  6183,  3987, 59449, 37456, 18120, 21652,\n",
      "         17343, 16455, 48066, 54579, 13338, 25734, 54987, 11285, 35356,  9529,\n",
      "          5293, 38238, 27002, 38802, 57045, 37839, 22682, 30864, 38802, 15675,\n",
      "         36207],\n",
      "        [    0,     0,     0,     0,  9539, 22308,  6053, 49699, 25766, 57342,\n",
      "         55877, 29590,  7786, 51451, 12654,  9529, 37456, 36711,  4572, 45830,\n",
      "         38802, 44520,  4565, 17343, 56188, 52134, 21652, 42095,  9529,  7786,\n",
      "         15535, 11690, 26567, 42266, 59449, 17343, 36280, 42266, 37456, 42458,\n",
      "         36207],\n",
      "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,  7786,\n",
      "          6183, 35584,  2602,  9529, 50973, 46407, 44323, 57773, 59449, 51946,\n",
      "         36207]], device='cuda:0')\n",
      "tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0, 10, 10,  5,  6, 10,  1,  5, 11, 11, 12, 10,  8, 12,\n",
      "          1, 10, 10,  6,  5],\n",
      "        [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 10, 10, 11, 10,  8, 10, 10,  5,\n",
      "         12, 10, 11, 10,  5],\n",
      "        [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 10, 10,  5,  6,\n",
      "         10,  1,  2,  1, 10,  8, 10, 10, 10, 10,  5, 11, 11,  9, 12,  1, 10,  8,\n",
      "         12,  1,  1, 10,  5],\n",
      "        [12, 10,  8, 10,  3, 11,  9,  9,  7, 11, 10, 10, 10, 11, 11, 12,  1, 10,\n",
      "          8, 10, 10,  8, 12, 10,  8, 10, 11,  9,  7,  4,  3,  8,  6, 10,  8,  5,\n",
      "         10, 11,  9,  9,  5],\n",
      "        [ 0,  0,  0,  0,  0,  0, 12, 10, 10,  5, 10,  5, 11,  3,  1,  8,  4, 11,\n",
      "         12, 10,  5,  8,  3,  1, 10,  7,  4, 11, 10, 12,  9, 11,  7, 10,  1,  5,\n",
      "         10, 11,  9,  9,  5],\n",
      "        [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 10, 10,  5, 12,\n",
      "         10,  8,  1,  1, 10, 10, 12,  9, 11, 10, 10,  5, 11, 11, 10,  8,  4, 10,\n",
      "         10, 10,  8,  6,  5],\n",
      "        [ 0,  0,  0,  0,  8,  1, 10, 11, 11,  9,  3,  8, 12, 10,  8,  5, 12,  1,\n",
      "         10, 11,  8, 10,  7, 10, 10, 10,  8, 10,  5, 12, 10,  1,  9,  7, 11,  1,\n",
      "         10,  7, 12, 10,  5],\n",
      "        [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 12, 10, 10, 11,  5,  5, 12,\n",
      "         11, 12,  1, 10,  5]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(len(X_final))\n",
    "print(len(Y_final))\n",
    "print(X_final[0].shape)\n",
    "print(Y_final[0].shape)\n",
    "print(X_final[0][0].shape)\n",
    "print(Y_final[0][0].shape)\n",
    "print(X_final[1].shape)\n",
    "print(Y_final[1].shape)\n",
    "print(X_final[0])\n",
    "print(Y_final[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_batches = []\n",
    "# Y_batches = []\n",
    "# batch_size = 8\n",
    "\n",
    "\n",
    "# for sentence in X:\n",
    "#     X_encoded.append(prepare_sequence(sentence, unique_words_dict))\n",
    "# for tags in Y:\n",
    "#     Y_encoded.append(prepare_sequence(tags, unique_tags_dict))\n",
    "\n",
    "# MAX_SEQ_LENGTH = 100  # sequences greater than 100 in length will be truncated\n",
    "\n",
    "# X_padded = pad_sequences(X_encoded, maxlen=MAX_SEQ_LENGTH, padding=\"pre\", truncating=\"post\")\n",
    "# Y_padded = pad_sequences(Y_encoded, maxlen=MAX_SEQ_LENGTH, padding=\"pre\", truncating=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(X_padded))\n",
    "# X_padded[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(Y_encoded))\n",
    "# print(Y_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([59450, 300])\n"
     ]
    }
   ],
   "source": [
    "EMBEDDING_SIZE  = 300  # each word in word2vec model is represented using a 300 dimensional vector\n",
    "VOCABULARY_SIZE = num_words + 2\n",
    "\n",
    "with open('./embedding_weights.pickle', 'rb') as file:\n",
    "    embedding_weights = pickle.load(file)\n",
    "\n",
    "print(embedding_weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_emb_layer(weights_matrix, non_trainable=False):\n",
    "\n",
    "    num_embeddings, embedding_dim = weights_matrix.size()\n",
    "    emb_layer = nn.Embedding(num_embeddings, embedding_dim)\n",
    "    emb_layer.load_state_dict({'weight': weights_matrix})\n",
    "    \n",
    "    if non_trainable:\n",
    "        emb_layer.weight.requires_grad = False\n",
    "\n",
    "    return emb_layer, num_embeddings, embedding_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNTagger(nn.Module):\n",
    "    def __init__(self, hidden_dim, target_size, batch_size):\n",
    "        super(RNNTagger, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.word_embeddings, vocab_size, embedding_dim = create_emb_layer(embedding_weights, True)\n",
    "        #self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim, batch_first = True)\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, target_size)\n",
    "        \n",
    "    def forward(self, sentence):\n",
    "        #print(\"REACHONG FORWARD\")\n",
    "\n",
    "        #Input shape: [batch_size,max_length in that batch]\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        #print(\"DONE embeds:\", embeds.shape)  -- torch.Size([batch_Size, max_length_in_that_batch, embedding_dim])\n",
    "  \n",
    "        \n",
    "        #input shape: [len(sentence),1,embedding_dim] (L,N,Hin​) when batch_first=False)\n",
    "    \n",
    "        rnn_out, hidden_state_out = self.rnn(embeds) \n",
    "        #print(rnn_out.shape)  -- torch.Size([8, 45, 64])\n",
    "        #print(hidden_state_out.shape) --torch.Size([1, 8, 64])\n",
    "\n",
    "\n",
    "        #input shape: -- torch.Size([8, 45, 64])\n",
    "        tag_space = self.hidden2tag(rnn_out)\n",
    "        #print(\"DONE LINEAR LAYER: \", tag_space.shape) --torch.Size([8, 45, 13])\n",
    "        \n",
    "        tag_scores = F.log_softmax(tag_space, dim=2)\n",
    "        #print(\"DONE SOFTMAX:\", tag_scores.shape) --torch.Size([8, 45, 13])\n",
    "        \n",
    "        return tag_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(model,loss_function,optimizer,device,X,Y):\n",
    "    train_length = len(X)\n",
    "    epoch_train_loss = 0 \n",
    "   \n",
    "    model.train()\n",
    "    for i in tqdm(range(train_length)):\n",
    "        sentence_batch = X[i]\n",
    "        tags_batch = Y[i]\n",
    "\n",
    "        model.zero_grad()\n",
    "\n",
    "        #print(\"Input shape:\",  sentence_batch.shape) --torch.Size([batch_size, max_length_in_batch]) \n",
    "        tag_scores = model(sentence_batch)\n",
    "\n",
    "       \n",
    "        #print(\"model output shape: \", tag_scores.shape) -- torch.Size([8, 45, 13]) \n",
    "        #print(\"tags batch shape: \", tags_batch.shape) --torch.Size([8, 45])\n",
    "        new1 = tag_scores.transpose(1,2)\n",
    "        #print(\"new input shape:\" , new1.shape) --torch.Size([8, 13, 45])\n",
    "\n",
    "        #print(\"STARTING LOSS FUNCTION\")\n",
    "        loss = loss_function(new1 , tags_batch)\n",
    "        #print(\"DONE WITH LOSS FUNCTION\")\n",
    "        #print(loss)\n",
    "        epoch_train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    return model, epoch_train_loss/train_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_loop(model,loss_function,device,X,Y):\n",
    "    val_length = len(X)\n",
    "    epoch_val_loss = 0 \n",
    "\n",
    "    for i in tqdm(range(val_length)):\n",
    "        sentence_batch = X[i]\n",
    "        tags_batch = Y[i]\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        tag_scores = model(sentence_batch)\n",
    "\n",
    "        #CALL A FUNCTION WITH tag_scores and targets, GET PRECISION RECALL FScores        \n",
    "\n",
    "        new1 = tag_scores.transpose(1,2) \n",
    "        loss = loss_function(new1, tags_batch)\n",
    "        epoch_val_loss += loss.item()\n",
    "          \n",
    "     \n",
    "    \n",
    "    return epoch_val_loss/val_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "HIDDEN_DIM = 64\n",
    "batch_size = 4\n",
    "model =RNNTagger(HIDDEN_DIM, len(unique_tags_dict.keys())+1, batch_size)\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "model = model.to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_SIZE = 0.1\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_final, Y_final, test_size=TEST_SIZE, random_state=4)\n",
    "\n",
    "VALID_SIZE = 0.15\n",
    "X_train, X_validation, Y_train, Y_validation = train_test_split(X_train, Y_train, test_size=VALID_SIZE, random_state=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  0,  0,  0,  0,  0, 12, 10, 10, 11, 12,  1, 10,  8, 10,  8, 10,  2,\n",
       "         10,  7,  1, 10, 10, 11,  8, 12, 10, 11, 11,  5],\n",
       "        [ 0,  0,  0,  0,  0,  0,  5,  8, 12,  1, 10,  5,  4, 11,  3, 11, 10,  5,\n",
       "          7, 11, 10,  7, 12, 10,  5, 11, 12, 10, 10,  5],\n",
       "        [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  5,  4, 11,  1,  7,\n",
       "         10, 11,  8,  4, 11,  3, 11,  4, 10,  8,  5,  5],\n",
       "        [ 5,  4, 11,  8,  4, 11,  4,  8, 12, 10,  8, 12, 10,  2, 11,  4, 12, 11,\n",
       "          6, 10,  7, 11,  5,  5, 11,  6,  1, 10, 10,  5],\n",
       "        [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  5,  3,  4, 11,  5,  5,  8,\n",
       "         12, 10,  5,  6,  8, 12, 10, 11, 11,  5,  5,  5],\n",
       "        [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0, 12, 10, 10,  3, 11,  7, 10,  5],\n",
       "        [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0, 10, 10, 11, 12,  1, 10,  5],\n",
       "        [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 11,  8, 12, 10, 10,  5,\n",
       "         12, 10, 10, 11, 11,  8,  3,  5,  6, 12, 10,  5]], device='cuda:0')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    0,     0,     0,     0,     0,     0, 37456, 58484, 37753, 55439,\n",
       "          7786, 23037, 55329, 38802, 59449, 38802, 52997, 12909, 35681,  4565,\n",
       "         56062, 37916, 42107, 21637, 29634, 37456, 30087, 49699, 59442, 36207],\n",
       "        [    0,     0,     0,     0,     0,     0, 50973,  1176, 37456, 59449,\n",
       "         18816,  9529,  6756, 16150, 56334, 13895, 12820, 45705, 42266,  1523,\n",
       "         28622, 42266, 37456, 37229,  9529, 31269,  7786, 40107,  6742, 36207],\n",
       "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0, 50973, 32789,  4491, 50899, 42266, 51686,   583,\n",
       "         13338, 52933, 16150, 59449, 29520, 26858, 52740, 14806, 36207, 45705],\n",
       "        [50973, 32789,  4491, 22151, 52933, 21723,  6756, 38802,  7786, 59449,\n",
       "          1176,  7786, 37869, 50302, 21677,  6756, 37427, 49699, 20056, 57044,\n",
       "         42266, 14403,  9529, 45705,  2602, 30257, 49529, 42107,  2951, 36207],\n",
       "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0, 50973, 59449, 52933,  2602,  9529, 34093, 16693, 37456, 23029,\n",
       "          9529,   497, 21652, 37456, 57044, 49264, 34202, 36207, 10586, 45705],\n",
       "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0, 37456, 11906,  9076, 59449, 10755, 42266, 33842, 36207],\n",
       "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,  5647, 54579, 44323, 59449, 15555, 21294, 36207],\n",
       "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0, 18159, 29634, 59449, 49422, 50022,  9529, 37456, 13051,\n",
       "         45874,  4491, 45548, 50064, 49112, 42724, 22012,  7786,  7283, 36207]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6904\n",
      "6904\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train))\n",
    "print(len(Y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6904/6904 [00:15<00:00, 457.59it/s]\n",
      "100%|██████████| 1219/1219 [00:00<00:00, 1463.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 0, training loss: 0.47562323025884246, validation loss: 0.38053657368766763\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6904/6904 [00:10<00:00, 682.70it/s]\n",
      "100%|██████████| 1219/1219 [00:00<00:00, 1733.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 1, training loss: 0.34589468784574184, validation loss: 0.32674455427556276\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6904/6904 [00:10<00:00, 680.64it/s]\n",
      "100%|██████████| 1219/1219 [00:00<00:00, 1702.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 2, training loss: 0.3056178611799263, validation loss: 0.2995219011901831\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6904/6904 [00:10<00:00, 651.32it/s]\n",
      "100%|██████████| 1219/1219 [00:00<00:00, 1271.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 3, training loss: 0.2825986559708902, validation loss: 0.28270342392912645\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6904/6904 [00:11<00:00, 618.22it/s]\n",
      "100%|██████████| 1219/1219 [00:00<00:00, 1461.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 4, training loss: 0.26707962754496556, validation loss: 0.27118278883391467\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "    model , train_loss = train_loop(model,loss_function,optimizer,device,X_train,Y_train)\n",
    "    val_loss = validation_loop(model,loss_function,device,X_validation,Y_validation)\n",
    "    print(\"For epoch {}, training loss: {}, validation loss: {}\".format(epoch, train_loss, val_loss))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DEMO CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [WinError 10054] An\n",
      "[nltk_data]     existing connection was forcibly closed by the remote\n",
      "[nltk_data]     host>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_TESTsequence_sentence(seq, to_ix):\n",
    "    values = []\n",
    "    for i in to_ix.keys():\n",
    "        values.append(to_ix[i])\n",
    "    \n",
    "    max_value_dict = max(values)\n",
    "    \n",
    "    idxs = []\n",
    "\n",
    "    for w in seq:\n",
    "        if w.lower() in to_ix.keys():\n",
    "            idxs.append(to_ix[w.lower()])\n",
    "        else:\n",
    "            idxs.append(max_value_dict+1)\n",
    "    return idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 'adj',\n",
       " 2: 'conj',\n",
       " 3: 'adv',\n",
       " 4: 'pron',\n",
       " 5: '.',\n",
       " 6: 'num',\n",
       " 7: 'prt',\n",
       " 8: 'adp',\n",
       " 9: 'x',\n",
       " 10: 'noun',\n",
       " 11: 'verb',\n",
       " 12: 'det',\n",
       " 0: '0'}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_to_tags = {}\n",
    "for tag in unique_tags_dict:\n",
    "    index_to_tags[unique_tags_dict[tag]] = tag \n",
    "index_to_tags[0] = '0'\n",
    "index_to_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bottle', 'is', '5', 'years', 'old', ',', 'non', 'executive', 'director', 'of', 'CSK']\n",
      "[54310, 44323, 6796, 18958, 11126, 9529, 49443, 8565, 46726, 21652, 59449]\n",
      "bottle noun\n",
      "is verb\n",
      "5 num\n",
      "years noun\n",
      "old adj\n",
      ", .\n",
      "non noun\n",
      "executive noun\n",
      "director noun\n",
      "of adp\n",
      "CSK noun\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "text = \"bottle is 5 years old, non executive director of CSK\"\n",
    "tokenized_seq = word_tokenize(text)\n",
    "print(tokenized_seq)\n",
    "\n",
    "tokens= prepare_TESTsequence_sentence(tokenized_seq,unique_words_dict)\n",
    "print(tokens) \n",
    "\n",
    "tokens = torch.tensor(tokens).to(device = device)\n",
    "tokens = tokens.unsqueeze(0)\n",
    "output = model(tokens)\n",
    "output = output.squeeze(0)\n",
    "tag_index = torch.argmax(output,dim=1)\n",
    "#print(tag_index.shape)\n",
    "#print(output.shape)\n",
    "for predicted_tag,word in zip(tag_index,tokenized_seq):\n",
    "    predicted_tag = predicted_tag.item()\n",
    "    predicted_tag = index_to_tags[predicted_tag]\n",
    "    print(word, predicted_tag)\n",
    "\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[5, 6, 8]])\n",
      "tensor([1, 2, 3])\n",
      "tensor([[5, 6, 8],\n",
      "        [1, 2, 3]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 300])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#weight = torch.FloatTensor([[1, 2.3, 3], [4, 5.1, 6.3]])\n",
    "embedding = nn.Embedding.from_pretrained(embedding_weights)\n",
    "# Get embeddings for index 1\n",
    "list1 = [5,6,8],\n",
    "list1 = torch.tensor(list1)\n",
    "list2 = [1,2,3]\n",
    "list2 = torch.tensor(list2)\n",
    "list3 = torch.zeros((2,3), dtype = int)\n",
    "print(list1)\n",
    "print(list2)\n",
    "list3[0] = list1 \n",
    "list3[1] = list2\n",
    "print(list3)\n",
    "a = embedding(list3)\n",
    "a.shape -- batch_size,L_embedding_dim\n",
    "#for i in list3:\n",
    "#    print(i)\n",
    "#    a = embedding(i)\n",
    "#    print(a.shape)\n",
    "#print(list1)\n",
    "#input = torch.LongTensor([1])\n",
    "#a = embedding(list3)\n",
    "#a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
