{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "import torch\n",
    "word2id  = {}\n",
    "vocab_size = len(word2id.keys())\n",
    "hidden_emb_size = 200 \n",
    "U = torch.zeros([vocab_size, hidden_emb_size], dtype=float)\n",
    "V = torch.zeros([ hidden_emb_size, vocab_size], dtype=float)\n",
    "\n",
    "def getword2id(vocab_file_path):\n",
    "    with open(vocab_file_path) as f:\n",
    "        lines = f.readlines()      \n",
    "        for idx,line in enumerate(lines):\n",
    "            word = line.strip().split()[0] #see if we need any processing.\n",
    "            word2id[word] = idx\n",
    "\n",
    "def getOneHotVectors(context_words, output_word):\n",
    "    input_vec = torch.zeros(vocab_size, dtype=float)\n",
    "    context_idxs = [ word2id[word] for word in context_words]\n",
    "    input_vec[context_idxs]  =  1.0\n",
    "    output_vec =  word2id[output_word]\n",
    "    return input_vec, output_vec\n",
    "\n",
    "def lossCBOW()\n",
    "\n",
    "def forwardPropCBOW(context_words, output_word):\n",
    "    input_vec, output_vec = getOneHotVectors(context_words, output_word)\n",
    "    # hidden_layer_embeddings = matmul(U.transpose(), input_vec)   # D*1\n",
    "    # predicted_vec = softmax(matmul(V.transpose(), hidden_layer_embeddings))   #V*1\n",
    "\n",
    "def backProp():\n",
    "\n",
    "\n",
    "input_vec, output_vec = getOneHotVectors(context_words)\n",
    "\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
